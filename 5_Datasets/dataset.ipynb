{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecace47",
   "metadata": {},
   "source": [
    "# 🥐 Bäckerei Umsatz Vorhersage - Trainingsdatensatz Erstellung\n",
    "\n",
    "Dieses Notebook erstellt einen kombinierten Trainingsdatensatz für die Vorhersage von Bäckerei-Umsätzen basierend auf:\n",
    "- Historischen Umsatzdaten\n",
    "- Wetterdaten  \n",
    "- Feiertagen\n",
    "- Besonderen Events (Kieler Woche)\n",
    "- Preisindizes für Backwaren\n",
    "\n",
    "**Ziel:** Ein sauberer DataFrame für Regressionsanalyse und neuronale Netze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f911bc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bibliotheken erfolgreich geladen!\n",
      "🐼 Pandas Version: 2.2.3\n",
      "🔢 NumPy Version: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "# 1. BIBLIOTHEKEN LADEN UND ANZEIGEOPTIONEN SETZEN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pandas Anzeigeoptionen für bessere Darstellung\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"✅ Bibliotheken erfolgreich geladen!\")\n",
    "print(f\"🐼 Pandas Version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy Version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e661c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Lade Umsatzdaten...\n",
      "Shape der Umsatzdaten: (9334, 4)\n",
      "Spalten: ['id', 'Datum', 'Warengruppe', 'Umsatz']\n",
      "\n",
      "Erste 5 Zeilen:\n",
      "        id       Datum  Warengruppe      Umsatz\n",
      "0  1307011  2013-07-01            1  148.828353\n",
      "1  1307021  2013-07-02            1  159.793757\n",
      "2  1307031  2013-07-03            1  111.885594\n",
      "3  1307041  2013-07-04            1  168.864941\n",
      "4  1307051  2013-07-05            1  171.280754\n",
      "\n",
      "🔄 Extrahiere Datum und Warengruppe aus ID...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     38\u001b[39m df_umsatz[\u001b[33m'\u001b[39m\u001b[33mWarengruppe\u001b[39m\u001b[33m'\u001b[39m] = df_umsatz[\u001b[33m'\u001b[39m\u001b[33mWarengruppe\u001b[39m\u001b[33m'\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Weitere Datums-Features erstellen\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m df_umsatz[\u001b[33m'\u001b[39m\u001b[33mJahr\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf_umsatz\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDatum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdt\u001b[49m.year\n\u001b[32m     42\u001b[39m df_umsatz[\u001b[33m'\u001b[39m\u001b[33mMonat\u001b[39m\u001b[33m'\u001b[39m] = df_umsatz[\u001b[33m'\u001b[39m\u001b[33mDatum\u001b[39m\u001b[33m'\u001b[39m].dt.month\n\u001b[32m     43\u001b[39m df_umsatz[\u001b[33m'\u001b[39m\u001b[33mTag\u001b[39m\u001b[33m'\u001b[39m] = df_umsatz[\u001b[33m'\u001b[39m\u001b[33mDatum\u001b[39m\u001b[33m'\u001b[39m].dt.day\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/generic.py:6299\u001b[39m, in \u001b[36mNDFrame.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   6293\u001b[39m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_names_set\n\u001b[32m   6294\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._metadata\n\u001b[32m   6295\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessors\n\u001b[32m   6296\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6297\u001b[39m ):\n\u001b[32m   6298\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[32m-> \u001b[39m\u001b[32m6299\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/accessor.py:224\u001b[39m, in \u001b[36mCachedAccessor.__get__\u001b[39m\u001b[34m(self, obj, cls)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._accessor\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m accessor_obj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_accessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[38;5;66;03m# NDFrame\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(obj, \u001b[38;5;28mself\u001b[39m._name, accessor_obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/core/indexes/accessors.py:643\u001b[39m, in \u001b[36mCombinedDatetimelikeProperties.__new__\u001b[39m\u001b[34m(cls, data)\u001b[39m\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data.dtype, PeriodDtype):\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PeriodProperties(data, orig)\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan only use .dt accessor with datetimelike values\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# 2. UMSATZDATEN LADEN UND VERARBEITEN\n",
    "print(\"📊 Lade Umsatzdaten...\")\n",
    "\n",
    "# Umsatzdaten von GitHub laden\n",
    "umsatz_url = \"https://raw.githubusercontent.com/opencampus-sh/einfuehrung-in-data-science-und-ml/main/umsatzdaten_gekuerzt.csv\"\n",
    "df_umsatz = pd.read_csv(umsatz_url)\n",
    "\n",
    "print(f\"Shape der Umsatzdaten: {df_umsatz.shape}\")\n",
    "print(f\"Spalten: {list(df_umsatz.columns)}\")\n",
    "print(\"\\nErste 5 Zeilen:\")\n",
    "print(df_umsatz.head())\n",
    "\n",
    "# Datum aus der ID extrahieren (Format: YYYYMMDW)\n",
    "def extract_date_info(row_id):\n",
    "    \"\"\"Extrahiert Datum und Warengruppe aus der ID\"\"\"\n",
    "    id_str = str(int(row_id))\n",
    "    \n",
    "    # YYYYMMDD + W (Warengruppe)\n",
    "    if len(id_str) == 9:\n",
    "        jahr = int(id_str[:4])\n",
    "        monat = int(id_str[4:6])\n",
    "        tag = int(id_str[6:8])\n",
    "        warengruppe = int(id_str[8])\n",
    "        \n",
    "        try:\n",
    "            datum = pd.to_datetime(f\"{jahr}-{monat:02d}-{tag:02d}\")\n",
    "            return datum, warengruppe\n",
    "        except:\n",
    "            return None, None\n",
    "    return None, None\n",
    "\n",
    "# Datum und Warengruppe extrahieren\n",
    "print(\"\\n🔄 Extrahiere Datum und Warengruppe aus ID...\")\n",
    "df_umsatz['Datum'], df_umsatz['Warengruppe'] = zip(*df_umsatz.iloc[:, 0].apply(extract_date_info))\n",
    "\n",
    "# Ungültige Daten entfernen\n",
    "df_umsatz = df_umsatz.dropna(subset=['Datum', 'Warengruppe'])\n",
    "df_umsatz['Warengruppe'] = df_umsatz['Warengruppe'].astype(int)\n",
    "\n",
    "# Weitere Datums-Features erstellen\n",
    "df_umsatz['Jahr'] = df_umsatz['Datum'].dt.year\n",
    "df_umsatz['Monat'] = df_umsatz['Datum'].dt.month\n",
    "df_umsatz['Tag'] = df_umsatz['Datum'].dt.day\n",
    "df_umsatz['Wochentag'] = df_umsatz['Datum'].dt.day_name()\n",
    "df_umsatz['Wochentag_Nr'] = df_umsatz['Datum'].dt.dayofweek\n",
    "\n",
    "print(f\"✅ {len(df_umsatz)} gültige Datensätze nach Extraktion\")\n",
    "print(f\"📅 Zeitraum: {df_umsatz['Datum'].min()} bis {df_umsatz['Datum'].max()}\")\n",
    "print(f\"🏷️ Warengruppen: {sorted(df_umsatz['Warengruppe'].unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARENGRUPPEN IN LESBARE NAMEN UMWANDELN\n",
    "warengruppen_mapping = {\n",
    "    1: \"Brot\",\n",
    "    2: \"Brötchen\", \n",
    "    3: \"Croissant\",\n",
    "    4: \"Konditorei\",\n",
    "    5: \"Kuchen\",\n",
    "    6: \"Saisonbrot\"\n",
    "}\n",
    "\n",
    "df_umsatz['Warengruppe_Name'] = df_umsatz['Warengruppe'].map(warengruppen_mapping)\n",
    "\n",
    "# One-Hot-Encoding für Warengruppen\n",
    "print(\"🔄 Erstelle One-Hot-Encoding für Warengruppen...\")\n",
    "warengruppen_dummies = pd.get_dummies(df_umsatz['Warengruppe_Name'], prefix='Warengruppe')\n",
    "df_umsatz = pd.concat([df_umsatz, warengruppen_dummies], axis=1)\n",
    "\n",
    "print(\"✅ Warengruppen-Features erstellt:\")\n",
    "warengruppen_cols = [col for col in df_umsatz.columns if col.startswith('Warengruppe_')]\n",
    "print(warengruppen_cols)\n",
    "\n",
    "# Überblick über die Daten\n",
    "print(f\"\\n📈 Dataset Info:\")\n",
    "print(f\"Zeilen: {len(df_umsatz)}\")\n",
    "print(f\"Spalten: {len(df_umsatz.columns)}\")\n",
    "print(f\"Umsatzbereich: {df_umsatz.iloc[:, 1].min():.2f} € - {df_umsatz.iloc[:, 1].max():.2f} €\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. HISTOGRAMM DER UMSÄTZE ERSTELLEN\n",
    "print(\"📊 Erstelle Umsatz-Visualisierungen...\")\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Gesamtverteilung der Umsätze\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(df_umsatz.iloc[:, 1], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Verteilung aller Umsätze')\n",
    "plt.xlabel('Umsatz (€)')\n",
    "plt.ylabel('Häufigkeit')\n",
    "\n",
    "# Subplot 2: Umsätze nach Warengruppe\n",
    "plt.subplot(2, 3, 2)\n",
    "df_umsatz.boxplot(column=df_umsatz.columns[1], by='Warengruppe_Name', ax=plt.gca())\n",
    "plt.title('Umsätze nach Warengruppe')\n",
    "plt.xticks(rotation=45)\n",
    "plt.suptitle('')\n",
    "\n",
    "# Subplot 3: Umsätze nach Wochentag\n",
    "plt.subplot(2, 3, 3)\n",
    "wochentag_umsatz = df_umsatz.groupby('Wochentag')[df_umsatz.columns[1]].mean()\n",
    "wochentag_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "wochentag_umsatz = wochentag_umsatz.reindex(wochentag_order)\n",
    "wochentag_umsatz.plot(kind='bar', color='lightgreen')\n",
    "plt.title('Durchschnittsumsatz nach Wochentag')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Umsatz (€)')\n",
    "\n",
    "# Subplot 4: Umsätze nach Monat\n",
    "plt.subplot(2, 3, 4)\n",
    "monat_umsatz = df_umsatz.groupby('Monat')[df_umsatz.columns[1]].mean()\n",
    "monat_umsatz.plot(kind='bar', color='orange')\n",
    "plt.title('Durchschnittsumsatz nach Monat')\n",
    "plt.xlabel('Monat')\n",
    "plt.ylabel('Umsatz (€)')\n",
    "\n",
    "# Subplot 5: Zeitreihe der Umsätze\n",
    "plt.subplot(2, 3, 5)\n",
    "tagesumsatz = df_umsatz.groupby('Datum')[df_umsatz.columns[1]].sum()\n",
    "plt.plot(tagesumsatz.index, tagesumsatz.values, alpha=0.7)\n",
    "plt.title('Tagesumsätze über Zeit')\n",
    "plt.xlabel('Datum')\n",
    "plt.ylabel('Tagesumsatz (€)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Subplot 6: Top Warengruppen\n",
    "plt.subplot(2, 3, 6)\n",
    "warengruppen_umsatz = df_umsatz.groupby('Warengruppe_Name')[df_umsatz.columns[1]].sum().sort_values(ascending=False)\n",
    "warengruppen_umsatz.plot(kind='bar', color='purple')\n",
    "plt.title('Gesamtumsatz nach Warengruppe')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Gesamtumsatz (€)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"📊 Umsatz-Statistiken:\")\n",
    "print(f\"Durchschnitt: {df_umsatz.iloc[:, 1].mean():.2f} €\")\n",
    "print(f\"Median: {df_umsatz.iloc[:, 1].median():.2f} €\")\n",
    "print(f\"Standardabweichung: {df_umsatz.iloc[:, 1].std():.2f} €\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. WETTERDATEN LADEN UND VERKNÜPFEN\n",
    "print(\"🌤️ Lade Wetterdaten...\")\n",
    "\n",
    "# Wetterdaten von GitHub laden\n",
    "wetter_url = \"https://raw.githubusercontent.com/opencampus-sh/einfuehrung-in-data-science-und-ml/main/wetter.csv\"\n",
    "df_wetter = pd.read_csv(wetter_url)\n",
    "\n",
    "print(f\"Shape der Wetterdaten: {df_wetter.shape}\")\n",
    "print(f\"Spalten: {list(df_wetter.columns)}\")\n",
    "print(\"\\nErste 5 Zeilen Wetterdaten:\")\n",
    "print(df_wetter.head())\n",
    "\n",
    "# Datum in Wetterdaten konvertieren\n",
    "df_wetter['Datum'] = pd.to_datetime(df_wetter['Datum'])\n",
    "\n",
    "# Feature: Wettercode fehlt\n",
    "df_wetter['Wettercode_fehlt'] = df_wetter['Wettercode'].isna().astype(int)\n",
    "\n",
    "# Fehlende Wetterdaten mit Median füllen\n",
    "numeric_weather_cols = ['Temperatur', 'Windgeschwindigkeit', 'Bewoelkung']\n",
    "for col in numeric_weather_cols:\n",
    "    if col in df_wetter.columns:\n",
    "        median_val = df_wetter[col].median()\n",
    "        df_wetter[col] = df_wetter[col].fillna(median_val)\n",
    "        print(f\"📊 {col}: Fehlende Werte mit Median {median_val:.2f} gefüllt\")\n",
    "\n",
    "# Wetterdaten mit Umsatzdaten verknüpfen\n",
    "print(\"\\n🔗 Verknüpfe Wetter- mit Umsatzdaten...\")\n",
    "df_combined = df_umsatz.merge(df_wetter, on='Datum', how='left')\n",
    "\n",
    "print(f\"✅ Kombinierter Datensatz: {df_combined.shape}\")\n",
    "print(f\"🌡️ Temperaturbereich: {df_combined['Temperatur'].min():.1f}°C - {df_combined['Temperatur'].max():.1f}°C\")\n",
    "\n",
    "# Fehlende Wetterdaten anzeigen\n",
    "missing_weather = df_combined['Temperatur'].isna().sum()\n",
    "if missing_weather > 0:\n",
    "    print(f\"⚠️ {missing_weather} Tage ohne Wetterdaten - werden mit Durchschnitt gefüllt\")\n",
    "    for col in numeric_weather_cols:\n",
    "        if col in df_combined.columns:\n",
    "            df_combined[col] = df_combined[col].fillna(df_combined[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. FEIERTAGE LADEN UND VERKNÜPFEN\n",
    "print(\"🎄 Lade Feiertagsdaten...\")\n",
    "\n",
    "try:\n",
    "    # Feiertage aus lokaler Datei laden\n",
    "    feiertage_path = \"/workspaces/bakery_sales_prediction/5_Datasets/DE-Feiertage_2020_bis_2035.csv\"\n",
    "    df_feiertage = pd.read_csv(feiertage_path)\n",
    "    \n",
    "    print(f\"Shape der Feiertagsdaten: {df_feiertage.shape}\")\n",
    "    print(f\"Spalten: {list(df_feiertage.columns)}\")\n",
    "    print(\"\\nErste 5 Feiertage:\")\n",
    "    print(df_feiertage.head())\n",
    "    \n",
    "    # Datum in Feiertagsdaten konvertieren\n",
    "    datum_col = df_feiertage.columns[0]  # Erste Spalte als Datum\n",
    "    df_feiertage['Datum'] = pd.to_datetime(df_feiertage[datum_col])\n",
    "    \n",
    "    # Nur relevante Jahre filtern (2013-2018)\n",
    "    df_feiertage = df_feiertage[\n",
    "        (df_feiertage['Datum'].dt.year >= 2013) & \n",
    "        (df_feiertage['Datum'].dt.year <= 2018)\n",
    "    ]\n",
    "    \n",
    "    # Binäre Feiertag-Spalte erstellen\n",
    "    feiertage_dates = set(df_feiertage['Datum'].dt.date)\n",
    "    df_combined['ist_feiertag'] = df_combined['Datum'].dt.date.isin(feiertage_dates).astype(int)\n",
    "    \n",
    "    print(f\"✅ {len(feiertage_dates)} Feiertage im relevanten Zeitraum gefunden\")\n",
    "    print(f\"📅 Tage mit Feiertag im Dataset: {df_combined['ist_feiertag'].sum()}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ Feiertagsdatei nicht gefunden - erstelle manuell Feiertage\")\n",
    "    # Manuelle Definition wichtiger Feiertage\n",
    "    deutsche_feiertage = [\n",
    "        # 2013\n",
    "        '2013-01-01', '2013-03-29', '2013-04-01', '2013-05-01', '2013-05-09', '2013-05-20', \n",
    "        '2013-10-03', '2013-12-25', '2013-12-26',\n",
    "        # 2014\n",
    "        '2014-01-01', '2014-04-18', '2014-04-21', '2014-05-01', '2014-05-29', '2014-06-09',\n",
    "        '2014-10-03', '2014-12-25', '2014-12-26',\n",
    "        # 2015-2018 weitere Feiertage...\n",
    "    ]\n",
    "    \n",
    "    feiertage_dates = set(pd.to_datetime(deutsche_feiertage).date)\n",
    "    df_combined['ist_feiertag'] = df_combined['Datum'].dt.date.isin(feiertage_dates).astype(int)\n",
    "    print(f\"✅ {len(feiertage_dates)} manuelle Feiertage erstellt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. JAHRESZEIT BESTIMMEN\n",
    "print(\"🌸 Erstelle Jahreszeiten-Feature...\")\n",
    "\n",
    "def get_season(month):\n",
    "    \"\"\"Bestimmt die Jahreszeit basierend auf dem Monat\"\"\"\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Frühling\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Sommer\"\n",
    "    else:  # 9, 10, 11\n",
    "        return \"Herbst\"\n",
    "\n",
    "df_combined['Jahreszeit'] = df_combined['Monat'].apply(get_season)\n",
    "\n",
    "# One-Hot-Encoding für Jahreszeiten\n",
    "jahreszeiten_dummies = pd.get_dummies(df_combined['Jahreszeit'], prefix='Jahreszeit')\n",
    "df_combined = pd.concat([df_combined, jahreszeiten_dummies], axis=1)\n",
    "\n",
    "print(\"✅ Jahreszeiten-Features erstellt:\")\n",
    "jahreszeiten_cols = [col for col in df_combined.columns if col.startswith('Jahreszeit_')]\n",
    "print(jahreszeiten_cols)\n",
    "\n",
    "# Verteilung der Jahreszeiten anzeigen\n",
    "jahreszeit_verteilung = df_combined['Jahreszeit'].value_counts()\n",
    "print(f\"\\n📊 Verteilung der Jahreszeiten:\")\n",
    "print(jahreszeit_verteilung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa7ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. SONNTAG FEATURE ERSTELLEN\n",
    "print(\"📅 Erstelle Sonntag-Feature...\")\n",
    "\n",
    "# Sonntag = Wochentag 6 (Montag = 0)\n",
    "df_combined['ist_sonntag'] = (df_combined['Wochentag_Nr'] == 6).astype(int)\n",
    "\n",
    "print(f\"✅ Sonntag-Feature erstellt\")\n",
    "print(f\"📊 Anzahl Sonntage im Dataset: {df_combined['ist_sonntag'].sum()}\")\n",
    "print(f\"📊 Prozent Sonntage: {df_combined['ist_sonntag'].mean()*100:.1f}%\")\n",
    "\n",
    "# Umsätze an Sonntagen vs. anderen Tagen\n",
    "sonntag_umsatz = df_combined[df_combined['ist_sonntag'] == 1][df_combined.columns[1]].mean()\n",
    "werktag_umsatz = df_combined[df_combined['ist_sonntag'] == 0][df_combined.columns[1]].mean()\n",
    "\n",
    "print(f\"💰 Durchschnittsumsatz Sonntags: {sonntag_umsatz:.2f} €\")\n",
    "print(f\"💰 Durchschnittsumsatz andere Tage: {werktag_umsatz:.2f} €\")\n",
    "print(f\"📈 Sonntag vs. Werktag Ratio: {sonntag_umsatz/werktag_umsatz:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c40f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. KIELER WOCHE INTEGRIEREN\n",
    "print(\"⛵ Lade Kieler Woche Daten...\")\n",
    "\n",
    "try:\n",
    "    # KiWo-Daten von GitHub laden\n",
    "    kiwo_url = \"https://raw.githubusercontent.com/opencampus-sh/einfuehrung-in-data-science-und-ml/main/kiwo.csv\"\n",
    "    df_kiwo = pd.read_csv(kiwo_url)\n",
    "    \n",
    "    print(f\"Shape der KiWo-Daten: {df_kiwo.shape}\")\n",
    "    print(f\"Spalten: {list(df_kiwo.columns)}\")\n",
    "    print(\"\\nKieler Woche Termine:\")\n",
    "    print(df_kiwo.head())\n",
    "    \n",
    "    # Datumsspalten konvertieren\n",
    "    start_col = df_kiwo.columns[0]\n",
    "    end_col = df_kiwo.columns[1] if len(df_kiwo.columns) > 1 else df_kiwo.columns[0]\n",
    "    \n",
    "    df_kiwo['Start'] = pd.to_datetime(df_kiwo[start_col])\n",
    "    df_kiwo['Ende'] = pd.to_datetime(df_kiwo[end_col]) if end_col != start_col else df_kiwo['Start']\n",
    "    \n",
    "    # KiWo-Feature erstellen\n",
    "    def is_kiwo_date(datum):\n",
    "        \"\"\"Prüft ob ein Datum in der Kieler Woche liegt\"\"\"\n",
    "        for _, row in df_kiwo.iterrows():\n",
    "            if row['Start'] <= datum <= row['Ende']:\n",
    "                return 1\n",
    "        return 0\n",
    "    \n",
    "    df_combined['ist_kiwo'] = df_combined['Datum'].apply(is_kiwo_date)\n",
    "    \n",
    "    print(f\"✅ KiWo-Feature erstellt\")\n",
    "    print(f\"📊 Tage während Kieler Woche: {df_combined['ist_kiwo'].sum()}\")\n",
    "    \n",
    "    # Umsätze während KiWo vs. normale Tage\n",
    "    if df_combined['ist_kiwo'].sum() > 0:\n",
    "        kiwo_umsatz = df_combined[df_combined['ist_kiwo'] == 1][df_combined.columns[1]].mean()\n",
    "        normal_umsatz = df_combined[df_combined['ist_kiwo'] == 0][df_combined.columns[1]].mean()\n",
    "        \n",
    "        print(f\"💰 Durchschnittsumsatz während KiWo: {kiwo_umsatz:.2f} €\")\n",
    "        print(f\"💰 Durchschnittsumsatz normale Tage: {normal_umsatz:.2f} €\")\n",
    "        print(f\"📈 KiWo vs. Normal Ratio: {kiwo_umsatz/normal_umsatz:.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Fehler beim Laden der KiWo-Daten: {e}\")\n",
    "    print(\"Erstelle Standard KiWo-Feature (alle 0)\")\n",
    "    df_combined['ist_kiwo'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9556cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. PREISINDEX LADEN UND ZUORDNEN\n",
    "print(\"📈 Lade Preisindex-Daten...\")\n",
    "\n",
    "try:\n",
    "    # Preisindex aus lokaler Datei laden\n",
    "    preisindex_path = \"/workspaces/bakery_sales_prediction/5_Datasets/additional data/Preisentwicklung_Backwaren.csv\"\n",
    "    df_preise = pd.read_csv(preisindex_path)\n",
    "    \n",
    "    print(f\"Shape der Preisdaten: {df_preise.shape}\")\n",
    "    print(f\"Spalten: {list(df_preise.columns)}\")\n",
    "    print(\"\\nErste 5 Zeilen Preisdaten:\")\n",
    "    print(df_preise.head())\n",
    "    \n",
    "    # Datumsspalte konvertieren (erste Spalte)\n",
    "    datum_col = df_preise.columns[0]\n",
    "    df_preise['Datum'] = pd.to_datetime(df_preise[datum_col])\n",
    "    \n",
    "    # Preisindex-Spalte identifizieren (meist zweite Spalte)\n",
    "    preis_col = df_preise.columns[1]\n",
    "    df_preise['Preisindex'] = pd.to_numeric(df_preise[preis_col], errors='coerce')\n",
    "    \n",
    "    # Wöchentliche Zuordnung: Jeden Tag den Preisindex vom Montag der Woche zuordnen\n",
    "    def get_monday_of_week(date):\n",
    "        \"\"\"Gibt den Montag der Woche für ein gegebenes Datum zurück\"\"\"\n",
    "        days_since_monday = date.weekday()\n",
    "        monday = date - timedelta(days=days_since_monday)\n",
    "        return monday\n",
    "    \n",
    "    # Montag für jeden Tag berechnen\n",
    "    df_combined['Montag_der_Woche'] = df_combined['Datum'].apply(get_monday_of_week)\n",
    "    \n",
    "    # Preisindex für jeden Montag zuordnen\n",
    "    preis_dict = dict(zip(df_preise['Datum'].dt.date, df_preise['Preisindex']))\n",
    "    \n",
    "    def get_price_index(montag):\n",
    "        \"\"\"Holt den Preisindex für einen bestimmten Montag\"\"\"\n",
    "        montag_date = montag.date()\n",
    "        if montag_date in preis_dict:\n",
    "            return preis_dict[montag_date]\n",
    "        \n",
    "        # Falls kein exakter Montag gefunden wird, suche den nächstliegenden\n",
    "        available_dates = list(preis_dict.keys())\n",
    "        if available_dates:\n",
    "            closest_date = min(available_dates, key=lambda x: abs((x - montag_date).days))\n",
    "            return preis_dict[closest_date]\n",
    "        \n",
    "        return 100.0  # Standardwert falls nichts gefunden wird\n",
    "    \n",
    "    df_combined['Preisindex'] = df_combined['Montag_der_Woche'].apply(get_price_index)\n",
    "    \n",
    "    print(f\"✅ Preisindex-Feature erstellt\")\n",
    "    print(f\"📊 Preisindex Bereich: {df_combined['Preisindex'].min():.2f} - {df_combined['Preisindex'].max():.2f}\")\n",
    "    print(f\"📊 Durchschnittlicher Preisindex: {df_combined['Preisindex'].mean():.2f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ Preisindex-Datei nicht gefunden - erstelle Standard-Preisindex\")\n",
    "    # Standard-Preisindex erstellen (leichte Inflation über Zeit)\n",
    "    base_index = 100.0\n",
    "    df_combined['Jahre_seit_2013'] = df_combined['Jahr'] - 2013\n",
    "    df_combined['Preisindex'] = base_index + (df_combined['Jahre_seit_2013'] * 2.5)  # 2.5% Inflation pro Jahr\n",
    "    \n",
    "    print(f\"✅ Standard-Preisindex erstellt (100-112.5)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Fehler beim Verarbeiten der Preisdaten: {e}\")\n",
    "    df_combined['Preisindex'] = 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b51b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. FINALISIERUNG DES DATASETS\n",
    "print(\"🔧 Finalisiere den Trainingsdatensatz...\")\n",
    "\n",
    "# Überprüfe fehlende Werte\n",
    "print(\"\\n🔍 Überprüfe fehlende Werte:\")\n",
    "missing_values = df_combined.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "if len(missing_values) > 0:\n",
    "    print(missing_values)\n",
    "    \n",
    "    # Fülle fehlende numerische Werte mit Median\n",
    "    numeric_cols = df_combined.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_combined[col].isnull().sum() > 0:\n",
    "            median_val = df_combined[col].median()\n",
    "            df_combined[col] = df_combined[col].fillna(median_val)\n",
    "            print(f\"✅ {col}: {df_combined[col].isnull().sum()} fehlende Werte mit Median {median_val:.2f} gefüllt\")\n",
    "else:\n",
    "    print(\"✅ Keine fehlenden Werte gefunden!\")\n",
    "\n",
    "# Relevante Spalten für das finale Dataset auswählen\n",
    "umsatz_col = df_combined.columns[1]  # Zweite Spalte ist Umsatz\n",
    "\n",
    "feature_columns = [\n",
    "    'Datum', 'Jahr', 'Monat', 'Tag', 'Wochentag', 'Wochentag_Nr',\n",
    "    'Warengruppe', 'Warengruppe_Name',\n",
    "    'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung', 'Wettercode_fehlt',\n",
    "    'ist_feiertag', 'ist_sonntag', 'Jahreszeit', 'ist_kiwo', 'Preisindex',\n",
    "    umsatz_col\n",
    "]\n",
    "\n",
    "# One-Hot encoded Spalten hinzufügen\n",
    "warengruppen_cols = [col for col in df_combined.columns if col.startswith('Warengruppe_')]\n",
    "jahreszeiten_cols = [col for col in df_combined.columns if col.startswith('Jahreszeit_')]\n",
    "\n",
    "all_features = feature_columns + warengruppen_cols + jahreszeiten_cols\n",
    "\n",
    "# Finale Spalten filtern (nur die die existieren)\n",
    "final_columns = [col for col in all_features if col in df_combined.columns]\n",
    "df_final = df_combined[final_columns].copy()\n",
    "\n",
    "# Umsatz-Spalte umbenennen für Klarheit\n",
    "df_final = df_final.rename(columns={umsatz_col: 'Umsatz'})\n",
    "\n",
    "print(f\"\\n📊 FINALER TRAININGSDATENSATZ:\")\n",
    "print(f\"📏 Shape: {df_final.shape}\")\n",
    "print(f\"📅 Zeitraum: {df_final['Datum'].min()} bis {df_final['Datum'].max()}\")\n",
    "print(f\"💰 Umsatzbereich: {df_final['Umsatz'].min():.2f} € - {df_final['Umsatz'].max():.2f} €\")\n",
    "\n",
    "print(f\"\\n🏷️ FEATURES ({len(df_final.columns)} Spalten):\")\n",
    "for i, col in enumerate(df_final.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7c2226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET ÜBERSICHT UND BEISPIELDATEN\n",
    "print(\"📋 DATASET ÜBERSICHT:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Grundlegende Statistiken\n",
    "print(f\"📊 Datentypen:\")\n",
    "print(df_final.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\n📈 Numerische Statistiken:\")\n",
    "numeric_cols = df_final.select_dtypes(include=[np.number]).columns\n",
    "print(df_final[numeric_cols].describe().round(2))\n",
    "\n",
    "print(f\"\\n🗂️ Kategorische Variablen:\")\n",
    "categorical_cols = df_final.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if col != 'Datum':\n",
    "        print(f\"{col}: {df_final[col].nunique()} eindeutige Werte\")\n",
    "        print(f\"  Top 3: {list(df_final[col].value_counts().head(3).index)}\")\n",
    "\n",
    "print(f\"\\n📝 BEISPIELDATEN (erste 10 Zeilen):\")\n",
    "print(df_final.head(10))\n",
    "\n",
    "print(f\"\\n📝 BEISPIELDATEN (zufällige 5 Zeilen):\")\n",
    "print(df_final.sample(5))\n",
    "\n",
    "# Korrelationsanalyse für numerische Features\n",
    "print(f\"\\n🔗 TOP KORRELATIONEN MIT UMSATZ:\")\n",
    "correlations = df_final[numeric_cols].corr()['Umsatz'].abs().sort_values(ascending=False)\n",
    "print(correlations.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33107b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALISIERUNG DER NUMERISCHEN FEATURES\n",
    "print(\"🔧 Normalisiere numerische Features für Modelltraining...\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Kopie des finalen Datasets für Normalisierung erstellen\n",
    "df_normalized = df_final.copy()\n",
    "\n",
    "# Identifiziere numerische Features (ohne Zielvariable und One-Hot-Features)\n",
    "numeric_features_to_scale = [\n",
    "    'Jahr', 'Monat', 'Tag', 'Wochentag_Nr',\n",
    "    'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung', \n",
    "    'Preisindex'\n",
    "]\n",
    "\n",
    "# Nur Features behalten, die auch im Dataset existieren\n",
    "numeric_features_to_scale = [col for col in numeric_features_to_scale if col in df_normalized.columns]\n",
    "\n",
    "print(f\"📊 Zu normalisierende Features: {numeric_features_to_scale}\")\n",
    "\n",
    "# Originale Features vor Normalisierung anzeigen\n",
    "print(f\"\\n📈 ORIGINALE FEATURE-STATISTIKEN:\")\n",
    "print(df_normalized[numeric_features_to_scale].describe().round(2))\n",
    "\n",
    "# 1. STANDARD SCALER (Z-Score Normalisierung)\n",
    "print(f\"\\n🔄 Methode 1: Standard Scaler (Z-Score)\")\n",
    "scaler_standard = StandardScaler()\n",
    "df_normalized[['std_' + col for col in numeric_features_to_scale]] = scaler_standard.fit_transform(\n",
    "    df_normalized[numeric_features_to_scale]\n",
    ")\n",
    "\n",
    "# 2. MIN-MAX SCALER (0-1 Normalisierung)\n",
    "print(f\"🔄 Methode 2: Min-Max Scaler (0-1)\")\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_normalized[['minmax_' + col for col in numeric_features_to_scale]] = scaler_minmax.fit_transform(\n",
    "    df_normalized[numeric_features_to_scale]\n",
    ")\n",
    "\n",
    "# 3. ROBUST SCALER (Median-basiert, weniger empfindlich gegen Ausreißer)\n",
    "print(f\"🔄 Methode 3: Robust Scaler (Median-basiert)\")\n",
    "scaler_robust = RobustScaler()\n",
    "df_normalized[['robust_' + col for col in numeric_features_to_scale]] = scaler_robust.fit_transform(\n",
    "    df_normalized[numeric_features_to_scale]\n",
    ")\n",
    "\n",
    "# 4. POWER TRANSFORMER (Yeo-Johnson für Normalverteilung)\n",
    "print(f\"🔄 Methode 4: Power Transformer (Yeo-Johnson)\")\n",
    "try:\n",
    "    power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    df_normalized[['power_' + col for col in numeric_features_to_scale]] = power_transformer.fit_transform(\n",
    "        df_normalized[numeric_features_to_scale]\n",
    "    )\n",
    "except:\n",
    "    print(\"⚠️ Power Transformer übersprungen (numerische Probleme)\")\n",
    "\n",
    "print(f\"✅ Normalisierung abgeschlossen!\")\n",
    "\n",
    "# Vergleiche die Normalisierungsmethoden\n",
    "scaling_methods = ['std', 'minmax', 'robust', 'power']\n",
    "print(f\"\\n📊 VERGLEICH DER NORMALISIERUNGSMETHODEN:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for method in scaling_methods:\n",
    "    method_cols = [col for col in df_normalized.columns if col.startswith(f'{method}_')]\n",
    "    if method_cols:\n",
    "        print(f\"\\n{method.upper()}-SCALER:\")\n",
    "        stats = df_normalized[method_cols].describe().round(3)\n",
    "        print(f\"Mean Bereich: {stats.loc['mean'].min():.3f} bis {stats.loc['mean'].max():.3f}\")\n",
    "        print(f\"Std Bereich: {stats.loc['std'].min():.3f} bis {stats.loc['std'].max():.3f}\")\n",
    "        print(f\"Min Bereich: {stats.loc['min'].min():.3f} bis {stats.loc['min'].max():.3f}\")\n",
    "        print(f\"Max Bereich: {stats.loc['max'].min():.3f} bis {stats.loc['max'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852fe7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALISIERUNG DER NORMALISIERUNGSEFFEKTE\n",
    "print(\"📊 Visualisiere Normalisierungseffekte...\")\n",
    "\n",
    "# Wähle ein repräsentatives Feature für Visualisierung\n",
    "example_feature = 'Temperatur' if 'Temperatur' in numeric_features_to_scale else numeric_features_to_scale[0]\n",
    "print(f\"Beispiel-Feature: {example_feature}\")\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Original\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(df_normalized[example_feature], bins=50, alpha=0.7, color='gray', edgecolor='black')\n",
    "plt.title(f'Original: {example_feature}')\n",
    "plt.xlabel('Wert')\n",
    "plt.ylabel('Häufigkeit')\n",
    "\n",
    "# Standard Scaler\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(df_normalized[f'std_{example_feature}'], bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title(f'Standard Scaler: {example_feature}')\n",
    "plt.xlabel('Z-Score')\n",
    "plt.ylabel('Häufigkeit')\n",
    "\n",
    "# Min-Max Scaler\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(df_normalized[f'minmax_{example_feature}'], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.title(f'Min-Max Scaler: {example_feature}')\n",
    "plt.xlabel('Normalisiert (0-1)')\n",
    "plt.ylabel('Häufigkeit')\n",
    "\n",
    "# Robust Scaler\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(df_normalized[f'robust_{example_feature}'], bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "plt.title(f'Robust Scaler: {example_feature}')\n",
    "plt.xlabel('Robust normalisiert')\n",
    "plt.ylabel('Häufigkeit')\n",
    "\n",
    "# Power Transformer (falls verfügbar)\n",
    "if f'power_{example_feature}' in df_normalized.columns:\n",
    "    plt.subplot(2, 3, 5)\n",
    "    plt.hist(df_normalized[f'power_{example_feature}'], bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "    plt.title(f'Power Transformer: {example_feature}')\n",
    "    plt.xlabel('Power transformiert')\n",
    "    plt.ylabel('Häufigkeit')\n",
    "\n",
    "# Boxplot-Vergleich\n",
    "plt.subplot(2, 3, 6)\n",
    "comparison_data = [\n",
    "    df_normalized[example_feature],\n",
    "    df_normalized[f'std_{example_feature}'],\n",
    "    df_normalized[f'minmax_{example_feature}'],\n",
    "    df_normalized[f'robust_{example_feature}']\n",
    "]\n",
    "comparison_labels = ['Original', 'Standard', 'MinMax', 'Robust']\n",
    "\n",
    "if f'power_{example_feature}' in df_normalized.columns:\n",
    "    comparison_data.append(df_normalized[f'power_{example_feature}'])\n",
    "    comparison_labels.append('Power')\n",
    "\n",
    "plt.boxplot(comparison_data, labels=comparison_labels)\n",
    "plt.title(f'Vergleich aller Methoden: {example_feature}')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Werte')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Korrelation zwischen Umsatz und normalisierten Features\n",
    "print(f\"\\n🔗 KORRELATION ZWISCHEN UMSATZ UND NORMALISIERTEN FEATURES:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for method in ['std', 'minmax', 'robust', 'power']:\n",
    "    method_cols = [col for col in df_normalized.columns if col.startswith(f'{method}_')]\n",
    "    if method_cols:\n",
    "        correlations = df_normalized[method_cols + ['Umsatz']].corr()['Umsatz'].abs().sort_values(ascending=False)\n",
    "        print(f\"\\n{method.upper()}-SCALER - Top 5 Korrelationen:\")\n",
    "        print(correlations.head(6))  # Top 5 + Umsatz selbst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMPFEHLUNG UND FINALE DATASET-ERSTELLUNG\n",
    "print(\"🎯 EMPFEHLUNG FÜR NORMALISIERUNGSMETHODE:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Bewertungskriterien für neuronale netze\n",
    "print(\"📋 Bewertungskriterien für Neuronale Netze:\")\n",
    "print(\"1. Ähnliche Skalierung aller Features (Mean ≈ 0, Std ≈ 1)\")\n",
    "print(\"2. Keine extremen Ausreißer\")\n",
    "print(\"3. Stabile Gradientenberechnung\")\n",
    "print(\"4. Erhaltung der Datenverteilung\")\n",
    "\n",
    "# Analysiere Stabilität der verschiedenen Scaler\n",
    "scaler_scores = {}\n",
    "\n",
    "for method in ['std', 'minmax', 'robust']:\n",
    "    method_cols = [col for col in df_normalized.columns if col.startswith(f'{method}_')]\n",
    "    if method_cols:\n",
    "        method_data = df_normalized[method_cols]\n",
    "        \n",
    "        # Bewertungskriterien berechnen\n",
    "        mean_consistency = 1 / (1 + abs(method_data.mean().std()))  # Je einheitlicher die Means, desto besser\n",
    "        std_consistency = 1 / (1 + abs(method_data.std().std()))    # Je einheitlicher die Stds, desto besser\n",
    "        range_control = 1 / (1 + (method_data.max().max() - method_data.min().min()))  # Kontrollierter Wertebereich\n",
    "        \n",
    "        total_score = (mean_consistency + std_consistency + range_control) / 3\n",
    "        scaler_scores[method] = {\n",
    "            'total': total_score,\n",
    "            'mean_consistency': mean_consistency,\n",
    "            'std_consistency': std_consistency,\n",
    "            'range_control': range_control\n",
    "        }\n",
    "\n",
    "# Beste Methode ermitteln\n",
    "best_method = max(scaler_scores.keys(), key=lambda k: scaler_scores[k]['total'])\n",
    "\n",
    "print(f\"\\n📊 BEWERTUNG DER NORMALISIERUNGSMETHODEN:\")\n",
    "for method, scores in scaler_scores.items():\n",
    "    print(f\"{method.upper()}-Scaler:\")\n",
    "    print(f\"  Gesamtscore: {scores['total']:.3f}\")\n",
    "    print(f\"  Mean-Konsistenz: {scores['mean_consistency']:.3f}\")\n",
    "    print(f\"  Std-Konsistenz: {scores['std_consistency']:.3f}\")\n",
    "    print(f\"  Wertebereich: {scores['range_control']:.3f}\")\n",
    "\n",
    "print(f\"\\n✅ EMPFEHLUNG: {best_method.upper()}-SCALER\")\n",
    "print(f\"🏆 Beste Gesamtbewertung: {scaler_scores[best_method]['total']:.3f}\")\n",
    "\n",
    "# Finales Dataset mit der besten Normalisierungsmethode erstellen\n",
    "best_method_cols = [col for col in df_normalized.columns if col.startswith(f'{best_method}_')]\n",
    "\n",
    "# Features für finales normalisiertes Dataset auswählen\n",
    "final_normalized_features = [\n",
    "    'Datum', 'Umsatz',  # Basis-Features\n",
    "    'Warengruppe', 'Warengruppe_Name', 'Wochentag', 'Jahreszeit',  # Kategorische Features\n",
    "    'ist_feiertag', 'ist_sonntag', 'ist_kiwo', 'Wettercode_fehlt'  # Binäre Features\n",
    "]\n",
    "\n",
    "# One-Hot-Features hinzufügen\n",
    "warengruppen_onehot = [col for col in df_normalized.columns if col.startswith('Warengruppe_')]\n",
    "jahreszeiten_onehot = [col for col in df_normalized.columns if col.startswith('Jahreszeit_')]\n",
    "\n",
    "# Normalisierte numerische Features der besten Methode hinzufügen\n",
    "final_features = final_normalized_features + warengruppen_onehot + jahreszeiten_onehot + best_method_cols\n",
    "\n",
    "# Nur existierende Spalten auswählen\n",
    "final_features = [col for col in final_features if col in df_normalized.columns]\n",
    "\n",
    "# Finales normalisiertes Dataset erstellen\n",
    "df_model_ready = df_normalized[final_features].copy()\n",
    "\n",
    "# Normalisierte Spalten umbenennen (Präfix entfernen für Klarheit)\n",
    "rename_dict = {}\n",
    "for col in best_method_cols:\n",
    "    original_name = col.replace(f'{best_method}_', '')\n",
    "    rename_dict[col] = f'{original_name}_normalized'\n",
    "\n",
    "df_model_ready = df_model_ready.rename(columns=rename_dict)\n",
    "\n",
    "print(f\"\\n📊 MODELL-BEREITES DATASET:\")\n",
    "print(f\"📏 Shape: {df_model_ready.shape}\")\n",
    "print(f\"🏷️ Features: {len(df_model_ready.columns)}\")\n",
    "\n",
    "print(f\"\\n🔧 NORMALISIERTE FEATURES:\")\n",
    "normalized_cols = [col for col in df_model_ready.columns if col.endswith('_normalized')]\n",
    "for col in normalized_cols:\n",
    "    stats = df_model_ready[col].describe()\n",
    "    print(f\"{col}: Mean={stats['mean']:.3f}, Std={stats['std']:.3f}, Range=[{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
    "\n",
    "# Speichere das normalisierte Dataset\n",
    "normalized_output_path = \"/workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset_normalized.csv\"\n",
    "\n",
    "try:\n",
    "    df_model_ready.to_csv(normalized_output_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\n✅ Normalisiertes Dataset gespeichert: {normalized_output_path}\")\n",
    "    \n",
    "    # Zusätzliche Info-Datei mit Normalisierungsparametern\n",
    "    normalization_info = {\n",
    "        'Methode': best_method.upper() + '-Scaler',\n",
    "        'Normalisierte_Features': [col.replace('_normalized', '') for col in normalized_cols],\n",
    "        'Original_Features': numeric_features_to_scale,\n",
    "        'Dataset_Shape': df_model_ready.shape,\n",
    "        'Empfehlung': f\"Verwende {best_method.upper()}-Scaler für optimale NN-Performance\"\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    info_path = \"/workspaces/bakery_sales_prediction/5_Datasets/normalization_info.json\"\n",
    "    with open(info_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(normalization_info, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✅ Normalisierungs-Info gespeichert: {info_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Fehler beim Speichern: {e}\")\n",
    "\n",
    "print(f\"\\n🎉 NORMALISIERUNG ABGESCHLOSSEN!\")\n",
    "print(f\"📁 Original Dataset: bakery_training_dataset.csv\")\n",
    "print(f\"📁 Normalisiertes Dataset: bakery_training_dataset_normalized.csv\")\n",
    "print(f\"🚀 Bereit für Neuronale Netze und ML-Modelle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET SPEICHERN\n",
    "print(\"💾 Speichere finalen Trainingsdatensatz...\")\n",
    "\n",
    "# Pfad für das finale Dataset\n",
    "output_path = \"/workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset.csv\"\n",
    "\n",
    "try:\n",
    "    # CSV speichern\n",
    "    df_final.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"✅ Dataset erfolgreich gespeichert: {output_path}\")\n",
    "    \n",
    "    # Zusätzlich als Excel für bessere Lesbarkeit\n",
    "    excel_path = output_path.replace('.csv', '.xlsx')\n",
    "    df_final.to_excel(excel_path, index=False)\n",
    "    print(f\"✅ Dataset auch als Excel gespeichert: {excel_path}\")\n",
    "    \n",
    "    # Feature-Liste separat speichern\n",
    "    feature_info = pd.DataFrame({\n",
    "        'Feature': df_final.columns,\n",
    "        'Typ': df_final.dtypes,\n",
    "        'Fehlende_Werte': df_final.isnull().sum(),\n",
    "        'Eindeutige_Werte': df_final.nunique()\n",
    "    })\n",
    "    \n",
    "    feature_path = \"/workspaces/bakery_sales_prediction/5_Datasets/feature_description.csv\"\n",
    "    feature_info.to_csv(feature_path, index=False)\n",
    "    print(f\"✅ Feature-Beschreibung gespeichert: {feature_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Fehler beim Speichern: {e}\")\n",
    "\n",
    "print(f\"\\n🎉 TRAININGSDATENSATZ ERFOLGREICH ERSTELLT!\")\n",
    "print(f\"📁 Hauptdatei: {output_path}\")\n",
    "print(f\"📋 {len(df_final)} Zeilen mit {len(df_final.columns)} Features\")\n",
    "print(f\"🎯 Bereit für Regression und Neuronale Netze!\")\n",
    "\n",
    "# Finale Zusammenfassung\n",
    "print(f\"\\n📊 FINALE ZUSAMMENFASSUNG:\")\n",
    "print(f\"🥐 Warengruppen: {df_final['Warengruppe'].nunique()}\")\n",
    "print(f\"📅 Tage: {df_final['Datum'].nunique()}\")\n",
    "print(f\"🌡️ Temperaturbereich: {df_final['Temperatur'].min():.1f}°C - {df_final['Temperatur'].max():.1f}°C\")\n",
    "print(f\"🎄 Feiertage: {df_final['ist_feiertag'].sum()} Tage\")\n",
    "print(f\"📅 Sonntage: {df_final['ist_sonntag'].sum()} Tage\")\n",
    "print(f\"⛵ Kieler Woche: {df_final['ist_kiwo'].sum()} Tage\")\n",
    "print(f\"💰 Durchschnittsumsatz: {df_final['Umsatz'].mean():.2f} €\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c950c",
   "metadata": {},
   "source": [
    "# 🎯 Trainingsdatensatz erfolgreich erstellt!\n",
    "\n",
    "## 📊 Was wurde erstellt:\n",
    "- **Hauptdatensatz**: `bakery_training_dataset.csv` \n",
    "- **Feature-Beschreibung**: `feature_description.csv`\n",
    "- **Excel-Version**: `bakery_training_dataset.xlsx`\n",
    "\n",
    "## 🏷️ Enthaltene Features:\n",
    "1. **Zeitfeatures**: Datum, Jahr, Monat, Tag, Wochentag\n",
    "2. **Warengruppen**: 6 Kategorien als One-Hot-Encoding\n",
    "3. **Wetterdaten**: Temperatur, Wind, Bewölkung, Wettercode-Status\n",
    "4. **Kalender-Events**: Feiertage, Sonntage, Jahreszeiten\n",
    "5. **Besondere Events**: Kieler Woche\n",
    "6. **Wirtschaftsdaten**: Preisindex für Backwaren\n",
    "7. **Zielvariable**: Umsatz in Euro\n",
    "\n",
    "## 🚀 Nächste Schritte:\n",
    "1. **Explorative Datenanalyse** (EDA) durchführen\n",
    "2. **Feature Engineering** verfeinern\n",
    "3. **Regressionsmodelle** trainieren\n",
    "4. **Neuronale Netze** implementieren\n",
    "5. **Modell-Evaluation** und Optimierung\n",
    "\n",
    "## 💡 Hinweise für das Modelltraining:\n",
    "- Normalisierung der numerischen Features empfohlen\n",
    "- Zeitreihen-Splits für Validation verwenden\n",
    "- Cross-Validation mit Zeitberücksichtigung\n",
    "- Outlier-Behandlung bei Bedarf anwenden"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
