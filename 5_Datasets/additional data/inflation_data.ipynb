{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2560686",
   "metadata": {},
   "source": [
    "# 🥖 Backwaren-Preisdaten Integration\n",
    "\n",
    "## 🎯 Ziel\n",
    "Integration der Backwaren-Preisdaten aus `Preisentwicklung_Backwaren.csv` in `data_clean.csv`\n",
    "\n",
    "## 📋 Workflow in Blöcken\n",
    "1. **Setup & Imports** - Bibliotheken laden\n",
    "2. **Daten laden** - Beide CSV-Dateien einlesen\n",
    "3. **Datenverarbeitung** - Preisdaten aufbereiten und mergen\n",
    "4. **Export** - Als neue CSV-Datei speichern\n",
    "5. **Validierung** - Qualitätskontrolle\n",
    "\n",
    "## 📁 Dateien\n",
    "- **Input**: `data_clean.csv` (Hauptdaten)\n",
    "- **Input**: `Preisentwicklung_Backwaren.csv` (Preisdaten)\n",
    "- **Output**: `data_clean_with_bakery_prices.csv` (Ergebnis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34075613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 BLOCK 1: Setup & Imports\n",
      "==================================================\n",
      "✅ Bibliotheken geladen\n",
      "📅 Aktuelles Datum: 2025-06-24 16:17\n",
      "📁 Arbeitsverzeichnis: /workspaces/bakery_sales_prediction/2_BaselineModel/data\n",
      "✅ Block 1 abgeschlossen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 📦 BLOCK 1: SETUP & IMPORTS\n",
    "print(\"📦 BLOCK 1: Setup & Imports\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"✅ Bibliotheken geladen\")\n",
    "print(\"📅 Aktuelles Datum:\", datetime.now().strftime(\"%Y-%m-%d %H:%M\"))\n",
    "print(\"📁 Arbeitsverzeichnis:\", os.getcwd())\n",
    "print(\"✅ Block 1 abgeschlossen\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71bb36bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 BLOCK 2: Daten laden und analysieren\n",
      "==================================================\n",
      "📊 Lade Hauptdaten...\n",
      "✅ Hauptdaten geladen: 9,334 Zeilen, 46 Spalten\n",
      "🥖 Lade Backwaren-Preisdaten...\n",
      "✅ Preisdaten geladen: 252 Zeilen, 5 Spalten\n",
      "\n",
      "🔍 Analyse der Hauptdaten:\n",
      "Datumsspalte: object\n",
      "Datumsbereich: 2013-07-01 bis 2018-07-31\n",
      "\n",
      "🔍 Analyse der Backwaren-Preisdaten:\n",
      "Verfügbare Spalten: ['time', '1_variable_attribute_code', '1_variable_attribute_label', 'value', 'value_unit']\n",
      "Erste 5 Zeilen der Preisdaten:\n",
      "   time 1_variable_attribute_code 1_variable_attribute_label value value_unit\n",
      "0  2013                   MONAT01                     Januar  91,6   2020=100\n",
      "1  2013                   MONAT01                     Januar    88   2020=100\n",
      "2  2013                   MONAT01                     Januar  88,8   2020=100\n",
      "3  2013                   MONAT02                    Februar  91,9   2020=100\n",
      "4  2013                   MONAT02                    Februar    88   2020=100\n",
      "\n",
      "✅ Block 2 abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "# 📈 BLOCK 2: DATEN LADEN UND ANALYSIEREN\n",
    "print(\"📈 BLOCK 2: Daten laden und analysieren\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Hauptdaten laden\n",
    "print(\"📊 Lade Hauptdaten...\")\n",
    "df_main = pd.read_csv('data_clean.csv')\n",
    "print(f\"✅ Hauptdaten geladen: {df_main.shape[0]:,} Zeilen, {df_main.shape[1]} Spalten\")\n",
    "\n",
    "# Backwaren-Preisdaten laden\n",
    "print(\"🥖 Lade Backwaren-Preisdaten...\")\n",
    "df_bakery = pd.read_csv('Preisentwicklung_Backwaren.csv', sep=';')\n",
    "print(f\"✅ Preisdaten geladen: {df_bakery.shape[0]:,} Zeilen, {df_bakery.shape[1]} Spalten\")\n",
    "\n",
    "# Datenstrukturen analysieren\n",
    "print(\"\\n🔍 Analyse der Hauptdaten:\")\n",
    "print(f\"Datumsspalte: {df_main['Datum'].dtype}\")\n",
    "print(f\"Datumsbereich: {df_main['Datum'].min()} bis {df_main['Datum'].max()}\")\n",
    "\n",
    "print(\"\\n🔍 Analyse der Backwaren-Preisdaten:\")\n",
    "print(\"Verfügbare Spalten:\", df_bakery.columns.tolist())\n",
    "print(\"Erste 5 Zeilen der Preisdaten:\")\n",
    "print(df_bakery.head())\n",
    "\n",
    "print(\"\\n✅ Block 2 abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf7492f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ BLOCK 3: Datenverarbeitung\n",
      "==================================================\n",
      "🥖 Verwende bereits geladene Backwaren-Preisdaten...\n",
      "✅ Preisdaten verfügbar: 252 Zeilen\n",
      "🔍 Verwende alle verfügbaren Preisdaten...\n",
      "✅ Alle Daten: 252 Einträge\n",
      "📅 Bereite Zeitstempel auf...\n",
      "✅ Preisdaten aufbereitet: 84 Jahr-Monat-Kombinationen\n",
      "📊 Zeitraum Preisdaten: 2013-2019\n",
      "📊 Bereite Hauptdaten vor...\n",
      "📅 Zeitraum Hauptdaten: 2013-2018\n",
      "🔗 Führe Merge durch...\n",
      "✅ Nach Merge: 9334 Zeilen\n",
      "📈 Forward Fill für fehlende Preisdaten...\n",
      "📊 Fehlende Werte: 0 → 0\n",
      "✅ Finale Daten: 9334 Zeilen, 46 Spalten\n",
      "📈 Neue Spalte hinzugefügt: VPI_Backwaren\n",
      "📊 VPI_Backwaren Statistik:\n",
      "   Min: 90.33\n",
      "   Max: 96.83\n",
      "   Median: 92.63\n",
      "\n",
      "✅ Block 3 abgeschlossen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6543/1219488465.py:63: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_merged['VPI_Backwaren'] = df_merged['VPI_Backwaren'].fillna(method='ffill')\n"
     ]
    }
   ],
   "source": [
    "# ⚙️ BLOCK 3: DATENVERARBEITUNG - PREISDATEN INTEGRIEREN\n",
    "print(\"⚙️ BLOCK 3: Datenverarbeitung\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Backwaren-Preisdaten bereits in Block 2 geladen als df_bakery\n",
    "print(\"🥖 Verwende bereits geladene Backwaren-Preisdaten...\")\n",
    "print(f\"✅ Preisdaten verfügbar: {df_bakery.shape[0]:,} Zeilen\")\n",
    "\n",
    "# 2. Alle Preisdaten verwenden (da nur eine Kategorie vorhanden)\n",
    "print(\"🔍 Verwende alle verfügbaren Preisdaten...\")\n",
    "bakery_clean = df_bakery.copy()\n",
    "print(f\"✅ Alle Daten: {len(bakery_clean)} Einträge\")\n",
    "\n",
    "# 3. Zeitstempel und Werte aufbereiten\n",
    "print(\"📅 Bereite Zeitstempel auf...\")\n",
    "\n",
    "# Monat-Mapping\n",
    "month_map = {f'MONAT{i:02d}': i for i in range(1, 13)}\n",
    "\n",
    "# Zeitstempel erstellen\n",
    "bakery_clean['Jahr'] = bakery_clean['time']\n",
    "bakery_clean['Monat'] = bakery_clean['1_variable_attribute_code'].map(month_map)\n",
    "\n",
    "# Value-Spalte bereinigen (Komma zu Punkt)\n",
    "bakery_clean['VPI_Backwaren'] = pd.to_numeric(\n",
    "    bakery_clean['value'].astype(str).str.replace(',', '.'), \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Nur relevante Spalten behalten und nach Jahr/Monat aggregieren (Mittelwert bei mehreren Werten pro Monat)\n",
    "price_data = bakery_clean[['Jahr', 'Monat', 'VPI_Backwaren']].dropna()\n",
    "price_data = price_data.groupby(['Jahr', 'Monat'])['VPI_Backwaren'].mean().reset_index()\n",
    "price_data = price_data.sort_values(['Jahr', 'Monat']).reset_index(drop=True)\n",
    "\n",
    "print(f\"✅ Preisdaten aufbereitet: {len(price_data)} Jahr-Monat-Kombinationen\")\n",
    "print(f\"📊 Zeitraum Preisdaten: {price_data['Jahr'].min():.0f}-{price_data['Jahr'].max():.0f}\")\n",
    "\n",
    "# 4. Hauptdaten vorbereiten\n",
    "print(\"📊 Bereite Hauptdaten vor...\")\n",
    "df_main['Datum'] = pd.to_datetime(df_main['Datum'])\n",
    "df_main['Jahr'] = df_main['Datum'].dt.year\n",
    "df_main['Monat'] = df_main['Datum'].dt.month\n",
    "\n",
    "# Zeitraum der Hauptdaten\n",
    "main_start = df_main['Jahr'].min()\n",
    "main_end = df_main['Jahr'].max()\n",
    "print(f\"📅 Zeitraum Hauptdaten: {main_start}-{main_end}\")\n",
    "\n",
    "# 5. Merge durchführen\n",
    "print(\"🔗 Führe Merge durch...\")\n",
    "df_merged = pd.merge(\n",
    "    df_main, \n",
    "    price_data, \n",
    "    on=['Jahr', 'Monat'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"✅ Nach Merge: {len(df_merged)} Zeilen\")\n",
    "\n",
    "# 6. Forward Fill für fehlende Werte\n",
    "print(\"📈 Forward Fill für fehlende Preisdaten...\")\n",
    "missing_before = df_merged['VPI_Backwaren'].isnull().sum()\n",
    "df_merged['VPI_Backwaren'] = df_merged['VPI_Backwaren'].fillna(method='ffill')\n",
    "missing_after = df_merged['VPI_Backwaren'].isnull().sum()\n",
    "\n",
    "print(f\"📊 Fehlende Werte: {missing_before} → {missing_after}\")\n",
    "\n",
    "# 7. Daten außerhalb des Preis-Zeitraums anzeigen\n",
    "price_start = price_data['Jahr'].min()\n",
    "price_end = price_data['Jahr'].max()\n",
    "\n",
    "if price_start > main_start or price_end < main_end:\n",
    "    print(f\"⚠️ Zeitraum-Unterschied erkannt:\")\n",
    "    print(f\"   Preisdaten: {price_start:.0f}-{price_end:.0f}\")\n",
    "    print(f\"   Hauptdaten: {main_start}-{main_end}\")\n",
    "\n",
    "# 8. Temporäre Spalten entfernen\n",
    "df_final = df_merged.drop(columns=['Jahr', 'Monat'])\n",
    "\n",
    "print(f\"✅ Finale Daten: {len(df_final)} Zeilen, {len(df_final.columns)} Spalten\")\n",
    "print(f\"📈 Neue Spalte hinzugefügt: VPI_Backwaren\")\n",
    "\n",
    "# Kurze Statistik der neuen Spalte\n",
    "if 'VPI_Backwaren' in df_final.columns:\n",
    "    vpi_stats = df_final['VPI_Backwaren'].describe()\n",
    "    print(f\"📊 VPI_Backwaren Statistik:\")\n",
    "    print(f\"   Min: {vpi_stats['min']:.2f}\")\n",
    "    print(f\"   Max: {vpi_stats['max']:.2f}\")\n",
    "    print(f\"   Median: {vpi_stats['50%']:.2f}\")\n",
    "\n",
    "print(\"\\n✅ Block 3 abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df80d8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ BLOCK 4: Datenbereinigung\n",
      "==================================================\n",
      "🔍 Prüfe vorhandene Feiertags-Spalten...\n",
      "📊 Spalten vor Bereinigung: 46\n",
      "✅ Gefundene Spalten zum Entfernen: 5\n",
      "   • Feiertag_Ostern\n",
      "   • Feiertag_Pfingsten\n",
      "   • Feiertag_Pfingstmontag\n",
      "   • Feiertag_Silvester\n",
      "   • Feiertag_Tag der Arbeit\n",
      "🗑️ 5 Feiertags-Spalten entfernt\n",
      "⚠️ Nicht gefundene Spalten: 1\n",
      "   • Feiertag_Ostermontag\n",
      "📊 Spalten nach Bereinigung: 41\n",
      "📉 Reduzierung: 5 Spalten entfernt\n",
      "\n",
      "✅ Block 4 abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "# 🗑️ BLOCK 4: DATENBEREINIGUNG - FEIERTAGS-SPALTEN ENTFERNEN\n",
    "print(\"🗑️ BLOCK 4: Datenbereinigung\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Liste der zu entfernenden Feiertags-Spalten\n",
    "columns_to_remove = [\n",
    "    'Feiertag_Ostermontag',\n",
    "    'Feiertag_Ostern', \n",
    "    'Feiertag_Pfingsten',\n",
    "    'Feiertag_Pfingstmontag',\n",
    "    'Feiertag_Silvester',\n",
    "    'Feiertag_Tag der Arbeit'\n",
    "]\n",
    "\n",
    "print(\"🔍 Prüfe vorhandene Feiertags-Spalten...\")\n",
    "print(f\"📊 Spalten vor Bereinigung: {len(df_final.columns)}\")\n",
    "\n",
    "# Prüfen welche Spalten tatsächlich vorhanden sind\n",
    "existing_columns = [col for col in columns_to_remove if col in df_final.columns]\n",
    "missing_columns = [col for col in columns_to_remove if col not in df_final.columns]\n",
    "\n",
    "if existing_columns:\n",
    "    print(f\"✅ Gefundene Spalten zum Entfernen: {len(existing_columns)}\")\n",
    "    for col in existing_columns:\n",
    "        print(f\"   • {col}\")\n",
    "    \n",
    "    # Spalten entfernen\n",
    "    df_final = df_final.drop(columns=existing_columns)\n",
    "    print(f\"🗑️ {len(existing_columns)} Feiertags-Spalten entfernt\")\n",
    "else:\n",
    "    print(\"ℹ️ Keine der angegebenen Feiertags-Spalten gefunden\")\n",
    "\n",
    "if missing_columns:\n",
    "    print(f\"⚠️ Nicht gefundene Spalten: {len(missing_columns)}\")\n",
    "    for col in missing_columns:\n",
    "        print(f\"   • {col}\")\n",
    "\n",
    "print(f\"📊 Spalten nach Bereinigung: {len(df_final.columns)}\")\n",
    "print(f\"📉 Reduzierung: {len(existing_columns)} Spalten entfernt\")\n",
    "\n",
    "print(\"\\n✅ Block 4 abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e87cdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌦️ BLOCK 4.5: Wettercode-Bereinigung\n",
      "==================================================\n",
      "🔍 Gefundene Wettercode-Spalten: 2\n",
      "   • Wettercode\n",
      "   • Wettercode_fehlt\n",
      "\n",
      "📊 Verarbeite Spalte: Wettercode\n",
      "   • Werte '99': 2325 (24.9%)\n",
      "   • Gültige Werte: 7009\n",
      "   🔧 Führe k-nearest Imputation durch...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Imputation abgeschlossen\n",
      "   • Verbleibende NaN: 0\n",
      "   • Eindeutige Werte nach Imputation: [np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9)]...\n",
      "\n",
      "📊 Verarbeite Spalte: Wettercode_fehlt\n",
      "   • Werte '99': 0 (0.0%)\n",
      "   • Gültige Werte: 9334\n",
      "   ✅ Keine '99'-Werte gefunden\n",
      "\n",
      "📋 Zusammenfassung Wettercode-Bereinigung:\n",
      "   • Verarbeitete Spalten: 2\n",
      "   • Methode: k-nearest neighbors (k=5)\n",
      "   • Fallback: Forward/Backward Fill\n",
      "\n",
      "✅ Block 4.5 abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "# 🌦️ BLOCK 4.5: WETTERCODE-BEREINIGUNG MIT K-NEAREST IMPUTATION\n",
    "print(\"🌦️ BLOCK 4.5: Wettercode-Bereinigung\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Wettercode-Spalten identifizieren\n",
    "weather_cols = [col for col in df_final.columns if 'wettercode' in col.lower() or 'weather' in col.lower()]\n",
    "print(f\"🔍 Gefundene Wettercode-Spalten: {len(weather_cols)}\")\n",
    "for col in weather_cols:\n",
    "    print(f\"   • {col}\")\n",
    "\n",
    "if weather_cols:\n",
    "    for col in weather_cols:\n",
    "        print(f\"\\n📊 Verarbeite Spalte: {col}\")\n",
    "        \n",
    "        # Anzahl der \"99\"-Werte prüfen\n",
    "        count_99 = (df_final[col] == 99).sum()\n",
    "        total_values = len(df_final[col])\n",
    "        percentage_99 = (count_99 / total_values) * 100\n",
    "        \n",
    "        print(f\"   • Werte '99': {count_99} ({percentage_99:.1f}%)\")\n",
    "        print(f\"   • Gültige Werte: {total_values - count_99}\")\n",
    "        \n",
    "        if count_99 > 0:\n",
    "            # \"99\"-Werte als NaN markieren\n",
    "            df_final[col] = df_final[col].replace(99, np.nan)\n",
    "            \n",
    "            # Bereite Features für KNN-Imputation vor\n",
    "            # Verwende numerische Spalten als Features (ohne Datum und Target)\n",
    "            feature_cols = df_final.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            feature_cols = [c for c in feature_cols if c != col]  # Target-Spalte ausschließen\n",
    "            \n",
    "            # Erstelle DataFrame für Imputation\n",
    "            impute_data = df_final[[col] + feature_cols].copy()\n",
    "            \n",
    "            # KNN-Imputation durchführen\n",
    "            print(f\"   🔧 Führe k-nearest Imputation durch...\")\n",
    "            knn_imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
    "            \n",
    "            try:\n",
    "                # Imputation anwenden\n",
    "                imputed_values = knn_imputer.fit_transform(impute_data)\n",
    "                \n",
    "                # Nur die Target-Spalte zurückschreiben\n",
    "                df_final[col] = imputed_values[:, 0]\n",
    "                \n",
    "                # Auf ganze Zahlen runden (da Wettercode diskret ist)\n",
    "                df_final[col] = np.round(df_final[col]).astype(int)\n",
    "                \n",
    "                # Validierung nach Imputation\n",
    "                remaining_nan = df_final[col].isnull().sum()\n",
    "                print(f\"   ✅ Imputation abgeschlossen\")\n",
    "                print(f\"   • Verbleibende NaN: {remaining_nan}\")\n",
    "                \n",
    "                # Werteverteilung nach Imputation\n",
    "                unique_values = sorted(df_final[col].unique())\n",
    "                print(f\"   • Eindeutige Werte nach Imputation: {unique_values[:10]}...\")  # Erste 10 zeigen\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Fehler bei Imputation: {e}\")\n",
    "                print(f\"   🔄 Verwende Forward-Fill als Fallback...\")\n",
    "                df_final[col] = df_final[col].fillna(method='ffill').fillna(method='bfill')\n",
    "        else:\n",
    "            print(f\"   ✅ Keine '99'-Werte gefunden\")\n",
    "else:\n",
    "    print(\"ℹ️ Keine Wettercode-Spalten gefunden\")\n",
    "    # Suche nach anderen möglichen Wetterspalten\n",
    "    potential_weather = [col for col in df_final.columns if any(word in col.lower() for word in ['temp', 'rain', 'wind', 'humid', 'press'])]\n",
    "    if potential_weather:\n",
    "        print(f\"💡 Mögliche Wetter-Spalten gefunden: {potential_weather}\")\n",
    "\n",
    "# Zusammenfassung\n",
    "print(f\"\\n📋 Zusammenfassung Wettercode-Bereinigung:\")\n",
    "print(f\"   • Verarbeitete Spalten: {len(weather_cols)}\")\n",
    "print(f\"   • Methode: k-nearest neighbors (k=5)\")\n",
    "print(f\"   • Fallback: Forward/Backward Fill\")\n",
    "\n",
    "print(\"\\n✅ Block 4.5 abgeschlossen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52b8cd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 BLOCK 5: Export\n",
      "==================================================\n",
      "📁 Speichere finale Daten als: data_clean_with_bakery_sales.csv\n",
      "✅ Export erfolgreich!\n",
      "📊 Finale Datei-Statistiken:\n",
      "   • Dateiname: data_clean_with_bakery_sales.csv\n",
      "   • Dateigröße: 1,271,464 Bytes (1.21 MB)\n",
      "   • Zeilen: 9,334\n",
      "   • Spalten: 41\n",
      "\n",
      "📋 Spalten in der finalen Datei:\n",
      "    1. 📊 id\n",
      "    2. 📊 Datum\n",
      "    3. 📊 Umsatz\n",
      "    4. 📊 KielerWoche\n",
      "    5. 📊 Bewoelkung\n",
      "    6. 📊 Temperatur\n",
      "    7. 📊 Windgeschwindigkeit\n",
      "    8. 📊 Wettercode\n",
      "    9. 📊 ist_feiertag\n",
      "   10. 📊 feiertag_vortag\n",
      "   11. 📊 feiertag_folgetag\n",
      "   12. 📊 Wettercode_fehlt\n",
      "   13. 📊 Warengruppe_Brot\n",
      "   14. 📊 Warengruppe_Brötchen\n",
      "   15. 📊 Warengruppe_Croissant\n",
      "   16. 📊 Warengruppe_Konditorei\n",
      "   17. 📊 Warengruppe_Kuchen\n",
      "   18. 📊 Warengruppe_Saisonbrot\n",
      "   19. 📊 Wochentag_Monday\n",
      "   20. 📊 Wochentag_Saturday\n",
      "   21. 📊 Wochentag_Sunday\n",
      "   22. 📊 Wochentag_Thursday\n",
      "   23. 📊 Wochentag_Tuesday\n",
      "   24. 📊 Wochentag_Wednesday\n",
      "   25. 📊 Monat_2\n",
      "   26. 📊 Monat_3\n",
      "   27. 📊 Monat_4\n",
      "   28. 📊 Monat_5\n",
      "   29. 📊 Monat_6\n",
      "   30. 📊 Monat_7\n",
      "   31. 📊 Monat_8\n",
      "   32. 📊 Monat_9\n",
      "   33. 📊 Monat_10\n",
      "   34. 📊 Monat_11\n",
      "   35. 📊 Monat_12\n",
      "   36. 📊 Jahreszeit_Herbst\n",
      "   37. 📊 Jahreszeit_Sommer\n",
      "   38. 📊 Jahreszeit_Winter\n",
      "   39. 📊 Feiertag_Heiligabend\n",
      "   40. 📊 Feiertag_Kein_Feiertag\n",
      "   41. 🆕 VPI_Backwaren\n",
      "\n",
      "👀 Datenvorschau (erste 3 Zeilen):\n",
      "     Datum     Umsatz  VPI_Backwaren\n",
      "2013-07-01 148.828353      90.933333\n",
      "2013-07-01 201.198426      90.933333\n",
      "2013-07-01 317.475875      90.933333\n",
      "\n",
      "📅 Zeitraum der finalen Daten:\n",
      "   • Von: 2013-07-01 00:00:00\n",
      "   • Bis: 2018-07-31 00:00:00\n",
      "   • Tage: 9334 Datensätze\n",
      "\n",
      "✅ Block 5 abgeschlossen\n",
      "\n",
      "🎉 GESAMTER WORKFLOW ABGESCHLOSSEN!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 💾 BLOCK 5: EXPORT - FINALE DATEN SPEICHERN\n",
    "print(\"💾 BLOCK 5: Export\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Dateiname für Export definieren\n",
    "output_filename = 'data_clean_with_bakery_sales.csv'\n",
    "\n",
    "print(f\"📁 Speichere finale Daten als: {output_filename}\")\n",
    "\n",
    "try:\n",
    "    # CSV-Export durchführen\n",
    "    df_final.to_csv(output_filename, index=False)\n",
    "    \n",
    "    # Dateigröße prüfen\n",
    "    file_size = os.path.getsize(output_filename)\n",
    "    file_size_mb = file_size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"✅ Export erfolgreich!\")\n",
    "    print(f\"📊 Finale Datei-Statistiken:\")\n",
    "    print(f\"   • Dateiname: {output_filename}\")\n",
    "    print(f\"   • Dateigröße: {file_size:,} Bytes ({file_size_mb:.2f} MB)\")\n",
    "    print(f\"   • Zeilen: {len(df_final):,}\")\n",
    "    print(f\"   • Spalten: {len(df_final.columns)}\")\n",
    "    \n",
    "    # Spalten-Übersicht\n",
    "    print(f\"\\n📋 Spalten in der finalen Datei:\")\n",
    "    for i, col in enumerate(df_final.columns, 1):\n",
    "        marker = \"🆕\" if col == \"VPI_Backwaren\" else \"📊\"\n",
    "        print(f\"   {i:2d}. {marker} {col}\")\n",
    "    \n",
    "    # Datenvorschau\n",
    "    print(f\"\\n👀 Datenvorschau (erste 3 Zeilen):\")\n",
    "    preview_cols = ['Datum', 'Umsatz', 'VPI_Backwaren'] if 'VPI_Backwaren' in df_final.columns else ['Datum', 'Umsatz']\n",
    "    available_preview_cols = [col for col in preview_cols if col in df_final.columns]\n",
    "    \n",
    "    if available_preview_cols:\n",
    "        print(df_final[available_preview_cols].head(3).to_string(index=False))\n",
    "    \n",
    "    # Zeitraum-Info\n",
    "    if 'Datum' in df_final.columns:\n",
    "        print(f\"\\n📅 Zeitraum der finalen Daten:\")\n",
    "        print(f\"   • Von: {df_final['Datum'].min()}\")\n",
    "        print(f\"   • Bis: {df_final['Datum'].max()}\")\n",
    "        print(f\"   • Tage: {len(df_final)} Datensätze\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Fehler beim Export: {e}\")\n",
    "    print(\"💡 Mögliche Lösungen:\")\n",
    "    print(\"   • Schreibrechte im Ordner prüfen\")\n",
    "    print(\"   • Ausreichend Speicherplatz verfügbar?\")\n",
    "    print(\"   • Datei bereits geöffnet in Excel?\")\n",
    "\n",
    "print(\"\\n✅ Block 5 abgeschlossen\")\n",
    "print(\"\\n🎉 GESAMTER WORKFLOW ABGESCHLOSSEN!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
