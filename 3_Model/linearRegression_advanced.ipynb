{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614b1c9e",
   "metadata": {},
   "source": [
    "# Advanced Linear Regression Model f√ºr B√§ckerei Umsatzvorhersage\n",
    "\n",
    "Dieses Notebook implementiert ein erweitertes Regressionsmodell, das f√ºr jede Warengruppe ein separates LinearRegression-Modell trainiert. Dies erm√∂glicht pr√§zisere Vorhersagen durch spezifische Modellierung der Zusammenh√§nge pro Produktkategorie.\n",
    "\n",
    "## Aufgabenstellung:\n",
    "- **Datensatz**: bakery_training_dataset.csv\n",
    "- **Ziel**: Separate Vorhersagemodelle f√ºr jede Warengruppe (1-6)\n",
    "- **Zeitraum**: \n",
    "  - Training: 2013-07-01 bis 2017-07-31\n",
    "  - Validation: 2017-08-01 bis 2018-07-31\n",
    "  - Test: 2018-08-01 bis 2019-07-31\n",
    "- **Features**: Temperatur, Windgeschwindigkeit, Bew√∂lkung, Zeitvariablen, Dummies f√ºr Feiertage/Wochentage/Jahreszeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04bb4552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries erfolgreich importiert!\n"
     ]
    }
   ],
   "source": [
    "# 1. Import der erforderlichen Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries erfolgreich importiert!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477f2859",
   "metadata": {},
   "source": [
    "## üì• 1. Daten einlesen & Aufbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aee31c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Datensatz von: /workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset.csv\n",
      "‚úÖ Datei gefunden!\n",
      "‚úÖ Alle erwarteten Spalten vorhanden\n",
      "Gefundene Warengruppen: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "Datensatz Shape: (9334, 34)\n",
      "Datum Range: 2013-07-01 00:00:00 bis 2018-07-31 00:00:00\n",
      "\n",
      "Spalten: ['Datum', 'Jahr', 'Monat', 'Tag', 'Wochentag', 'Wochentag_Nr', 'Warengruppe', 'Warengruppe_Name', 'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung', 'Wettercode_fehlt', 'ist_feiertag', 'Jahreszeit', 'ist_kiwo', 'Umsatz', 'Warengruppe_Brot', 'Warengruppe_Br√∂tchen', 'Warengruppe_Croissant', 'Warengruppe_Konditorei', 'Warengruppe_Kuchen', 'Warengruppe_Saisonbrot', 'Jahreszeit_Winter', 'Jahreszeit_Fr√ºhling', 'Jahreszeit_Sommer', 'Jahreszeit_Herbst', 'Wochentag_Nr.1', 'Wochentag_Monday', 'Wochentag_Tuesday', 'Wochentag_Wednesday', 'Wochentag_Thursday', 'Wochentag_Friday', 'Wochentag_Saturday', 'Wochentag_Sunday']\n",
      "\n",
      "Erste 3 Zeilen:\n",
      "       Datum  Jahr  Monat  Tag  Wochentag  Wochentag_Nr  Warengruppe  \\\n",
      "0 2013-07-01  2013      7    1     Monday             0            1   \n",
      "1 2013-07-02  2013      7    2    Tuesday             1            1   \n",
      "2 2013-07-03  2013      7    3  Wednesday             2            1   \n",
      "\n",
      "  Warengruppe_Name  Temperatur  Windgeschwindigkeit  ...  Jahreszeit_Sommer  \\\n",
      "0             Brot     17.8375                 15.0  ...                  1   \n",
      "1             Brot     17.3125                 10.0  ...                  1   \n",
      "2             Brot     21.0750                  6.0  ...                  1   \n",
      "\n",
      "   Jahreszeit_Herbst  Wochentag_Nr.1 Wochentag_Monday  Wochentag_Tuesday  \\\n",
      "0                  0               0                1                  0   \n",
      "1                  0               1                0                  1   \n",
      "2                  0               2                0                  0   \n",
      "\n",
      "   Wochentag_Wednesday  Wochentag_Thursday  Wochentag_Friday  \\\n",
      "0                    0                   0                 0   \n",
      "1                    0                   0                 0   \n",
      "2                    1                   0                 0   \n",
      "\n",
      "   Wochentag_Saturday  Wochentag_Sunday  \n",
      "0                   0                 0  \n",
      "1                   0                 0  \n",
      "2                   0                 0  \n",
      "\n",
      "[3 rows x 34 columns]\n",
      "\n",
      "Fehlende Werte:\n",
      "Datum                  0\n",
      "Jahr                   0\n",
      "Monat                  0\n",
      "Tag                    0\n",
      "Wochentag              0\n",
      "Wochentag_Nr           0\n",
      "Warengruppe            0\n",
      "Warengruppe_Name       0\n",
      "Temperatur             0\n",
      "Windgeschwindigkeit    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Datensatz einlesen\n",
    "dataset_path = '/workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset.csv'\n",
    "print(f\"Lade Datensatz von: {dataset_path}\")\n",
    "\n",
    "# Pr√ºfe, ob der Datensatz existiert\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"‚úÖ Datei gefunden!\")\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    # Validiere Datensatz\n",
    "    expected_columns = ['Datum', 'Umsatz', 'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung', 'Warengruppe']\n",
    "    missing_cols = [col for col in expected_columns if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è Warnung: Folgende erwartete Spalten fehlen: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Alle erwarteten Spalten vorhanden\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Datei nicht gefunden: {dataset_path}\")\n",
    "\n",
    "# Datum als datetime parsen\n",
    "df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "\n",
    "# √úberpr√ºfe Warengruppen\n",
    "if 'Warengruppe' in df.columns:\n",
    "    warengruppen = df['Warengruppe'].unique()\n",
    "    print(f\"Gefundene Warengruppen: {sorted(warengruppen)}\")\n",
    "elif 'Warengruppe_Nr' in df.columns:\n",
    "    warengruppen = df['Warengruppe_Nr'].unique()\n",
    "    print(f\"Gefundene Warengruppen (numerisch): {sorted(warengruppen)}\")\n",
    "else:\n",
    "    # Pr√ºfe auf One-Hot-Encoded Warengruppen\n",
    "    warengruppen_cols = [col for col in df.columns if col.startswith('Warengruppe_')]\n",
    "    if warengruppen_cols:\n",
    "        print(f\"Gefundene One-Hot-Encoded Warengruppen: {warengruppen_cols}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Keine Warengruppen-Spalten gefunden!\")\n",
    "\n",
    "print(f\"Datensatz Shape: {df.shape}\")\n",
    "print(f\"Datum Range: {df['Datum'].min()} bis {df['Datum'].max()}\")\n",
    "print(f\"\\nSpalten: {list(df.columns)}\")\n",
    "print(f\"\\nErste 3 Zeilen:\")\n",
    "print(df.head(3))\n",
    "print(f\"\\nFehlende Werte:\\n{df.isnull().sum().sort_values(ascending=False)[:10]}\")  # Top 10 mit fehlenden Werten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5651f934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verwende Warengruppen-Spalte: Warengruppe_Nr\n",
      "Verf√ºgbare Warengruppen: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "\n",
      "Beispieldaten mit Warengruppen:\n",
      "       Datum  Warengruppe_Nr  Warengruppe      Umsatz\n",
      "0 2013-07-01               1            1  148.828353\n",
      "1 2013-07-02               1            1  159.793757\n",
      "2 2013-07-03               1            1  111.885594\n",
      "3 2013-07-04               1            1  168.864941\n",
      "4 2013-07-05               1            1  171.280754\n",
      "\n",
      "Verteilung der Warengruppen:\n",
      "Warengruppe\n",
      "1    1819\n",
      "2    1819\n",
      "3    1819\n",
      "5    1819\n",
      "4    1766\n",
      "6     292\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Zeitbasierte Aufteilung:\n",
      "Training: 7493 Samples (2013-07-01 00:00:00 bis 2017-07-31 00:00:00)\n",
      "Validation: 1841 Samples (2017-08-01 00:00:00 bis 2018-07-31 00:00:00)\n",
      "Test: 0 Samples (NaT bis NaT)\n",
      "\n",
      "Verf√ºgbare Warengruppen: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "\n",
      "Beispieldaten mit Warengruppen:\n",
      "       Datum  Warengruppe_Nr  Warengruppe      Umsatz\n",
      "0 2013-07-01               1            1  148.828353\n",
      "1 2013-07-02               1            1  159.793757\n",
      "2 2013-07-03               1            1  111.885594\n",
      "3 2013-07-04               1            1  168.864941\n",
      "4 2013-07-05               1            1  171.280754\n",
      "\n",
      "Verteilung der Warengruppen:\n",
      "Warengruppe\n",
      "1    1819\n",
      "2    1819\n",
      "3    1819\n",
      "5    1819\n",
      "4    1766\n",
      "6     292\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Zeitbasierte Aufteilung:\n",
      "Training: 7493 Samples (2013-07-01 00:00:00 bis 2017-07-31 00:00:00)\n",
      "Validation: 1841 Samples (2017-08-01 00:00:00 bis 2018-07-31 00:00:00)\n",
      "Test: 0 Samples (NaT bis NaT)\n"
     ]
    }
   ],
   "source": [
    "# Warengruppen-Mapping erstellen\n",
    "# Pr√ºfe, ob eine numerische oder String-basierte Warengruppen-Spalte existiert\n",
    "if 'Warengruppe_Nr' in df.columns:\n",
    "    # Numerische Warengruppe ist bereits vorhanden\n",
    "    warengruppe_col = 'Warengruppe_Nr'\n",
    "elif 'Warengruppe' in df.columns:\n",
    "    # String-basierte Warengruppe ist vorhanden - pr√ºfen, ob es numerisch konvertierbar ist\n",
    "    try:\n",
    "        # Versuche zu konvertieren, falls es sich um Zahlen als Strings handelt\n",
    "        df['Warengruppe_Nr'] = pd.to_numeric(df['Warengruppe'])\n",
    "        warengruppe_col = 'Warengruppe_Nr'\n",
    "    except:\n",
    "        # Falls Strings wie \"Brot\", \"Br√∂tchen\", etc.\n",
    "        warengruppe_mapping = {\n",
    "            'Brot': 1,\n",
    "            'Br√∂tchen': 2,\n",
    "            'Croissant': 3,\n",
    "            'Konditorei': 4,\n",
    "            'Kuchen': 5,\n",
    "            'Sonstige': 6\n",
    "        }\n",
    "        df['Warengruppe_Nr'] = df['Warengruppe'].map(warengruppe_mapping)\n",
    "        warengruppe_col = 'Warengruppe_Nr'\n",
    "else:\n",
    "    # Wenn nur One-Hot-Encoding vorliegt, konvertiere zur√ºck\n",
    "    warengruppen_cols = [col for col in df.columns if col.startswith('Warengruppe_')]\n",
    "    if warengruppen_cols:\n",
    "        # Mapping f√ºr R√ºckkonvertierung\n",
    "        warengruppen_mapping = {}\n",
    "        for i, col in enumerate(warengruppen_cols, 1):\n",
    "            warenname = col.split('_')[1]\n",
    "            warengruppen_mapping[col] = (i, warenname)\n",
    "        \n",
    "        # Erstelle Warengruppe_Nr basierend auf One-Hot-Encoding\n",
    "        df['Warengruppe_Nr'] = 0\n",
    "        df['Warengruppe'] = ''\n",
    "        \n",
    "        for col, (nr, name) in warengruppen_mapping.items():\n",
    "            mask = df[col] == 1\n",
    "            df.loc[mask, 'Warengruppe_Nr'] = nr\n",
    "            df.loc[mask, 'Warengruppe'] = name\n",
    "        \n",
    "        warengruppe_col = 'Warengruppe_Nr'\n",
    "    else:\n",
    "        raise ValueError(\"Keine Warengruppen-Information im Datensatz gefunden!\")\n",
    "\n",
    "# √úberpr√ºfe das Ergebnis\n",
    "print(f\"Verwende Warengruppen-Spalte: {warengruppe_col}\")\n",
    "print(f\"Verf√ºgbare Warengruppen: {sorted(df[warengruppe_col].unique())}\")\n",
    "\n",
    "# Erstelle Mapping f√ºr Namen und Nummern\n",
    "warengruppe_num_to_name = {\n",
    "    1: 'Brot',\n",
    "    2: 'Br√∂tchen', \n",
    "    3: 'Croissant',\n",
    "    4: 'Konditorei',\n",
    "    5: 'Kuchen',\n",
    "    6: 'Sonstige'\n",
    "}\n",
    "\n",
    "# Stelle sicher, dass wir sowohl Nr als auch Namen haben\n",
    "if 'Warengruppe' not in df.columns:\n",
    "    df['Warengruppe'] = df['Warengruppe_Nr'].map(warengruppe_num_to_name)\n",
    "\n",
    "# Zeige einige Daten an\n",
    "print(\"\\nBeispieldaten mit Warengruppen:\")\n",
    "print(df[['Datum', 'Warengruppe_Nr', 'Warengruppe', 'Umsatz']].head())\n",
    "\n",
    "# Verteilung der Warengruppen\n",
    "warengruppen_counts = df['Warengruppe'].value_counts()\n",
    "print(\"\\nVerteilung der Warengruppen:\")\n",
    "print(warengruppen_counts)\n",
    "\n",
    "# Zeitbasierter Split vorbereiten\n",
    "train_period = (df['Datum'] >= '2013-07-01') & (df['Datum'] <= '2017-07-31')\n",
    "val_period = (df['Datum'] >= '2017-08-01') & (df['Datum'] <= '2018-07-31')\n",
    "test_period = (df['Datum'] >= '2018-08-01') & (df['Datum'] <= '2019-07-31')\n",
    "\n",
    "# Kurze √úbersicht √ºber die Datensatzaufteilung\n",
    "print(\"\\nZeitbasierte Aufteilung:\")\n",
    "print(f\"Training: {df[train_period].shape[0]} Samples ({df[train_period]['Datum'].min()} bis {df[train_period]['Datum'].max()})\")\n",
    "print(f\"Validation: {df[val_period].shape[0]} Samples ({df[val_period]['Datum'].min()} bis {df[val_period]['Datum'].max()})\")\n",
    "print(f\"Test: {df[test_period].shape[0]} Samples ({df[test_period]['Datum'].min()} bis {df[test_period]['Datum'].max()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd887fa3",
   "metadata": {},
   "source": [
    "## üß† 2. Feature Engineering & Interaktionsterme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9acd4416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature-√úbersicht:\n",
      "                          Feature  Vorhanden Datentyp  Fehlende Werte\n",
      "0                      Temperatur       True  float64               0\n",
      "1             Windgeschwindigkeit       True  float64               0\n",
      "2                      Bewoelkung       True  float64               0\n",
      "3                            Jahr       True    int64               0\n",
      "4                           Monat       True    int64               0\n",
      "..                            ...        ...      ...             ...\n",
      "82  Interaktion_Bewoelkung_Herbst       True  float64               0\n",
      "83              tage_vor_feiertag       True    int64               0\n",
      "84             tage_nach_feiertag       True    int64               0\n",
      "85                 laufende_woche       True    int64               0\n",
      "86                laufender_monat       True    int32               0\n",
      "\n",
      "[87 rows x 4 columns]\n",
      "\n",
      "Gesamtzahl der Features f√ºr Modellierung: 87\n",
      "Interaktions-, Feiertagsintensit√§ts- und Trend-Features wurden erstellt f√ºr bessere Modellgenauigkeit\n"
     ]
    }
   ],
   "source": [
    "# Feature-Auswahl gem√§√ü Anforderung\n",
    "basic_features = [\n",
    "    # Numerische Features\n",
    "    'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung',\n",
    "    'Jahr', 'Monat', 'Tag', 'Wochentag_Nr',\n",
    "    # Bin√§re Features\n",
    "    'Wettercode_fehlt', 'ist_feiertag', 'ist_kiwo',\n",
    "]\n",
    "\n",
    "# Pr√ºfe, ob die grundlegenden Features existieren, ansonsten erzeuge sie\n",
    "# Zeit-Features\n",
    "if 'Jahr' not in df.columns:\n",
    "    df['Jahr'] = df['Datum'].dt.year\n",
    "if 'Monat' not in df.columns:\n",
    "    df['Monat'] = df['Datum'].dt.month\n",
    "if 'Tag' not in df.columns:\n",
    "    df['Tag'] = df['Datum'].dt.day\n",
    "if 'Wochentag_Nr' not in df.columns:\n",
    "    df['Wochentag_Nr'] = df['Datum'].dt.dayofweek + 1  # 1-7 f√ºr Montag-Sonntag\n",
    "\n",
    "# Wetterdaten-Features pr√ºfen\n",
    "for feature in ['Temperatur', 'Windgeschwindigkeit', 'Bewoelkung', 'Wettercode_fehlt', 'ist_feiertag', 'ist_kiwo']:\n",
    "    if feature not in df.columns:\n",
    "        print(f\"‚ö†Ô∏è Feature '{feature}' nicht vorhanden - wird mit 0 aufgef√ºllt\")\n",
    "        df[feature] = 0\n",
    "\n",
    "# Jahreszeit-Features\n",
    "jahreszeiten = [\n",
    "    ('Winter', 0, 2),\n",
    "    ('Fr√ºhling', 3, 5),\n",
    "    ('Sommer', 6, 8),\n",
    "    ('Herbst', 9, 11)\n",
    "]\n",
    "for jahreszeit, start_monat, end_monat in jahreszeiten:\n",
    "    # Jahreszeit als numerische Variable\n",
    "    df[f'Jahreszeit_{jahreszeit}'] = 0\n",
    "    # Saisonale Dummy-Variablen\n",
    "    df[f'Jahreszeit_{jahreszeit}_Start'] = 0\n",
    "    df[f'Jahreszeit_{jahreszeit}_Mitte'] = 0\n",
    "    df[f'Jahreszeit_{jahreszeit}_Ende'] = 0\n",
    "    \n",
    "    # Weisen Sie Werte basierend auf dem Monat zu\n",
    "    df.loc[(df['Monat'] >= start_monat) & (df['Monat'] <= end_monat), f'Jahreszeit_{jahreszeit}'] = 1\n",
    "    df.loc[(df['Monat'] == start_monat), f'Jahreszeit_{jahreszeit}_Start'] = 1\n",
    "    df.loc[(df['Monat'] == end_monat), f'Jahreszeit_{jahreszeit}_Ende'] = 1\n",
    "    df.loc[(df['Monat'] == (start_monat + end_monat) // 2), f'Jahreszeit_{jahreszeit}_Mitte'] = 1\n",
    "\n",
    "# Wochentag-Features\n",
    "wochentage = [\n",
    "    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'\n",
    "]\n",
    "for wochentag in wochentage:\n",
    "    df[f'Wochentag_{wochentag}'] = 0\n",
    "    df.loc[df['Wochentag_Nr'] == (wochentage.index(wochentag) + 1) % 7 + 1, f'Wochentag_{wochentag}'] = 1\n",
    "\n",
    "# Optionale Features\n",
    "if 'preisindex' in df.columns:\n",
    "    basic_features.append('preisindex')\n",
    "if 'preis_delta' in df.columns:\n",
    "    basic_features.append('preis_delta')\n",
    "\n",
    "# Feiertags-Intensit√§t: Tage vor/nach Feiertag\n",
    "df['tage_vor_feiertag'] = 0\n",
    "df['tage_nach_feiertag'] = 0\n",
    "feiertage = df.loc[df['ist_feiertag'] == 1, 'Datum']\n",
    "for feiertag in feiertage:\n",
    "    for i in range(1, 4):\n",
    "        mask_vor = df['Datum'] == (feiertag - pd.Timedelta(days=i))\n",
    "        mask_nach = df['Datum'] == (feiertag + pd.Timedelta(days=i))\n",
    "        df.loc[mask_vor, 'tage_vor_feiertag'] = i\n",
    "        df.loc[mask_nach, 'tage_nach_feiertag'] = i\n",
    "\n",
    "# Trend-Features\n",
    "df['laufende_woche'] = (df['Datum'] - df['Datum'].min()).dt.days // 7\n",
    "df['laufender_monat'] = (df['Datum'].dt.year - df['Datum'].dt.year.min()) * 12 + (df['Datum'].dt.month - 1)\n",
    "\n",
    "# Interaktionsterme: Warengruppe + Wochentag, Warengruppe + Temperatur, Wetter √ó Jahreszeit\n",
    "for warengruppe_nr in sorted(df['Warengruppe_Nr'].unique()):\n",
    "    warengruppe_name = warengruppe_num_to_name[warengruppe_nr]\n",
    "    # Interaktion Warengruppe √ó Temperatur\n",
    "    df[f'Interaktion_{warengruppe_name}_Temperatur'] = (df['Warengruppe_Nr'] == warengruppe_nr) * df['Temperatur']\n",
    "    # Interaktion Warengruppe √ó Wochentag\n",
    "    for wochentag in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']:\n",
    "        if f'Wochentag_{wochentag}' in df.columns:\n",
    "            df[f'Interaktion_{warengruppe_name}_{wochentag}'] = (df['Warengruppe_Nr'] == warengruppe_nr) * df[f'Wochentag_{wochentag}']\n",
    "\n",
    "# Interaktion Wetter √ó Jahreszeit\n",
    "for jahreszeit in ['Winter', 'Sommer', 'Herbst']:\n",
    "    if f'Jahreszeit_{jahreszeit}' in df.columns:\n",
    "        df[f'Interaktion_Temperatur_{jahreszeit}'] = df['Temperatur'] * df[f'Jahreszeit_{jahreszeit}']\n",
    "        df[f'Interaktion_Bewoelkung_{jahreszeit}'] = df['Bewoelkung'] * df[f'Jahreszeit_{jahreszeit}']\n",
    "\n",
    "# Feature-Liste erweitern\n",
    "extended_features = basic_features.copy()\n",
    "jahreszeit_features = [col for col in df.columns if col.startswith('Jahreszeit_')]\n",
    "wochentag_features = [col for col in df.columns if col.startswith('Wochentag_')]\n",
    "interaktions_features = [col for col in df.columns if col.startswith('Interaktion_')]\n",
    "feiertagsintensitaet_features = ['tage_vor_feiertag', 'tage_nach_feiertag']\n",
    "trend_features = ['laufende_woche', 'laufender_monat']\n",
    "\n",
    "extended_features.extend(jahreszeit_features)\n",
    "extended_features.extend(wochentag_features)\n",
    "extended_features.extend(interaktions_features)\n",
    "extended_features.extend(feiertagsintensitaet_features)\n",
    "extended_features.extend(trend_features)\n",
    "\n",
    "# Erstelle ein DataFrame zur √úberpr√ºfung der Features\n",
    "feature_check = pd.DataFrame({\n",
    "    'Feature': extended_features,\n",
    "    'Vorhanden': [feature in df.columns for feature in extended_features],\n",
    "    'Datentyp': [str(df[feature].dtype) if feature in df.columns else 'N/A' for feature in extended_features],\n",
    "    'Fehlende Werte': [df[feature].isnull().sum() if feature in df.columns else 'N/A' for feature in extended_features]\n",
    "})\n",
    "\n",
    "print(\"Feature-√úbersicht:\")\n",
    "print(feature_check)\n",
    "\n",
    "# Fehlende Features hinzuf√ºgen und mit 0 f√ºllen\n",
    "missing_features = [feature for feature in extended_features if feature not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"\\n‚ö†Ô∏è Folgende Features fehlen und werden mit 0 aufgef√ºllt: {missing_features}\")\n",
    "    for feature in missing_features:\n",
    "        df[feature] = 0\n",
    "\n",
    "# Pr√ºfe auf fehlende Werte in den Features\n",
    "null_counts = df[extended_features].isnull().sum()\n",
    "features_with_nulls = null_counts[null_counts > 0]\n",
    "if not features_with_nulls.empty:\n",
    "    print(f\"\\n‚ö†Ô∏è Features mit fehlenden Werten:\")\n",
    "    print(features_with_nulls)\n",
    "    print(\"F√ºlle fehlende Werte mit 0...\")\n",
    "    df[extended_features] = df[extended_features].fillna(0)\n",
    "\n",
    "print(f\"\\nGesamtzahl der Features f√ºr Modellierung: {len(extended_features)}\")\n",
    "print(f\"Interaktions-, Feiertagsintensit√§ts- und Trend-Features wurden erstellt f√ºr bessere Modellgenauigkeit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7debb",
   "metadata": {},
   "source": [
    "## üîÑ 3. Trainingsschleife √ºber 6 Warengruppen mit erweiterten Features\n",
    "\n",
    "In dieser Sektion trainieren wir separate lineare Regressionsmodelle f√ºr jede Warengruppe unter Einbeziehung der neuen Features:\n",
    "- Feiertags-Intensit√§t (Tage vor/nach Feiertagen)\n",
    "- Trend-Features (laufende Woche/Monat)\n",
    "- Interaktions-Features (Warengruppe √ó Wetter, Warengruppe √ó Wochentag, Wetter √ó Jahreszeit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de784a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary zur Speicherung der Modelle und Metriken\n",
    "models = {}\n",
    "metrics = {}\n",
    "\n",
    "# Liste aller Warengruppen-Nummern\n",
    "warengruppen_nummern = sorted(df['Warengruppe_Nr'].unique())\n",
    "\n",
    "# Funktion zur Anzeige eines Fortschrittsbalkens\n",
    "def print_progress_bar(iteration, total, prefix='', suffix='', length=50, fill='‚ñà'):\n",
    "    percent = (\"{0:.1f}\").format(100 * (iteration / float(total)))\n",
    "    filled_length = int(length * iteration // total)\n",
    "    bar = fill * filled_length + '-' * (length - filled_length)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end='\\r')\n",
    "    if iteration == total: \n",
    "        print()\n",
    "\n",
    "print(f\"Training von {len(warengruppen_nummern)} separaten Modellen mit erweiterten Features...\")\n",
    "\n",
    "# F√ºr jede Warengruppe ein separates Modell trainieren\n",
    "for i, warengruppe_nr in enumerate(warengruppen_nummern):\n",
    "    warengruppe_name = warengruppe_num_to_name[warengruppe_nr]\n",
    "    \n",
    "    print_progress_bar(i, len(warengruppen_nummern), prefix=f'Modell f√ºr {warengruppe_name}:', suffix='Start')\n",
    "    \n",
    "    # 1. Filtere die Daten f√ºr diese Warengruppe\n",
    "    warengruppe_mask = df['Warengruppe_Nr'] == warengruppe_nr\n",
    "    df_warengruppe = df[warengruppe_mask].copy()\n",
    "    \n",
    "    # 2. W√§hle alle erweiterten Features und Zielvariable\n",
    "    # Hier nutzen wir die extended_features Liste, die alle neuen Features enth√§lt\n",
    "    X = df_warengruppe[extended_features]\n",
    "    y = df_warengruppe['Umsatz']\n",
    "    \n",
    "    # 3. Splitte nach Datum\n",
    "    X_train = X[train_period & warengruppe_mask]\n",
    "    y_train = y[train_period & warengruppe_mask]\n",
    "    \n",
    "    X_val = X[val_period & warengruppe_mask]\n",
    "    y_val = y[val_period & warengruppe_mask]\n",
    "    \n",
    "    X_test = X[test_period & warengruppe_mask]\n",
    "    y_test = y[test_period & warengruppe_mask]\n",
    "    \n",
    "    print_progress_bar(i + 0.3, len(warengruppen_nummern), prefix=f'Modell f√ºr {warengruppe_name}:', suffix='Daten vorbereitet')\n",
    "    \n",
    "    # 4. Trainiere das Modell mit allen erweiterten Features\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print_progress_bar(i + 0.6, len(warengruppen_nummern), prefix=f'Modell f√ºr {warengruppe_name}:', suffix='Training abgeschlossen')\n",
    "    \n",
    "    # 5. Evaluiere auf dem Validierungsset\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    \n",
    "    # Negative Vorhersagen auf 0 setzen (Umsatz kann nicht negativ sein)\n",
    "    y_val_pred = np.maximum(y_val_pred, 0)\n",
    "    \n",
    "    # Speichere Modell und Metriken\n",
    "    models[warengruppe_nr] = model\n",
    "    metrics[warengruppe_nr] = {\n",
    "        'Name': warengruppe_name,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'RMSE': rmse,\n",
    "        'Training_Samples': len(X_train),\n",
    "        'Validation_Samples': len(X_val),\n",
    "        'Test_Samples': len(X_test),\n",
    "        'Mean_Umsatz_Train': y_train.mean(),\n",
    "        'Mean_Umsatz_Val': y_val.mean(),\n",
    "        'Coefficients': dict(zip(extended_features, model.coef_)),\n",
    "        'Intercept': model.intercept_\n",
    "    }\n",
    "    \n",
    "    print_progress_bar(i + 1.0, len(warengruppen_nummern), prefix=f'Modell f√ºr {warengruppe_name}:', suffix='Abgeschlossen')\n",
    "\n",
    "print(\"\\n‚úÖ Training f√ºr alle Warengruppen mit erweiterten Features abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af748f",
   "metadata": {},
   "source": [
    "## üìä Analyse der Modellperformance mit erweiterten Features\n",
    "\n",
    "Nun analysieren wir, wie die erweiterten Features (Feiertagsintensit√§t, Trends und Interaktionen) die Modellperformance beeinflusst haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b56bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle eine Funktion zur Feature-Wichtigkeitsanalyse\n",
    "def analyze_feature_importance(model, features, warengruppe_name, top_n=15):\n",
    "    \"\"\"Analysiert die Wichtigkeit von Features in einem linearen Modell\"\"\"\n",
    "    # Extrahiere Koeffizienten\n",
    "    coefs = model.coef_\n",
    "    \n",
    "    # Erstelle DataFrame mit Features und ihren Koeffizienten\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Coefficient': coefs,\n",
    "        'Abs_Coefficient': np.abs(coefs)\n",
    "    })\n",
    "    \n",
    "    # Sortiere nach absoluter Wichtigkeit\n",
    "    feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    # Zeige Top-N Features\n",
    "    print(f\"Top {top_n} wichtigste Features f√ºr {warengruppe_name}:\")\n",
    "    print(feature_importance.head(top_n)[['Feature', 'Coefficient']])\n",
    "    \n",
    "    # Visualisiere Top-N Features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['green' if x > 0 else 'red' for x in feature_importance.head(top_n)['Coefficient']]\n",
    "    sns.barplot(\n",
    "        x='Coefficient', \n",
    "        y='Feature', \n",
    "        data=feature_importance.head(top_n),\n",
    "        palette=colors\n",
    "    )\n",
    "    plt.title(f'Feature-Wichtigkeit f√ºr {warengruppe_name}')\n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_importance\n",
    "\n",
    "# Analysiere die Wichtigkeit der verschiedenen Feature-Typen f√ºr jede Warengruppe\n",
    "for warengruppe_nr, model in models.items():\n",
    "    warengruppe_name = metrics[warengruppe_nr]['Name']\n",
    "    print(f\"\\n==== Feature-Analyse f√ºr {warengruppe_name} ====\")\n",
    "    \n",
    "    # Analysiere Feature-Wichtigkeit\n",
    "    importance_df = analyze_feature_importance(model, extended_features, warengruppe_name)\n",
    "    \n",
    "    # Gruppiere nach Feature-Typen\n",
    "    feature_types = {\n",
    "        'Basis-Features': basic_features,\n",
    "        'Jahreszeit-Features': jahreszeit_features,\n",
    "        'Wochentag-Features': wochentag_features,\n",
    "        'Interaktions-Features': interaktions_features,\n",
    "        'Feiertagsintensit√§t-Features': feiertagsintensitaet_features,\n",
    "        'Trend-Features': trend_features\n",
    "    }\n",
    "    \n",
    "    # Berechne durchschnittliche absolute Wichtigkeit pro Feature-Typ\n",
    "    print(\"\\nDurchschnittliche absolute Feature-Wichtigkeit nach Kategorie:\")\n",
    "    for category, features in feature_types.items():\n",
    "        # Filtere Features die tats√§chlich im Modell verwendet wurden\n",
    "        valid_features = [f for f in features if f in importance_df['Feature'].values]\n",
    "        if valid_features:\n",
    "            avg_importance = importance_df[importance_df['Feature'].isin(valid_features)]['Abs_Coefficient'].mean()\n",
    "            print(f\"{category}: {avg_importance:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
