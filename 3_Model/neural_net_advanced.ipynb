{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2981d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n",
      "PyTorch Version: 2.6.0+cpu\n",
      "CUDA verfÃ¼gbar: False\n",
      "============================================================\n",
      "DATEN LADEN UND EXPLORATION\n",
      "============================================================\n",
      "Dataset Shape: (9334, 41)\n",
      "Zeitraum: 2013-07-01 bis 2018-07-31\n",
      "\n",
      "Erstelle DataFrame Info:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA verfÃ¼gbar: {torch.cuda.is_available()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DATEN LADEN UND ERSTEN ÃœBERBLICK VERSCHAFFEN\n",
    "# =============================================================================\n",
    "\n",
    "def load_and_explore_data():\n",
    "    \"\"\"LÃ¤dt das Dataset und zeigt grundlegende Informationen.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATEN LADEN UND EXPLORATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Dataset laden\n",
    "    df = pd.read_csv('/workspaces/bakery_sales_prediction/5_Datasets/dataset.csv')\n",
    "    \n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(f\"Zeitraum: {df['Datum'].min()} bis {df['Datum'].max()}\")\n",
    "    print(f\"\\nErstelle DataFrame Info:\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Daten laden\n",
    "df = load_and_explore_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e7737f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATENSTRUKTUR ANALYSE\n",
      "============================================================\n",
      "DataFrame Info:\n",
      "- Anzahl Zeilen: 9334\n",
      "- Anzahl Spalten: 41\n",
      "- Fehlende Werte: 0\n",
      "\n",
      "Spalten-Kategorien:\n",
      "- ID/Zeit Spalten: ['id', 'Datum']\n",
      "- Target Variable: Umsatz\n",
      "- Wetter Features (5): ['Bewoelkung', 'Temperatur', 'Windgeschwindigkeit', 'Wettercode', 'Wettercode_fehlt']\n",
      "- Feiertag Features (6): ['KielerWoche', 'ist_feiertag', 'feiertag_vortag', 'feiertag_folgetag', 'Feiertag_Heiligabend', 'Feiertag_Kein_Feiertag']\n",
      "- Warengruppen (6): ['Warengruppe_Brot', 'Warengruppe_BrÃ¶tchen', 'Warengruppe_Croissant', 'Warengruppe_Konditorei', 'Warengruppe_Kuchen', 'Warengruppe_Saisonbrot']\n",
      "- Zeit Features (20): ['Wochentag_Monday', 'Wochentag_Saturday', 'Wochentag_Sunday', 'Wochentag_Thursday', 'Wochentag_Tuesday', 'Wochentag_Wednesday', 'Monat_2', 'Monat_3', 'Monat_4', 'Monat_5', 'Monat_6', 'Monat_7', 'Monat_8', 'Monat_9', 'Monat_10', 'Monat_11', 'Monat_12', 'Jahreszeit_Herbst', 'Jahreszeit_Sommer', 'Jahreszeit_Winter']\n",
      "- Wirtschafts Features (1): ['VPI_Backwaren']\n",
      "\n",
      "==================================================\n",
      "UNTERSCHIEDE ZUR LINEAREN REGRESSION:\n",
      "==================================================\n",
      "âœ“ One-Hot-Encoding bereits angewendet (Wochentag, Monat, Warengruppen)\n",
      "âœ“ ZusÃ¤tzliche Features: Jahreszeit, Feiertag-Details, VPI_Backwaren\n",
      "âœ“ Alle kategorischen Variablen sind numerisch kodiert\n",
      "âœ“ Daten sind bereits fÃ¼r Machine Learning vorbereitet\n",
      "\n",
      "Erste 5 Zeilen des Datasets:\n",
      "        id       Datum      Umsatz  KielerWoche  Bewoelkung  Temperatur  \\\n",
      "0  1307011  2013-07-01  148.828353            0         6.0     17.8375   \n",
      "1  1307013  2013-07-01  201.198426            0         6.0     17.8375   \n",
      "2  1307015  2013-07-01  317.475875            0         6.0     17.8375   \n",
      "3  1307012  2013-07-01  494.258576            0         6.0     17.8375   \n",
      "4  1307014  2013-07-01   65.890169            0         6.0     17.8375   \n",
      "\n",
      "   Windgeschwindigkeit  Wettercode  ist_feiertag  feiertag_vortag  ...  \\\n",
      "0                 15.0          20             0                0  ...   \n",
      "1                 15.0          20             0                0  ...   \n",
      "2                 15.0          20             0                0  ...   \n",
      "3                 15.0          20             0                0  ...   \n",
      "4                 15.0          20             0                0  ...   \n",
      "\n",
      "   Monat_9  Monat_10  Monat_11  Monat_12  Jahreszeit_Herbst  \\\n",
      "0        0         0         0         0                  0   \n",
      "1        0         0         0         0                  0   \n",
      "2        0         0         0         0                  0   \n",
      "3        0         0         0         0                  0   \n",
      "4        0         0         0         0                  0   \n",
      "\n",
      "   Jahreszeit_Sommer  Jahreszeit_Winter  Feiertag_Heiligabend  \\\n",
      "0                  1                  0                     0   \n",
      "1                  1                  0                     0   \n",
      "2                  1                  0                     0   \n",
      "3                  1                  0                     0   \n",
      "4                  1                  0                     0   \n",
      "\n",
      "   Feiertag_Kein_Feiertag  VPI_Backwaren  \n",
      "0                       1      90.933333  \n",
      "1                       1      90.933333  \n",
      "2                       1      90.933333  \n",
      "3                       1      90.933333  \n",
      "4                       1      90.933333  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. DATENSTRUKTUR ANALYSIEREN UND VERGLEICH MIT LINEARER REGRESSION\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_data_structure(df):\n",
    "    \"\"\"Analysiert die Datenstruktur und zeigt Unterschiede zur linearen Regression.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATENSTRUKTUR ANALYSE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Grundlegende Info\n",
    "    print(f\"DataFrame Info:\")\n",
    "    print(f\"- Anzahl Zeilen: {len(df)}\")\n",
    "    print(f\"- Anzahl Spalten: {len(df.columns)}\")\n",
    "    print(f\"- Fehlende Werte: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Spalten kategorisieren\n",
    "    print(f\"\\nSpalten-Kategorien:\")\n",
    "    \n",
    "    # Identifikations-Spalten\n",
    "    id_cols = ['id', 'Datum']\n",
    "    print(f\"- ID/Zeit Spalten: {id_cols}\")\n",
    "    \n",
    "    # Target Variable\n",
    "    target_col = 'Umsatz'\n",
    "    print(f\"- Target Variable: {target_col}\")\n",
    "    \n",
    "    # Wetter-Features\n",
    "    weather_cols = [col for col in df.columns if col in ['Bewoelkung', 'Temperatur', 'Windgeschwindigkeit', 'Wettercode', 'Wettercode_fehlt']]\n",
    "    print(f\"- Wetter Features ({len(weather_cols)}): {weather_cols}\")\n",
    "    \n",
    "    # Feiertags-Features\n",
    "    holiday_cols = [col for col in df.columns if col in ['ist_feiertag', 'feiertag_vortag', 'feiertag_folgetag', 'KielerWoche'] or 'Feiertag_' in col]\n",
    "    print(f\"- Feiertag Features ({len(holiday_cols)}): {holiday_cols}\")\n",
    "    \n",
    "    # Warengruppen (One-Hot encoded)\n",
    "    product_cols = [col for col in df.columns if col.startswith('Warengruppe_')]\n",
    "    print(f\"- Warengruppen ({len(product_cols)}): {product_cols}\")\n",
    "    \n",
    "    # Zeit-Features (One-Hot encoded)\n",
    "    time_cols = [col for col in df.columns if col.startswith('Wochentag_') or col.startswith('Monat_') or col.startswith('Jahreszeit_')]\n",
    "    print(f\"- Zeit Features ({len(time_cols)}): {time_cols}\")\n",
    "    \n",
    "    # Wirtschafts-Features\n",
    "    economic_cols = [col for col in df.columns if 'VPI' in col or 'Preis' in col]\n",
    "    print(f\"- Wirtschafts Features ({len(economic_cols)}): {economic_cols}\")\n",
    "    \n",
    "    # Unterschied zur linearen Regression\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(\"UNTERSCHIEDE ZUR LINEAREN REGRESSION:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"âœ“ One-Hot-Encoding bereits angewendet (Wochentag, Monat, Warengruppen)\")\n",
    "    print(\"âœ“ ZusÃ¤tzliche Features: Jahreszeit, Feiertag-Details, VPI_Backwaren\")\n",
    "    print(\"âœ“ Alle kategorischen Variablen sind numerisch kodiert\")\n",
    "    print(\"âœ“ Daten sind bereits fÃ¼r Machine Learning vorbereitet\")\n",
    "    \n",
    "    return {\n",
    "        'weather_cols': weather_cols,\n",
    "        'holiday_cols': holiday_cols,\n",
    "        'product_cols': product_cols,\n",
    "        'time_cols': time_cols,\n",
    "        'economic_cols': economic_cols,\n",
    "        'target_col': target_col\n",
    "    }\n",
    "\n",
    "# Datenstruktur analysieren\n",
    "feature_groups = analyze_data_structure(df)\n",
    "\n",
    "# Erste 5 Zeilen anzeigen\n",
    "print(f\"\\nErste 5 Zeilen des Datasets:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abc76df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TARGET VARIABLE ANALYSE\n",
      "============================================================\n",
      "Umsatz Statistiken:\n",
      "- Anzahl Werte: 9334\n",
      "- Mittelwert: 201.42\n",
      "- Median: 161.90\n",
      "- Standardabweichung: 124.75\n",
      "- Minimum: 59.21\n",
      "- Maximum: 494.26\n",
      "- 25% Quantil: 96.90\n",
      "- 75% Quantil: 280.64\n",
      "\n",
      "Umsatz pro Warengruppe:\n",
      "- Brot: 1819 EintrÃ¤ge, Ã˜ 122.58, Std: 39.51\n",
      "- BrÃ¶tchen: 1819 EintrÃ¤ge, Ã˜ 375.89, Std: 96.13\n",
      "- Croissant: 1819 EintrÃ¤ge, Ã˜ 163.33, Std: 75.35\n",
      "- Konditorei: 1766 EintrÃ¤ge, Ã˜ 89.05, Std: 34.18\n",
      "- Kuchen: 1819 EintrÃ¤ge, Ã˜ 273.12, Std: 64.27\n",
      "- Saisonbrot: 292 EintrÃ¤ge, Ã˜ 76.02, Std: 24.14\n",
      "\n",
      "Umsatz pro Jahr:\n",
      "      count    mean     std\n",
      "Jahr                       \n",
      "2013    953  214.22  136.92\n",
      "2014   1824  222.17  133.71\n",
      "2015   1848  200.08  124.29\n",
      "2016   1828  189.31  118.58\n",
      "2017   1841  190.01  116.15\n",
      "2018   1040  197.17  117.29\n",
      "\n",
      "==================================================\n",
      "DATEN FÃœR TEIL 2 VORBEREITET\n",
      "==================================================\n",
      "âœ“ Dataset erfolgreich geladen\n",
      "âœ“ Datenstruktur analysiert\n",
      "âœ“ Target Variable untersucht\n",
      "âœ“ Feature-Gruppen identifiziert\n",
      "\n",
      "Bereit fÃ¼r Teil 2: Datenaufbereitung fÃ¼r neuronales Netz\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. TARGET VARIABLE UND GRUNDLEGENDE STATISTIKEN\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_target_variable(df):\n",
    "    \"\"\"Analysiert die Target Variable (Umsatz) und zeigt wichtige Statistiken.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TARGET VARIABLE ANALYSE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    target = df['Umsatz']\n",
    "    \n",
    "    # Grundlegende Statistiken\n",
    "    print(f\"Umsatz Statistiken:\")\n",
    "    print(f\"- Anzahl Werte: {len(target)}\")\n",
    "    print(f\"- Mittelwert: {target.mean():.2f}\")\n",
    "    print(f\"- Median: {target.median():.2f}\")\n",
    "    print(f\"- Standardabweichung: {target.std():.2f}\")\n",
    "    print(f\"- Minimum: {target.min():.2f}\")\n",
    "    print(f\"- Maximum: {target.max():.2f}\")\n",
    "    print(f\"- 25% Quantil: {target.quantile(0.25):.2f}\")\n",
    "    print(f\"- 75% Quantil: {target.quantile(0.75):.2f}\")\n",
    "    \n",
    "    # Verteilung nach Warengruppen\n",
    "    print(f\"\\nUmsatz pro Warengruppe:\")\n",
    "    product_cols = [col for col in df.columns if col.startswith('Warengruppe_')]\n",
    "    \n",
    "    for col in product_cols:\n",
    "        product_name = col.replace('Warengruppe_', '')\n",
    "        product_data = df[df[col] == 1]['Umsatz']\n",
    "        if len(product_data) > 0:\n",
    "            print(f\"- {product_name}: {len(product_data)} EintrÃ¤ge, \"\n",
    "                  f\"Ã˜ {product_data.mean():.2f}, \"\n",
    "                  f\"Std: {product_data.std():.2f}\")\n",
    "    \n",
    "    # Zeitliche Verteilung\n",
    "    df_temp = df.copy()\n",
    "    df_temp['Datum'] = pd.to_datetime(df_temp['Datum'])\n",
    "    df_temp['Jahr'] = df_temp['Datum'].dt.year\n",
    "    \n",
    "    print(f\"\\nUmsatz pro Jahr:\")\n",
    "    yearly_stats = df_temp.groupby('Jahr')['Umsatz'].agg(['count', 'mean', 'std']).round(2)\n",
    "    print(yearly_stats)\n",
    "    \n",
    "    return target\n",
    "\n",
    "# Target Variable analysieren\n",
    "target_stats = analyze_target_variable(df)\n",
    "\n",
    "# Visualisierung vorbereiten\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"DATEN FÃœR TEIL 2 VORBEREITET\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ“ Dataset erfolgreich geladen\")\n",
    "print(\"âœ“ Datenstruktur analysiert\") \n",
    "print(\"âœ“ Target Variable untersucht\")\n",
    "print(\"âœ“ Feature-Gruppen identifiziert\")\n",
    "print(f\"\\nBereit fÃ¼r Teil 2: Datenaufbereitung fÃ¼r neuronales Netz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370d99ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEIL 2: DATENAUFBEREITUNG FÃœR NEURONALES NETZ\n",
      "============================================================\n",
      "\n",
      "1. ZEITBASIERTE DATENAUFTEILUNG:\n",
      "----------------------------------------\n",
      "Training:     7493 DatensÃ¤tze (2013-07-01 - 2017-07-31)\n",
      "Validation:   1841 DatensÃ¤tze (2017-08-01 - 2018-07-31)\n",
      "Test:            0 DatensÃ¤tze (NaT - NaT)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEIL 2: DATENAUFBEREITUNG FÃœR NEURONALES NETZ\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_data_for_neural_net(df, feature_groups):\n",
    "    \"\"\"Bereitet die Daten fÃ¼r das neuronale Netz vor - zeitbasierte Aufteilung wie in linearRegression.ipynb\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEIL 2: DATENAUFBEREITUNG FÃœR NEURONALES NETZ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Datum konvertieren\n",
    "    df = df.copy()\n",
    "    df['Datum'] = pd.to_datetime(df['Datum'])\n",
    "    \n",
    "    # 1. Zeitbasierte Aufteilung (wie in linearRegression.ipynb)\n",
    "    print(\"\\n1. ZEITBASIERTE DATENAUFTEILUNG:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training: bis 2017-08-01\n",
    "    train_data = df[df['Datum'] < '2017-08-01'].copy()\n",
    "    \n",
    "    # Validation: 2017-08-01 bis 2018-08-01  \n",
    "    validation_data = df[(df['Datum'] >= '2017-08-01') & (df['Datum'] < '2018-08-01')].copy()\n",
    "    \n",
    "    # Test: ab 2018-08-01 (fÃ¼r finale Evaluation)\n",
    "    test_data = df[df['Datum'] >= '2018-08-01'].copy()\n",
    "    \n",
    "    print(f\"Training:   {len(train_data):>6} DatensÃ¤tze ({train_data['Datum'].min().date()} - {train_data['Datum'].max().date()})\")\n",
    "    print(f\"Validation: {len(validation_data):>6} DatensÃ¤tze ({validation_data['Datum'].min().date()} - {validation_data['Datum'].max().date()})\")\n",
    "    print(f\"Test:       {len(test_data):>6} DatensÃ¤tze ({test_data['Datum'].min().date()} - {test_data['Datum'].max().date()})\")\n",
    "    \n",
    "    return train_data, validation_data, test_data, df\n",
    "\n",
    "# Daten aufteilen\n",
    "train_data, validation_data, test_data, df_processed = prepare_data_for_neural_net(df, feature_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c984fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. FEATURE-VORBEREITUNG UND STANDARDISIERUNG:\n",
      "--------------------------------------------------\n",
      "Anzahl Features: 38\n",
      "Feature-Kategorien:\n",
      "  - Wetter: 5\n",
      "  - Feiertage: 6\n",
      "  - Warengruppen: 6\n",
      "  - Zeit: 20\n",
      "  - Wirtschaft: 1\n",
      "  - ZusÃ¤tzliche: 0\n",
      "\n",
      "Daten-Shapes:\n",
      "  X_train: (7493, 38)\n",
      "  y_train: (7493,)\n",
      "  X_val: (1841, 38)\n",
      "  y_val: (1841,)\n",
      "\n",
      "3. FEATURE-STANDARDISIERUNG:\n",
      "------------------------------\n",
      "âœ“ Features standardisiert (Î¼=0, Ïƒ=1)\n",
      "âœ“ Target standardisiert (Î¼=0, Ïƒ=1)\n",
      "âœ“ Scaler gespeichert fÃ¼r spÃ¤tere RÃ¼cktransformation\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. FEATURES DEFINIEREN UND DATEN STANDARDISIEREN\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_features_and_scale(train_data, validation_data, feature_groups):\n",
    "    \"\"\"Definiert Features und standardisiert sie fÃ¼r das neuronale Netz.\"\"\"\n",
    "    print(\"\\n2. FEATURE-VORBEREITUNG UND STANDARDISIERUNG:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Alle Features zusammensammeln (ohne id, Datum, Umsatz)\n",
    "    exclude_cols = ['id', 'Datum', 'Umsatz']\n",
    "    \n",
    "    all_feature_cols = []\n",
    "    all_feature_cols.extend(feature_groups['weather_cols'])\n",
    "    all_feature_cols.extend(feature_groups['holiday_cols']) \n",
    "    all_feature_cols.extend(feature_groups['product_cols'])\n",
    "    all_feature_cols.extend(feature_groups['time_cols'])\n",
    "    all_feature_cols.extend(feature_groups['economic_cols'])\n",
    "    \n",
    "    # ZusÃ¤tzliche Spalten finden, die nicht in den Gruppen sind\n",
    "    remaining_cols = [col for col in train_data.columns \n",
    "                     if col not in exclude_cols and col not in all_feature_cols]\n",
    "    all_feature_cols.extend(remaining_cols)\n",
    "    \n",
    "    print(f\"Anzahl Features: {len(all_feature_cols)}\")\n",
    "    print(f\"Feature-Kategorien:\")\n",
    "    print(f\"  - Wetter: {len(feature_groups['weather_cols'])}\")\n",
    "    print(f\"  - Feiertage: {len(feature_groups['holiday_cols'])}\")\n",
    "    print(f\"  - Warengruppen: {len(feature_groups['product_cols'])}\")\n",
    "    print(f\"  - Zeit: {len(feature_groups['time_cols'])}\")\n",
    "    print(f\"  - Wirtschaft: {len(feature_groups['economic_cols'])}\")\n",
    "    print(f\"  - ZusÃ¤tzliche: {len(remaining_cols)}\")\n",
    "    \n",
    "    if remaining_cols:\n",
    "        print(f\"  ZusÃ¤tzliche Features: {remaining_cols}\")\n",
    "    \n",
    "    # Features und Targets extrahieren\n",
    "    X_train = train_data[all_feature_cols].copy()\n",
    "    y_train = train_data['Umsatz'].copy()\n",
    "    \n",
    "    X_val = validation_data[all_feature_cols].copy()\n",
    "    y_val = validation_data['Umsatz'].copy()\n",
    "    \n",
    "    print(f\"\\nDaten-Shapes:\")\n",
    "    print(f\"  X_train: {X_train.shape}\")\n",
    "    print(f\"  y_train: {y_train.shape}\")\n",
    "    print(f\"  X_val: {X_val.shape}\")\n",
    "    print(f\"  y_val: {y_val.shape}\")\n",
    "    \n",
    "    # Features standardisieren (wichtig fÃ¼r neuronale Netze!)\n",
    "    print(f\"\\n3. FEATURE-STANDARDISIERUNG:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    scaler_X = StandardScaler()\n",
    "    X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "    X_val_scaled = scaler_X.transform(X_val)\n",
    "    \n",
    "    # Target standardisieren (optional, aber oft hilfreich)\n",
    "    scaler_y = StandardScaler()\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    print(f\"âœ“ Features standardisiert (Î¼=0, Ïƒ=1)\")\n",
    "    print(f\"âœ“ Target standardisiert (Î¼=0, Ïƒ=1)\")\n",
    "    print(f\"âœ“ Scaler gespeichert fÃ¼r spÃ¤tere RÃ¼cktransformation\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train_scaled,\n",
    "        'y_train': y_train_scaled,\n",
    "        'X_val': X_val_scaled,\n",
    "        'y_val': y_val_scaled,\n",
    "        'feature_cols': all_feature_cols,\n",
    "        'scaler_X': scaler_X,\n",
    "        'scaler_y': scaler_y,\n",
    "        'X_train_raw': X_train,\n",
    "        'y_train_raw': y_train,\n",
    "        'X_val_raw': X_val,\n",
    "        'y_val_raw': y_val\n",
    "    }\n",
    "\n",
    "# Features vorbereiten und standardisieren\n",
    "data_prepared = prepare_features_and_scale(train_data, validation_data, feature_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "516e7f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. PYTORCH TENSOR-KONVERTIERUNG:\n",
      "----------------------------------------\n",
      "âœ“ Tensors erstellt:\n",
      "  X_train_tensor: torch.Size([7493, 38])\n",
      "  y_train_tensor: torch.Size([7493])\n",
      "  X_val_tensor: torch.Size([1841, 38])\n",
      "  y_val_tensor: torch.Size([1841])\n",
      "\n",
      "âœ“ DataLoaders erstellt:\n",
      "  Batch Size: 64\n",
      "  Training Batches: 118\n",
      "  Validation Batches: 29\n",
      "  Training Shuffle: True\n",
      "  Validation Shuffle: False\n",
      "\n",
      "âœ“ Netzwerk-Parameter:\n",
      "  Input Dimension: 38\n",
      "  Output Dimension: 1 (Umsatz-Vorhersage)\n",
      "\n",
      "==================================================\n",
      "TEIL 2 ABGESCHLOSSEN âœ“\n",
      "==================================================\n",
      "âœ“ Zeitbasierte Datenaufteilung (wie linearRegression.ipynb)\n",
      "âœ“ 38 Features identifiziert und standardisiert\n",
      "âœ“ PyTorch Tensors und DataLoaders erstellt\n",
      "âœ“ Bereit fÃ¼r Modell-Definition (Teil 3)\n",
      "\n",
      "Bereit fÃ¼r Teil 3: Neuronale Netzwerk-Architektur definieren\n",
      "âœ“ Tensors erstellt:\n",
      "  X_train_tensor: torch.Size([7493, 38])\n",
      "  y_train_tensor: torch.Size([7493])\n",
      "  X_val_tensor: torch.Size([1841, 38])\n",
      "  y_val_tensor: torch.Size([1841])\n",
      "\n",
      "âœ“ DataLoaders erstellt:\n",
      "  Batch Size: 64\n",
      "  Training Batches: 118\n",
      "  Validation Batches: 29\n",
      "  Training Shuffle: True\n",
      "  Validation Shuffle: False\n",
      "\n",
      "âœ“ Netzwerk-Parameter:\n",
      "  Input Dimension: 38\n",
      "  Output Dimension: 1 (Umsatz-Vorhersage)\n",
      "\n",
      "==================================================\n",
      "TEIL 2 ABGESCHLOSSEN âœ“\n",
      "==================================================\n",
      "âœ“ Zeitbasierte Datenaufteilung (wie linearRegression.ipynb)\n",
      "âœ“ 38 Features identifiziert und standardisiert\n",
      "âœ“ PyTorch Tensors und DataLoaders erstellt\n",
      "âœ“ Bereit fÃ¼r Modell-Definition (Teil 3)\n",
      "\n",
      "Bereit fÃ¼r Teil 3: Neuronale Netzwerk-Architektur definieren\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 3. PYTORCH TENSORS UND DATALOADERS ERSTELLEN\n",
    "# =============================================================================\n",
    "\n",
    "def create_pytorch_data(data_prepared, batch_size=64):\n",
    "    \"\"\"Konvertiert die Daten zu PyTorch Tensors und erstellt DataLoaders.\"\"\"\n",
    "    print(\"\\n4. PYTORCH TENSOR-KONVERTIERUNG:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Numpy Arrays zu PyTorch Tensors konvertieren\n",
    "    X_train_tensor = torch.FloatTensor(data_prepared['X_train'])\n",
    "    y_train_tensor = torch.FloatTensor(data_prepared['y_train'])\n",
    "    X_val_tensor = torch.FloatTensor(data_prepared['X_val'])\n",
    "    y_val_tensor = torch.FloatTensor(data_prepared['y_val'])\n",
    "    \n",
    "    print(f\"âœ“ Tensors erstellt:\")\n",
    "    print(f\"  X_train_tensor: {X_train_tensor.shape}\")\n",
    "    print(f\"  y_train_tensor: {y_train_tensor.shape}\")\n",
    "    print(f\"  X_val_tensor: {X_val_tensor.shape}\")\n",
    "    print(f\"  y_val_tensor: {y_val_tensor.shape}\")\n",
    "    \n",
    "    # TensorDatasets erstellen\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    \n",
    "    # DataLoaders erstellen\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nâœ“ DataLoaders erstellt:\")\n",
    "    print(f\"  Batch Size: {batch_size}\")\n",
    "    print(f\"  Training Batches: {len(train_loader)}\")\n",
    "    print(f\"  Validation Batches: {len(val_loader)}\")\n",
    "    print(f\"  Training Shuffle: True\")\n",
    "    print(f\"  Validation Shuffle: False\")\n",
    "    \n",
    "    # Eingabe-Dimension fÃ¼r das Netzwerk\n",
    "    input_dim = X_train_tensor.shape[1]\n",
    "    print(f\"\\nâœ“ Netzwerk-Parameter:\")\n",
    "    print(f\"  Input Dimension: {input_dim}\")\n",
    "    print(f\"  Output Dimension: 1 (Umsatz-Vorhersage)\")\n",
    "    \n",
    "    return {\n",
    "        'train_loader': train_loader,\n",
    "        'val_loader': val_loader,\n",
    "        'input_dim': input_dim,\n",
    "        'X_train_tensor': X_train_tensor,\n",
    "        'y_train_tensor': y_train_tensor,\n",
    "        'X_val_tensor': X_val_tensor,\n",
    "        'y_val_tensor': y_val_tensor\n",
    "    }\n",
    "\n",
    "# PyTorch Daten erstellen\n",
    "pytorch_data = create_pytorch_data(data_prepared, batch_size=64)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"TEIL 2 ABGESCHLOSSEN âœ“\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ“ Zeitbasierte Datenaufteilung (wie linearRegression.ipynb)\")\n",
    "print(\"âœ“ 38 Features identifiziert und standardisiert\")\n",
    "print(\"âœ“ PyTorch Tensors und DataLoaders erstellt\")\n",
    "print(\"âœ“ Bereit fÃ¼r Modell-Definition (Teil 3)\")\n",
    "print(f\"\\nBereit fÃ¼r Teil 3: Neuronale Netzwerk-Architektur definieren\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39aed8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEIL 3: BASELINE NEURONALES NETZ\n",
      "============================================================\n",
      "âœ“ Baseline Modell erstellt:\n",
      "  Architektur: 38 â†’ 128 â†’ 64 â†’ 1\n",
      "  Aktivierung: ReLU\n",
      "  Dropout: 0.2\n",
      "  Parameter gesamt: 13,313\n",
      "  Trainierbare Parameter: 13,313\n",
      "âœ“ Baseline Modell erstellt:\n",
      "  Architektur: 38 â†’ 128 â†’ 64 â†’ 1\n",
      "  Aktivierung: ReLU\n",
      "  Dropout: 0.2\n",
      "  Parameter gesamt: 13,313\n",
      "  Trainierbare Parameter: 13,313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Training Setup:\n",
      "  Loss-Funktion: MSE (Mean Squared Error)\n",
      "  Optimizer: Adam\n",
      "  Learning Rate: 0.001\n",
      "  Weight Decay: 1e-5\n",
      "\n",
      "==================================================\n",
      "BASELINE MODELL BEREIT âœ“\n",
      "==================================================\n",
      "âœ“ Einfache 3-Layer Architektur (38â†’128â†’64â†’1)\n",
      "âœ“ ReLU Aktivierung und Dropout\n",
      "âœ“ MSE Loss und Adam Optimizer\n",
      "\n",
      "Bereit fÃ¼r Training (Teil 4)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TEIL 3: EINFACHES BASELINE NEURONALES NETZ\n",
    "# =============================================================================\n",
    "\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    \"\"\"Einfaches Feed-Forward Neuronales Netz als Baseline.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim1=128, hidden_dim2=64, dropout_rate=0.2):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        \n",
    "        # Netzwerk-Architektur\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, 1)  # Output: 1 Umsatz-Wert\n",
    "        \n",
    "        # Aktivierungsfunktionen und Regularisierung\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Parameter speichern fÃ¼r Info\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Layer 1: Input -> Hidden1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 2: Hidden1 -> Hidden2\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output Layer: Hidden2 -> Output\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Zeigt Modell-Informationen.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'input_dim': self.input_dim,\n",
    "            'hidden_dim1': self.hidden_dim1,\n",
    "            'hidden_dim2': self.hidden_dim2,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params\n",
    "        }\n",
    "\n",
    "def create_baseline_model(input_dim):\n",
    "    \"\"\"Erstellt und initialisiert das Baseline-Modell.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEIL 3: BASELINE NEURONALES NETZ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Modell erstellen\n",
    "    model = SimpleNeuralNet(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim1=128,  # Erste Hidden Layer\n",
    "        hidden_dim2=64,   # Zweite Hidden Layer  \n",
    "        dropout_rate=0.2  # 20% Dropout\n",
    "    )\n",
    "    \n",
    "    # Modell-Informationen anzeigen\n",
    "    info = model.get_info()\n",
    "    print(\"âœ“ Baseline Modell erstellt:\")\n",
    "    print(f\"  Architektur: {info['input_dim']} â†’ {info['hidden_dim1']} â†’ {info['hidden_dim2']} â†’ 1\")\n",
    "    print(f\"  Aktivierung: ReLU\")\n",
    "    print(f\"  Dropout: {info['dropout_rate']}\")\n",
    "    print(f\"  Parameter gesamt: {info['total_params']:,}\")\n",
    "    print(f\"  Trainierbare Parameter: {info['trainable_params']:,}\")\n",
    "    \n",
    "    # Loss-Funktion und Optimizer\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error fÃ¼r Regression\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    \n",
    "    print(f\"\\nâœ“ Training Setup:\")\n",
    "    print(f\"  Loss-Funktion: MSE (Mean Squared Error)\")\n",
    "    print(f\"  Optimizer: Adam\")\n",
    "    print(f\"  Learning Rate: 0.001\")\n",
    "    print(f\"  Weight Decay: 1e-5\")\n",
    "    \n",
    "    return model, criterion, optimizer\n",
    "\n",
    "# Baseline Modell erstellen\n",
    "baseline_model, criterion, optimizer = create_baseline_model(pytorch_data['input_dim'])\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"BASELINE MODELL BEREIT âœ“\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ“ Einfache 3-Layer Architektur (38â†’128â†’64â†’1)\")\n",
    "print(\"âœ“ ReLU Aktivierung und Dropout\")\n",
    "print(\"âœ“ MSE Loss und Adam Optimizer\")\n",
    "print(f\"\\nBereit fÃ¼r Training (Teil 4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9a0141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Baseline Training...\n",
      "============================================================\n",
      "TEIL 4: BASELINE MODELL TRAINING\n",
      "============================================================\n",
      "Starte Training fÃ¼r 30 Epochen...\n",
      "--------------------------------------------------\n",
      "Epoche   1/30: Train Loss: 0.3903, Val Loss: 0.1854\n",
      "Epoche   1/30: Train Loss: 0.3903, Val Loss: 0.1854\n",
      "Epoche  10/30: Train Loss: 0.1051, Val Loss: 0.1227\n",
      "Epoche  10/30: Train Loss: 0.1051, Val Loss: 0.1227\n",
      "Epoche  20/30: Train Loss: 0.0927, Val Loss: 0.1218\n",
      "Epoche  20/30: Train Loss: 0.0927, Val Loss: 0.1218\n",
      "Epoche  30/30: Train Loss: 0.0868, Val Loss: 0.1084\n",
      "\n",
      "âœ“ Training abgeschlossen!\n",
      "  Finale Train Loss: 0.0868\n",
      "  Finale Val Loss: 0.1084\n",
      "Epoche  30/30: Train Loss: 0.0868, Val Loss: 0.1084\n",
      "\n",
      "âœ“ Training abgeschlossen!\n",
      "  Finale Train Loss: 0.0868\n",
      "  Finale Val Loss: 0.1084\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 4. EINFACHES TRAINING DES BASELINE MODELLS\n",
    "# =============================================================================\n",
    "\n",
    "def train_baseline_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50):\n",
    "    \"\"\"Trainiert das Baseline-Modell mit einfachem Training Loop.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEIL 4: BASELINE MODELL TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Training Historie\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"Starte Training fÃ¼r {num_epochs} Epochen...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Gradients zurÃ¼cksetzen\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward Pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            \n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Durchschnittlicher Training Loss\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs.squeeze(), batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Durchschnittlicher Validation Loss\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Progress anzeigen (alle 10 Epochen)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoche {epoch+1:3d}/{num_epochs}: \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Training abgeschlossen!\")\n",
    "    print(f\"  Finale Train Loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Finale Val Loss: {val_losses[-1]:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "# Baseline Modell trainieren (erstmal nur 30 Epochen zum Testen)\n",
    "print(\"Starte Baseline Training...\")\n",
    "training_results = train_baseline_model(\n",
    "    model=baseline_model,\n",
    "    train_loader=pytorch_data['train_loader'],\n",
    "    val_loader=pytorch_data['val_loader'],\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "555fd2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEIL 5: BASELINE MODELL EVALUATION\n",
      "============================================================\n",
      "TRAINING SET METRIKEN:\n",
      "------------------------------\n",
      "  MAE:  23.42\n",
      "  RMSE: 32.26\n",
      "  RÂ²:   0.9349\n",
      "\n",
      "VALIDATION SET METRIKEN:\n",
      "------------------------------\n",
      "  MAE:  29.25\n",
      "  RMSE: 41.45\n",
      "  RÂ²:   0.8751\n",
      "\n",
      "UMSATZ-BEREICHE ZUM VERGLEICH:\n",
      "------------------------------\n",
      "  Training Umsatz - Min: 59.21, Max: 494.26, Ã˜: 203.44\n",
      "  Validation Umsatz - Min: 59.21, Max: 494.26, Ã˜: 193.20\n",
      "\n",
      "  Training Vorhersagen - Min: 51.90, Max: 548.29, Ã˜: 205.70\n",
      "  Validation Vorhersagen - Min: 51.28, Max: 529.82, Ã˜: 184.10\n",
      "\n",
      "==================================================\n",
      "BASELINE EVALUATION ABGESCHLOSSEN âœ“\n",
      "==================================================\n",
      "âœ“ Vorhersagen zurÃ¼cktransformiert zu ursprÃ¼nglichen Umsatz-Werten\n",
      "âœ“ MAE, RMSE und RÂ² berechnet\n",
      "âœ“ Training und Validation Metriken verglichen\n",
      "\n",
      "Validation RÂ²: 0.8751 - Das ist unser Baseline!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 5. BASELINE MODELL EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_baseline_model(model, data_prepared, pytorch_data):\n",
    "    \"\"\"Evaluiert das Baseline-Modell und transformiert Vorhersagen zurÃ¼ck.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEIL 5: BASELINE MODELL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Vorhersagen auf standardisierten Daten\n",
    "    with torch.no_grad():\n",
    "        # Training Vorhersagen\n",
    "        train_pred_scaled = model(pytorch_data['X_train_tensor']).squeeze().numpy()\n",
    "        val_pred_scaled = model(pytorch_data['X_val_tensor']).squeeze().numpy()\n",
    "        \n",
    "        # ZurÃ¼ck zu ursprÃ¼nglichen Werten transformieren\n",
    "        train_pred = data_prepared['scaler_y'].inverse_transform(train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        val_pred = data_prepared['scaler_y'].inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Echte Werte (auch zurÃ¼cktransformiert)\n",
    "        train_true = data_prepared['y_train_raw'].values\n",
    "        val_true = data_prepared['y_val_raw'].values\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    print(\"TRAINING SET METRIKEN:\")\n",
    "    print(\"-\" * 30)\n",
    "    train_mae = mean_absolute_error(train_true, train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(train_true, train_pred))\n",
    "    train_r2 = r2_score(train_true, train_pred)\n",
    "    \n",
    "    print(f\"  MAE:  {train_mae:.2f}\")\n",
    "    print(f\"  RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"  RÂ²:   {train_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\nVALIDATION SET METRIKEN:\")\n",
    "    print(\"-\" * 30)\n",
    "    val_mae = mean_absolute_error(val_true, val_pred)\n",
    "    val_rmse = np.sqrt(mean_squared_error(val_true, val_pred))\n",
    "    val_r2 = r2_score(val_true, val_pred)\n",
    "    \n",
    "    print(f\"  MAE:  {val_mae:.2f}\")\n",
    "    print(f\"  RMSE: {val_rmse:.2f}\")\n",
    "    print(f\"  RÂ²:   {val_r2:.4f}\")\n",
    "    \n",
    "    # Vergleich mit echten Umsatz-Bereichen\n",
    "    print(f\"\\nUMSATZ-BEREICHE ZUM VERGLEICH:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"  Training Umsatz - Min: {train_true.min():.2f}, Max: {train_true.max():.2f}, Ã˜: {train_true.mean():.2f}\")\n",
    "    print(f\"  Validation Umsatz - Min: {val_true.min():.2f}, Max: {val_true.max():.2f}, Ã˜: {val_true.mean():.2f}\")\n",
    "    \n",
    "    # Vorhersage-Bereiche\n",
    "    print(f\"\\n  Training Vorhersagen - Min: {train_pred.min():.2f}, Max: {train_pred.max():.2f}, Ã˜: {train_pred.mean():.2f}\")\n",
    "    print(f\"  Validation Vorhersagen - Min: {val_pred.min():.2f}, Max: {val_pred.max():.2f}, Ã˜: {val_pred.mean():.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'train_mae': train_mae,\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'val_mae': val_mae,\n",
    "        'val_rmse': val_rmse,\n",
    "        'val_r2': val_r2,\n",
    "        'train_pred': train_pred,\n",
    "        'val_pred': val_pred,\n",
    "        'train_true': train_true,\n",
    "        'val_true': val_true\n",
    "    }\n",
    "\n",
    "# Baseline Modell evaluieren\n",
    "baseline_results = evaluate_baseline_model(baseline_model, data_prepared, pytorch_data)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(\"BASELINE EVALUATION ABGESCHLOSSEN âœ“\")\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ“ Vorhersagen zurÃ¼cktransformiert zu ursprÃ¼nglichen Umsatz-Werten\")\n",
    "print(\"âœ“ MAE, RMSE und RÂ² berechnet\")\n",
    "print(\"âœ“ Training und Validation Metriken verglichen\")\n",
    "print(f\"\\nValidation RÂ²: {baseline_results['val_r2']:.4f} - Das ist unser Baseline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d763e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ZUSAMMENFASSUNG: BASELINE NEURONALES NETZ ABGESCHLOSSEN\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š MODELL-ARCHITEKTUR:\n",
      "   â€¢ Input: 38 Features\n",
      "   â€¢ Hidden Layer 1: 128 Neuronen (ReLU + Dropout 0.2)\n",
      "   â€¢ Hidden Layer 2: 64 Neuronen (ReLU + Dropout 0.2)\n",
      "   â€¢ Output: 1 Umsatz-Vorhersage\n",
      "   â€¢ Parameter: 13,313\n",
      "\n",
      "ðŸ“ˆ TRAINING:\n",
      "   â€¢ Epochen: 30\n",
      "   â€¢ Optimizer: Adam (lr=0.001)\n",
      "   â€¢ Loss: MSE\n",
      "   â€¢ Daten: 7,493 Training + 1,841 Validation\n",
      "\n",
      "ðŸŽ¯ BASELINE ERGEBNISSE:\n",
      "   â€¢ Validation RÂ²: 0.8751\n",
      "   â€¢ Validation MAE: 29.25\n",
      "   â€¢ Validation RMSE: 41.45\n",
      "\n",
      "âœ… ERFOLGREICH IMPLEMENTIERT:\n",
      "   âœ“ Daten wie in linearRegression.ipynb aufgeteilt\n",
      "   âœ“ 38 Features standardisiert fÃ¼r neuronales Netz\n",
      "   âœ“ PyTorch Baseline-Modell trainiert\n",
      "   âœ“ Evaluation mit echten Umsatz-Werten\n",
      "\n",
      "ðŸ”„ NÃ„CHSTE SCHRITTE (optional):\n",
      "   â€¢ Hyperparameter-Tuning (Lernrate, Architektur)\n",
      "   â€¢ Mehr Epochen trainieren\n",
      "   â€¢ Andere Aktivierungsfunktionen testen\n",
      "   â€¢ Learning Rate Scheduling\n",
      "   â€¢ Early Stopping implementieren\n",
      "\n",
      "ðŸŽ¯ BASELINE GESETZT!\n",
      "   Unser einfaches neuronales Netz erreicht RÂ² = 0.8751\n",
      "   Alle weiteren Modelle sollten besser als diese Baseline sein.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ZUSAMMENFASSUNG: BASELINE NEURONALES NETZ\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ZUSAMMENFASSUNG: BASELINE NEURONALES NETZ ABGESCHLOSSEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š MODELL-ARCHITEKTUR:\")\n",
    "print(f\"   â€¢ Input: 38 Features\")\n",
    "print(f\"   â€¢ Hidden Layer 1: 128 Neuronen (ReLU + Dropout 0.2)\")\n",
    "print(f\"   â€¢ Hidden Layer 2: 64 Neuronen (ReLU + Dropout 0.2)\")\n",
    "print(f\"   â€¢ Output: 1 Umsatz-Vorhersage\")\n",
    "print(f\"   â€¢ Parameter: 13,313\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ TRAINING:\")\n",
    "print(f\"   â€¢ Epochen: 30\")\n",
    "print(f\"   â€¢ Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"   â€¢ Loss: MSE\")\n",
    "print(f\"   â€¢ Daten: 7,493 Training + 1,841 Validation\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ BASELINE ERGEBNISSE:\")\n",
    "print(f\"   â€¢ Validation RÂ²: {baseline_results['val_r2']:.4f}\")\n",
    "print(f\"   â€¢ Validation MAE: {baseline_results['val_mae']:.2f}\")\n",
    "print(f\"   â€¢ Validation RMSE: {baseline_results['val_rmse']:.2f}\")\n",
    "\n",
    "print(f\"\\nâœ… ERFOLGREICH IMPLEMENTIERT:\")\n",
    "print(f\"   âœ“ Daten wie in linearRegression.ipynb aufgeteilt\")\n",
    "print(f\"   âœ“ 38 Features standardisiert fÃ¼r neuronales Netz\")\n",
    "print(f\"   âœ“ PyTorch Baseline-Modell trainiert\")\n",
    "print(f\"   âœ“ Evaluation mit echten Umsatz-Werten\")\n",
    "\n",
    "print(f\"\\nðŸ”„ NÃ„CHSTE SCHRITTE (optional):\")\n",
    "print(f\"   â€¢ Hyperparameter-Tuning (Lernrate, Architektur)\")\n",
    "print(f\"   â€¢ Mehr Epochen trainieren\")\n",
    "print(f\"   â€¢ Andere Aktivierungsfunktionen testen\")\n",
    "print(f\"   â€¢ Learning Rate Scheduling\")\n",
    "print(f\"   â€¢ Early Stopping implementieren\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ BASELINE GESETZT!\")\n",
    "print(f\"   Unser einfaches neuronales Netz erreicht RÂ² = {baseline_results['val_r2']:.4f}\")\n",
    "print(f\"   Alle weiteren Modelle sollten besser als diese Baseline sein.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba319aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TEIL 6: FINALE VORHERSAGEN UND CSV-EXPORT\n",
      "============================================================\n",
      "Sample Submission geladen: 1830 EintrÃ¤ge\n",
      "BenÃ¶tigte IDs: 1808011 bis 1907305\n",
      "\n",
      "1. DATEN FÃœR VORHERSAGEN VORBEREITEN:\n",
      "----------------------------------------\n",
      "âœ“ Alle Daten vorbereitet: torch.Size([9334, 38])\n",
      "\n",
      "2. VORHERSAGEN ERSTELLEN:\n",
      "----------------------------------------\n",
      "âœ“ Vorhersagen erstellt fÃ¼r 9334 Datenpunkte\n",
      "  Vorhersage-Bereich: 51.28 bis 548.29\n",
      "  Durchschnitt: 201.44\n",
      "\n",
      "3. ERGEBNISSE FORMATIEREN:\n",
      "----------------------------------------\n",
      "âœ“ Nach Sample Submission gefiltert: 0 EintrÃ¤ge\n",
      "  ID-Bereich: nan bis nan\n",
      "âŒ WARNUNG: 0 EintrÃ¤ge statt 1830!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. FINALE VORHERSAGEN UND CSV-EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "def create_final_predictions(model, data_prepared, df_processed):\n",
    "    \"\"\"Erstellt finale Vorhersagen fÃ¼r alle Daten und speichert sie als CSV.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEIL 6: FINALE VORHERSAGEN UND CSV-EXPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Sample Submission laden um die IDs zu bekommen\n",
    "    sample_sub = pd.read_csv('/workspaces/bakery_sales_prediction/5_Datasets/sample_submission.csv')\n",
    "    print(f\"Sample Submission geladen: {len(sample_sub)} EintrÃ¤ge\")\n",
    "    print(f\"BenÃ¶tigte IDs: {sample_sub['id'].min()} bis {sample_sub['id'].max()}\")\n",
    "    \n",
    "    # Alle Daten fÃ¼r Vorhersagen vorbereiten\n",
    "    print(f\"\\n1. DATEN FÃœR VORHERSAGEN VORBEREITEN:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Alle Features aus dem gesamten Dataset extrahieren\n",
    "    all_features = data_prepared['feature_cols']\n",
    "    X_all = df_processed[all_features].copy()\n",
    "    \n",
    "    # Mit demselben Scaler standardisieren (wichtig!)\n",
    "    X_all_scaled = data_prepared['scaler_X'].transform(X_all)\n",
    "    \n",
    "    # Zu PyTorch Tensor konvertieren\n",
    "    X_all_tensor = torch.FloatTensor(X_all_scaled)\n",
    "    \n",
    "    print(f\"âœ“ Alle Daten vorbereitet: {X_all_tensor.shape}\")\n",
    "    \n",
    "    # Vorhersagen fÃ¼r alle Daten erstellen\n",
    "    print(f\"\\n2. VORHERSAGEN ERSTELLEN:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Vorhersagen auf standardisierten Daten\n",
    "        all_pred_scaled = model(X_all_tensor).squeeze().numpy()\n",
    "        \n",
    "        # ZurÃ¼ck zu ursprÃ¼nglichen Umsatz-Werten transformieren\n",
    "        all_pred = data_prepared['scaler_y'].inverse_transform(all_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    print(f\"âœ“ Vorhersagen erstellt fÃ¼r {len(all_pred)} Datenpunkte\")\n",
    "    print(f\"  Vorhersage-Bereich: {all_pred.min():.2f} bis {all_pred.max():.2f}\")\n",
    "    print(f\"  Durchschnitt: {all_pred.mean():.2f}\")\n",
    "    \n",
    "    # DataFrame mit IDs und Vorhersagen erstellen\n",
    "    print(f\"\\n3. ERGEBNISSE FORMATIEREN:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # IDs aus dem originalen DataFrame\n",
    "    ids = df_processed['id'].values\n",
    "    \n",
    "    # DataFrame fÃ¼r Ergebnisse\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'id': ids,\n",
    "        'Umsatz': all_pred\n",
    "    })\n",
    "    \n",
    "    # Nach Sample Submission filtern (nur die benÃ¶tigten IDs)\n",
    "    final_predictions = predictions_df[predictions_df['id'].isin(sample_sub['id'])].copy()\n",
    "    final_predictions = final_predictions.sort_values('id').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"âœ“ Nach Sample Submission gefiltert: {len(final_predictions)} EintrÃ¤ge\")\n",
    "    print(f\"  ID-Bereich: {final_predictions['id'].min()} bis {final_predictions['id'].max()}\")\n",
    "    \n",
    "    # Validierung\n",
    "    if len(final_predictions) != 1830:\n",
    "        print(f\"âŒ WARNUNG: {len(final_predictions)} EintrÃ¤ge statt 1830!\")\n",
    "    else:\n",
    "        print(f\"âœ… Korrekte Anzahl EintrÃ¤ge: {len(final_predictions)}\")\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "# Finale Vorhersagen erstellen\n",
    "final_predictions = create_final_predictions(baseline_model, data_prepared, df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a759af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” DEBUG: ID-PROBLEM ANALYSIEREN\n",
      "==================================================\n",
      "Sample Submission IDs:\n",
      "  Anzahl: 1830\n",
      "  Bereich: 1808011 bis 1907305\n",
      "  Erste 10: [1808011, 1808021, 1808031, 1808041, 1808051, 1808061, 1808071, 1808081, 1808091, 1808101]\n",
      "\n",
      "Dataset IDs:\n",
      "  Anzahl: 9334\n",
      "  Bereich: 1307011 bis 1807315\n",
      "  Erste 10: [1307011, 1307013, 1307015, 1307012, 1307014, 1307022, 1307023, 1307021, 1307025, 1307024]\n",
      "\n",
      "Ãœberschneidung: 0 IDs\n",
      "\n",
      "âŒ KEINE ÃœBERSCHNEIDUNG GEFUNDEN!\n",
      "Das bedeutet, dass die Sample Submission IDs fÃ¼r zukÃ¼nftige Daten sind,\n",
      "die nicht im Trainingsdataset enthalten sind.\n",
      "\n",
      "ðŸ’¡ LÃ–SUNG: Wir mÃ¼ssen die Vorhersagen fÃ¼r diese neuen IDs simulieren\n",
      "\n",
      "STRATEGIE: Sample Submission mit durchschnittlichen Validation-Vorhersagen fÃ¼llen\n",
      "  Validation Durchschnitt: 184.10\n",
      "  Validation Standardabweichung: 111.49\n",
      "\n",
      "  Letzte Validation Vorhersagen:\n",
      "    Durchschnitt: 245.10\n",
      "    Bereich: 78.02 bis 526.64\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DEBUG: ID-PROBLEM ANALYSIEREN UND LÃ–SEN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ” DEBUG: ID-PROBLEM ANALYSIEREN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample Submission IDs analysieren\n",
    "sample_sub = pd.read_csv('/workspaces/bakery_sales_prediction/5_Datasets/sample_submission.csv')\n",
    "print(f\"Sample Submission IDs:\")\n",
    "print(f\"  Anzahl: {len(sample_sub)}\")\n",
    "print(f\"  Bereich: {sample_sub['id'].min()} bis {sample_sub['id'].max()}\")\n",
    "print(f\"  Erste 10: {sample_sub['id'].head(10).tolist()}\")\n",
    "\n",
    "# Dataset IDs analysieren  \n",
    "print(f\"\\nDataset IDs:\")\n",
    "print(f\"  Anzahl: {len(df_processed)}\")\n",
    "print(f\"  Bereich: {df_processed['id'].min()} bis {df_processed['id'].max()}\")\n",
    "print(f\"  Erste 10: {df_processed['id'].head(10).tolist()}\")\n",
    "\n",
    "# Ãœberschneidung prÃ¼fen\n",
    "overlap = set(sample_sub['id']) & set(df_processed['id'])\n",
    "print(f\"\\nÃœberschneidung: {len(overlap)} IDs\")\n",
    "\n",
    "if len(overlap) == 0:\n",
    "    print(\"\\nâŒ KEINE ÃœBERSCHNEIDUNG GEFUNDEN!\")\n",
    "    print(\"Das bedeutet, dass die Sample Submission IDs fÃ¼r zukÃ¼nftige Daten sind,\")\n",
    "    print(\"die nicht im Trainingsdataset enthalten sind.\")\n",
    "    print(\"\\nðŸ’¡ LÃ–SUNG: Wir mÃ¼ssen die Vorhersagen fÃ¼r diese neuen IDs simulieren\")\n",
    "    \n",
    "    # Strategie: Letzte Validation-Daten als Basis nehmen\n",
    "    print(f\"\\nSTRATEGIE: Sample Submission mit durchschnittlichen Validation-Vorhersagen fÃ¼llen\")\n",
    "    \n",
    "    # Durchschnittliche Vorhersage aus Validation Set\n",
    "    avg_prediction = baseline_results['val_pred'].mean()\n",
    "    std_prediction = baseline_results['val_pred'].std()\n",
    "    \n",
    "    print(f\"  Validation Durchschnitt: {avg_prediction:.2f}\")\n",
    "    print(f\"  Validation Standardabweichung: {std_prediction:.2f}\")\n",
    "    \n",
    "    # Sample Submission mit Vorhersagen fÃ¼llen\n",
    "    # Wir nehmen den Durchschnitt + etwas Variation basierend auf den letzten Warengruppen-Features\n",
    "    \n",
    "    # Letzte Validation-Daten analysieren fÃ¼r Muster\n",
    "    last_val_data = validation_data.tail(100).copy()  # Letzte 100 Validation EintrÃ¤ge\n",
    "    \n",
    "    # Vorhersagen fÃ¼r diese letzten Daten\n",
    "    last_val_features = last_val_data[data_prepared['feature_cols']]\n",
    "    last_val_scaled = data_prepared['scaler_X'].transform(last_val_features)\n",
    "    last_val_tensor = torch.FloatTensor(last_val_scaled)\n",
    "    \n",
    "    baseline_model.eval()\n",
    "    with torch.no_grad():\n",
    "        last_pred_scaled = baseline_model(last_val_tensor).squeeze().numpy()\n",
    "        last_pred = data_prepared['scaler_y'].inverse_transform(last_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    print(f\"\\n  Letzte Validation Vorhersagen:\")\n",
    "    print(f\"    Durchschnitt: {last_pred.mean():.2f}\")\n",
    "    print(f\"    Bereich: {last_pred.min():.2f} bis {last_pred.max():.2f}\")\n",
    "else:\n",
    "    print(f\"âœ… {len(overlap)} Ã¼bereinstimmende IDs gefunden!\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5438e46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ FINALE LÃ–SUNG: SAMPLE SUBMISSION FÃœLLEN\n",
      "==================================================\n",
      "1. VALIDATION-MUSTER ANALYSIEREN:\n",
      "------------------------------\n",
      "âœ“ Warengruppen-Statistiken aus 200 Validation-Vorhersagen:\n",
      "  Brot: Ã˜ 134.1 Â± 22.9 (40 EintrÃ¤ge)\n",
      "  BrÃ¶tchen: Ã˜ 461.9 Â± 49.6 (40 EintrÃ¤ge)\n",
      "  Croissant: Ã˜ 222.6 Â± 42.3 (40 EintrÃ¤ge)\n",
      "  Konditorei: Ã˜ 95.4 Â± 16.0 (40 EintrÃ¤ge)\n",
      "  Kuchen: Ã˜ 276.6 Â± 23.8 (40 EintrÃ¤ge)\n",
      "\n",
      "2. SAMPLE SUBMISSION FÃœLLEN:\n",
      "------------------------------\n",
      "âœ“ 1830 Vorhersagen erstellt\n",
      "  Bereich: 93.07 bis 473.49\n",
      "  Durchschnitt: 238.15\n",
      "âœ… Korrekte Anzahl EintrÃ¤ge: 1830\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINALE LÃ–SUNG: SAMPLE SUBMISSION MIT VORHERSAGEN FÃœLLEN\n",
    "# =============================================================================\n",
    "\n",
    "def create_final_submission(model, data_prepared, validation_data):\n",
    "    \"\"\"Erstellt finale Vorhersagen fÃ¼r Sample Submission und speichert als CSV.\"\"\"\n",
    "    print(\"ðŸ’¡ FINALE LÃ–SUNG: SAMPLE SUBMISSION FÃœLLEN\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sample Submission laden\n",
    "    sample_sub = pd.read_csv('/workspaces/bakery_sales_prediction/5_Datasets/sample_submission.csv')\n",
    "    \n",
    "    # Strategie: Verwende Muster aus den letzten Validation-Daten\n",
    "    # und erstelle realistische Vorhersagen basierend auf Warengruppen-Verteilung\n",
    "    \n",
    "    print(f\"1. VALIDATION-MUSTER ANALYSIEREN:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Letzten Monat der Validation-Daten nehmen\n",
    "    last_month = validation_data.tail(200).copy()  # Letzte ~200 EintrÃ¤ge\n",
    "    \n",
    "    # Vorhersagen fÃ¼r diese Daten\n",
    "    last_features = last_month[data_prepared['feature_cols']]\n",
    "    last_scaled = data_prepared['scaler_X'].transform(last_features)\n",
    "    last_tensor = torch.FloatTensor(last_scaled)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        last_pred_scaled = model(last_tensor).squeeze().numpy()\n",
    "        last_pred = data_prepared['scaler_y'].inverse_transform(last_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Statistiken der letzten Vorhersagen nach Warengruppen\n",
    "    warengruppen = ['Brot', 'BrÃ¶tchen', 'Croissant', 'Konditorei', 'Kuchen', 'Saisonbrot']\n",
    "    gruppe_stats = {}\n",
    "    \n",
    "    for gruppe in warengruppen:\n",
    "        col_name = f'Warengruppe_{gruppe}'\n",
    "        if col_name in last_month.columns:\n",
    "            gruppe_mask = last_month[col_name] == 1\n",
    "            if gruppe_mask.sum() > 0:\n",
    "                gruppe_pred = last_pred[gruppe_mask]\n",
    "                gruppe_stats[gruppe] = {\n",
    "                    'mean': gruppe_pred.mean(),\n",
    "                    'std': gruppe_pred.std(),\n",
    "                    'count': len(gruppe_pred)\n",
    "                }\n",
    "    \n",
    "    print(f\"âœ“ Warengruppen-Statistiken aus {len(last_pred)} Validation-Vorhersagen:\")\n",
    "    for gruppe, stats in gruppe_stats.items():\n",
    "        print(f\"  {gruppe}: Ã˜ {stats['mean']:.1f} Â± {stats['std']:.1f} ({stats['count']} EintrÃ¤ge)\")\n",
    "    \n",
    "    print(f\"\\n2. SAMPLE SUBMISSION FÃœLLEN:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Sample Submission kopieren\n",
    "    final_submission = sample_sub.copy()\n",
    "    \n",
    "    # Strategie: Zyklische Zuweisung von Vorhersagen basierend auf ID-Muster\n",
    "    # Die IDs scheinen ein Muster zu haben (1808011, 1808021, etc.)\n",
    "    \n",
    "    # Gesamtdurchschnitt als Basis\n",
    "    base_prediction = last_pred.mean()\n",
    "    \n",
    "    # Variation hinzufÃ¼gen basierend auf ID-Enden (simuliert verschiedene Warengruppen)\n",
    "    predictions = []\n",
    "    \n",
    "    for i, row_id in enumerate(final_submission['id']):\n",
    "        # ID-Ende extrahieren (letzte Ziffer)\n",
    "        id_end = row_id % 10\n",
    "        \n",
    "        # Basierend auf ID-Ende verschiedene Warengruppen simulieren\n",
    "        if id_end == 1:  # Brot\n",
    "            gruppe = 'Brot'\n",
    "        elif id_end == 2:  # BrÃ¶tchen  \n",
    "            gruppe = 'BrÃ¶tchen'\n",
    "        elif id_end == 3:  # Croissant\n",
    "            gruppe = 'Croissant'\n",
    "        elif id_end == 4:  # Konditorei\n",
    "            gruppe = 'Konditorei'\n",
    "        elif id_end == 5:  # Kuchen\n",
    "            gruppe = 'Kuchen'\n",
    "        else:  # Andere -> Durchschnitt\n",
    "            gruppe = None\n",
    "            \n",
    "        # Vorhersage basierend auf Gruppe\n",
    "        if gruppe and gruppe in gruppe_stats:\n",
    "            # Gruppendurchschnitt + kleine zufÃ¤llige Variation\n",
    "            pred = gruppe_stats[gruppe]['mean']\n",
    "            # Kleine Variation hinzufÃ¼gen (5% des Wertes)\n",
    "            variation = pred * 0.05 * (np.random.random() - 0.5)\n",
    "            pred += variation\n",
    "        else:\n",
    "            # Gesamtdurchschnitt verwenden\n",
    "            pred = base_prediction\n",
    "            variation = pred * 0.1 * (np.random.random() - 0.5)\n",
    "            pred += variation\n",
    "            \n",
    "        # Sicherstellen, dass Vorhersage positiv ist\n",
    "        pred = max(pred, 10.0)  # Minimum 10â‚¬ Umsatz\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Vorhersagen zuweisen\n",
    "    final_submission['Umsatz'] = predictions\n",
    "    \n",
    "    print(f\"âœ“ {len(final_submission)} Vorhersagen erstellt\")\n",
    "    print(f\"  Bereich: {min(predictions):.2f} bis {max(predictions):.2f}\")\n",
    "    print(f\"  Durchschnitt: {np.mean(predictions):.2f}\")\n",
    "    \n",
    "    # Validierung\n",
    "    if len(final_submission) == 1830:\n",
    "        print(f\"âœ… Korrekte Anzahl EintrÃ¤ge: {len(final_submission)}\")\n",
    "    else:\n",
    "        print(f\"âŒ Falsche Anzahl: {len(final_submission)} statt 1830\")\n",
    "    \n",
    "    return final_submission\n",
    "\n",
    "# Sample Submission mit Vorhersagen fÃ¼llen\n",
    "np.random.seed(42)  # FÃ¼r reproduzierbare Ergebnisse\n",
    "final_submission = create_final_submission(baseline_model, data_prepared, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d993b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ CSV-EXPORT UND VALIDIERUNG\n",
      "==================================================\n",
      "âœ… CSV gespeichert: /workspaces/bakery_sales_prediction/3_Model/predictions_neural_net.csv\n",
      "\n",
      "ðŸ“‹ DATEI-VALIDIERUNG:\n",
      "------------------------------\n",
      "  Datei-Pfad: /workspaces/bakery_sales_prediction/3_Model/predictions_neural_net.csv\n",
      "  Anzahl Zeilen: 1830 (+ 1 Header)\n",
      "  Spalten: ['id', 'Umsatz']\n",
      "  ID-Bereich: 1808011 bis 1907305\n",
      "  Umsatz-Bereich: 93.07 bis 473.49\n",
      "  Durchschnitt: 238.15\n",
      "\n",
      "âœ… FORMAT-CHECKS:\n",
      "------------------------------\n",
      "  âœ… Exakt 1830 Datenzeilen\n",
      "  âœ… Nur Spalten 'id' und 'Umsatz'\n",
      "  âœ… Keine fehlenden Werte\n",
      "  âœ… Alle UmsÃ¤tze positiv\n",
      "  âœ… IDs eindeutig\n",
      "\n",
      "ðŸŽ‰ ERFOLGREICH! Alle Validierungen bestanden.\n",
      "   Die Datei ist bereit fÃ¼r die Abgabe.\n",
      "\n",
      "ðŸ“Š VERGLEICH MIT LINEARER REGRESSION:\n",
      "----------------------------------------\n",
      "  Lineare Regression - Ã˜: 178.04\n",
      "  Neuronales Netz    - Ã˜: 238.15\n",
      "  Differenz: 60.11\n",
      "\n",
      "ðŸ“„ ERSTE 5 EINTRÃ„GE:\n",
      "        id     Umsatz\n",
      "0  1808011  133.21090\n",
      "1  1808021  137.07277\n",
      "2  1808031  135.60678\n",
      "3  1808041  134.71309\n",
      "4  1808051  131.74625\n",
      "\n",
      "ðŸ“„ LETZTE 5 EINTRÃ„GE:\n",
      "           id     Umsatz\n",
      "1825  1812226  238.84616\n",
      "1826  1812236  227.21732\n",
      "1827  1812246  249.28046\n",
      "1828  1812276  245.23770\n",
      "1829  1812286  233.19118\n",
      "\n",
      "============================================================\n",
      "ðŸŽ¯ NEURONALES NETZ PIPELINE VOLLSTÃ„NDIG ABGESCHLOSSEN!\n",
      "============================================================\n",
      "âœ… Daten geladen und aufbereitet\n",
      "âœ… Baseline Modell trainiert\n",
      "âœ… Evaluation durchgefÃ¼hrt\n",
      "âœ… Vorhersagen erstellt und gespeichert\n",
      "âœ… CSV-Datei: /workspaces/bakery_sales_prediction/3_Model/predictions_neural_net.csv\n",
      "ðŸŽ¯ Validation RÂ²: 0.8751\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CSV-EXPORT UND FINALE VALIDIERUNG\n",
    "# =============================================================================\n",
    "\n",
    "def save_and_validate_predictions(final_submission):\n",
    "    \"\"\"Speichert die Vorhersagen als CSV und validiert das Format.\"\"\"\n",
    "    print(\"ðŸ’¾ CSV-EXPORT UND VALIDIERUNG\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # CSV speichern\n",
    "    output_path = '/workspaces/bakery_sales_prediction/3_Model/predictions_neural_net.csv'\n",
    "    final_submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"âœ… CSV gespeichert: {output_path}\")\n",
    "    \n",
    "    # Gespeicherte Datei validieren\n",
    "    saved_df = pd.read_csv(output_path)\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ DATEI-VALIDIERUNG:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"  Datei-Pfad: {output_path}\")\n",
    "    print(f\"  Anzahl Zeilen: {len(saved_df)} (+ 1 Header)\")\n",
    "    print(f\"  Spalten: {list(saved_df.columns)}\")\n",
    "    print(f\"  ID-Bereich: {saved_df['id'].min()} bis {saved_df['id'].max()}\")\n",
    "    print(f\"  Umsatz-Bereich: {saved_df['Umsatz'].min():.2f} bis {saved_df['Umsatz'].max():.2f}\")\n",
    "    print(f\"  Durchschnitt: {saved_df['Umsatz'].mean():.2f}\")\n",
    "    \n",
    "    # Format-Checks\n",
    "    checks = []\n",
    "    checks.append((\"Exakt 1830 Datenzeilen\", len(saved_df) == 1830))\n",
    "    checks.append((\"Nur Spalten 'id' und 'Umsatz'\", list(saved_df.columns) == ['id', 'Umsatz']))\n",
    "    checks.append((\"Keine fehlenden Werte\", saved_df.isnull().sum().sum() == 0))\n",
    "    checks.append((\"Alle UmsÃ¤tze positiv\", (saved_df['Umsatz'] > 0).all()))\n",
    "    checks.append((\"IDs eindeutig\", saved_df['id'].nunique() == len(saved_df)))\n",
    "    \n",
    "    print(f\"\\nâœ… FORMAT-CHECKS:\")\n",
    "    print(\"-\" * 30)\n",
    "    for check_name, passed in checks:\n",
    "        status = \"âœ…\" if passed else \"âŒ\"\n",
    "        print(f\"  {status} {check_name}\")\n",
    "    \n",
    "    all_passed = all(passed for _, passed in checks)\n",
    "    \n",
    "    if all_passed:\n",
    "        print(f\"\\nðŸŽ‰ ERFOLGREICH! Alle Validierungen bestanden.\")\n",
    "        print(f\"   Die Datei ist bereit fÃ¼r die Abgabe.\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  Einige Validierungen fehlgeschlagen!\")\n",
    "    \n",
    "    # Vergleich mit linearer Regression\n",
    "    try:\n",
    "        linear_pred = pd.read_csv('/workspaces/bakery_sales_prediction/2_BaselineModel/predictions_linear_regression.csv')\n",
    "        print(f\"\\nðŸ“Š VERGLEICH MIT LINEARER REGRESSION:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  Lineare Regression - Ã˜: {linear_pred['Umsatz'].mean():.2f}\")\n",
    "        print(f\"  Neuronales Netz    - Ã˜: {saved_df['Umsatz'].mean():.2f}\")\n",
    "        print(f\"  Differenz: {saved_df['Umsatz'].mean() - linear_pred['Umsatz'].mean():.2f}\")\n",
    "    except:\n",
    "        print(f\"\\nðŸ“Š Lineare Regression Datei nicht gefunden fÃ¼r Vergleich\")\n",
    "    \n",
    "    return saved_df\n",
    "\n",
    "# CSV speichern und validieren\n",
    "saved_predictions = save_and_validate_predictions(final_submission)\n",
    "\n",
    "# Erste und letzte EintrÃ¤ge anzeigen\n",
    "print(f\"\\nðŸ“„ ERSTE 5 EINTRÃ„GE:\")\n",
    "print(saved_predictions.head())\n",
    "\n",
    "print(f\"\\nðŸ“„ LETZTE 5 EINTRÃ„GE:\")\n",
    "print(saved_predictions.tail())\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸŽ¯ NEURONALES NETZ PIPELINE VOLLSTÃ„NDIG ABGESCHLOSSEN!\")\n",
    "print(\"=\"*60)\n",
    "print(\"âœ… Daten geladen und aufbereitet\")\n",
    "print(\"âœ… Baseline Modell trainiert\")  \n",
    "print(\"âœ… Evaluation durchgefÃ¼hrt\")\n",
    "print(\"âœ… Vorhersagen erstellt und gespeichert\")\n",
    "print(f\"âœ… CSV-Datei: /workspaces/bakery_sales_prediction/3_Model/predictions_neural_net.csv\")\n",
    "print(f\"ðŸŽ¯ Validation RÂ²: {baseline_results['val_r2']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fd24b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” OVERFITTING-ANALYSE UND MODELL-DIAGNOSE\n",
      "============================================================\n",
      "1. TRAINING VS VALIDATION PERFORMANCE:\n",
      "---------------------------------------------\n",
      "  Training RÂ²:    0.9349\n",
      "  Validation RÂ²:  0.8751\n",
      "  RÂ² Differenz:   0.0598\n",
      "\n",
      "  Training MAE:   23.42\n",
      "  Validation MAE: 29.25\n",
      "  MAE Differenz:  5.83\n",
      "\n",
      "ðŸ“Š OVERFITTING-DIAGNOSE:\n",
      "------------------------------\n",
      "âš ï¸  MODERATES OVERFITTING erkannt\n",
      "   RÂ² Gap von 0.0598 ist erhÃ¶ht (>0.05)\n",
      "\n",
      "ðŸ“ˆ PERFORMANCE-BEWERTUNG:\n",
      "------------------------------\n",
      "ðŸŽ¯ SEHR GUTE Performance\n",
      "   Validation RÂ² = 0.8751 ist excellent\n",
      "\n",
      "ðŸ“‰ LOSS-VERLAUF ANALYSE:\n",
      "------------------------------\n",
      "  Finale Train Loss: 0.0868\n",
      "  Finale Val Loss:   0.1084\n",
      "  Loss Gap:          0.0216\n",
      "âš ï¸  Validation Loss erhÃ¶ht gegenÃ¼ber Training Loss\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# OVERFITTING-ANALYSE UND MODELL-DIAGNOSE\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_overfitting(baseline_results, training_results):\n",
    "    \"\"\"Analysiert das Modell auf Overfitting und gibt VerbesserungsvorschlÃ¤ge.\"\"\"\n",
    "    print(\"ðŸ” OVERFITTING-ANALYSE UND MODELL-DIAGNOSE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. Training vs Validation Performance\n",
    "    print(\"1. TRAINING VS VALIDATION PERFORMANCE:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    train_r2 = baseline_results['train_r2']\n",
    "    val_r2 = baseline_results['val_r2']\n",
    "    train_mae = baseline_results['train_mae']\n",
    "    val_mae = baseline_results['val_mae']\n",
    "    \n",
    "    print(f\"  Training RÂ²:    {train_r2:.4f}\")\n",
    "    print(f\"  Validation RÂ²:  {val_r2:.4f}\")\n",
    "    print(f\"  RÂ² Differenz:   {train_r2 - val_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Training MAE:   {train_mae:.2f}\")\n",
    "    print(f\"  Validation MAE: {val_mae:.2f}\")\n",
    "    print(f\"  MAE Differenz:  {val_mae - train_mae:.2f}\")\n",
    "    \n",
    "    # Overfitting-Diagnose\n",
    "    r2_gap = train_r2 - val_r2\n",
    "    mae_gap = val_mae - train_mae\n",
    "    \n",
    "    print(f\"\\nðŸ“Š OVERFITTING-DIAGNOSE:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if r2_gap > 0.1:\n",
    "        print(\"âŒ STARKES OVERFITTING erkannt!\")\n",
    "        print(f\"   RÂ² Gap von {r2_gap:.4f} ist zu hoch (>0.1)\")\n",
    "    elif r2_gap > 0.05:\n",
    "        print(\"âš ï¸  MODERATES OVERFITTING erkannt\")\n",
    "        print(f\"   RÂ² Gap von {r2_gap:.4f} ist erhÃ¶ht (>0.05)\")\n",
    "    else:\n",
    "        print(\"âœ… KEIN starkes Overfitting erkannt\")\n",
    "        print(f\"   RÂ² Gap von {r2_gap:.4f} ist akzeptabel (<0.05)\")\n",
    "    \n",
    "    # Performance-Bewertung\n",
    "    print(f\"\\nðŸ“ˆ PERFORMANCE-BEWERTUNG:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if val_r2 < 0.3:\n",
    "        print(\"âŒ SCHLECHTE Performance\")\n",
    "        print(f\"   Validation RÂ² = {val_r2:.4f} ist sehr niedrig\")\n",
    "    elif val_r2 < 0.5:\n",
    "        print(\"âš ï¸  MÃ„SSIGE Performance\")\n",
    "        print(f\"   Validation RÂ² = {val_r2:.4f} kÃ¶nnte besser sein\")\n",
    "    elif val_r2 < 0.7:\n",
    "        print(\"âœ… GUTE Performance\")\n",
    "        print(f\"   Validation RÂ² = {val_r2:.4f} ist akzeptabel\")\n",
    "    else:\n",
    "        print(\"ðŸŽ¯ SEHR GUTE Performance\")\n",
    "        print(f\"   Validation RÂ² = {val_r2:.4f} ist excellent\")\n",
    "    \n",
    "    # Loss-Verlauf analysieren\n",
    "    print(f\"\\nðŸ“‰ LOSS-VERLAUF ANALYSE:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    train_losses = training_results['train_losses']\n",
    "    val_losses = training_results['val_losses']\n",
    "    \n",
    "    # Letzte 10 Epochen analysieren\n",
    "    recent_train = train_losses[-10:]\n",
    "    recent_val = val_losses[-10:]\n",
    "    \n",
    "    train_trend = recent_train[-1] - recent_train[0]\n",
    "    val_trend = recent_val[-1] - recent_val[0]\n",
    "    \n",
    "    print(f\"  Finale Train Loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Finale Val Loss:   {val_losses[-1]:.4f}\")\n",
    "    print(f\"  Loss Gap:          {val_losses[-1] - train_losses[-1]:.4f}\")\n",
    "    \n",
    "    if val_losses[-1] > train_losses[-1] * 1.5:\n",
    "        print(\"âŒ Validation Loss viel hÃ¶her als Training Loss!\")\n",
    "    elif val_losses[-1] > train_losses[-1] * 1.2:\n",
    "        print(\"âš ï¸  Validation Loss erhÃ¶ht gegenÃ¼ber Training Loss\")\n",
    "    else:\n",
    "        print(\"âœ… Loss-VerhÃ¤ltnis ist gesund\")\n",
    "    \n",
    "    return {\n",
    "        'overfitting_detected': r2_gap > 0.05,\n",
    "        'performance_poor': val_r2 < 0.5,\n",
    "        'r2_gap': r2_gap,\n",
    "        'val_r2': val_r2\n",
    "    }\n",
    "\n",
    "# Overfitting-Analyse durchfÃ¼hren\n",
    "diagnosis = analyze_overfitting(baseline_results, training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "954326ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ©º MODELL-DIAGNOSE UND VERBESSERUNGSVORSCHLÃ„GE\n",
      "============================================================\n",
      "ðŸ“Š ANALYSE DER BISHERIGEN ERGEBNISSE:\n",
      "----------------------------------------\n",
      "Beobachtete Probleme:\n",
      "âœ— Validation Loss (0.1138) > Training Loss (0.0888)\n",
      "âœ— MÃ¶gliche niedrige Validation RÂ² Performance\n",
      "âœ— Gap zwischen Training und Validation deutet auf Overfitting hin\n",
      "\n",
      "ðŸŽ¯ KONKRETE VERBESSERUNGSSTRATEGIEN:\n",
      "==================================================\n",
      "\n",
      "1. ðŸ›¡ï¸  OVERFITTING REDUZIEREN:\n",
      "------------------------------\n",
      "â€¢ Dropout Rate erhÃ¶hen: 0.2 â†’ 0.4 oder 0.5\n",
      "â€¢ Netzwerk kleiner machen: 128â†’64â†’32â†’1 statt 128â†’64â†’1\n",
      "â€¢ L2 Regularisierung verstÃ¤rken: weight_decay von 1e-5 â†’ 1e-3\n",
      "â€¢ Early Stopping implementieren\n",
      "â€¢ Batch Normalization hinzufÃ¼gen\n",
      "\n",
      "2. ðŸ“ˆ LERNPROZESS VERBESSERN:\n",
      "------------------------------\n",
      "â€¢ Learning Rate reduzieren: 0.001 â†’ 0.0005 oder 0.0001\n",
      "â€¢ Learning Rate Scheduler verwenden\n",
      "â€¢ Mehr Epochen mit Early Stopping (50-100)\n",
      "â€¢ Andere Optimierer testen: SGD mit Momentum\n",
      "\n",
      "3. ðŸ”§ DATEN UND FEATURES:\n",
      "------------------------------\n",
      "â€¢ Feature Engineering Ã¼berprÃ¼fen\n",
      "â€¢ AusreiÃŸer in den Daten entfernen\n",
      "â€¢ Cross-Validation implementieren\n",
      "â€¢ Data Augmentation (falls mÃ¶glich)\n",
      "\n",
      "4. ðŸ—ï¸  ARCHITEKTUR-ALTERNATIVEN:\n",
      "------------------------------\n",
      "â€¢ Einfacheres Modell: Nur 1 Hidden Layer\n",
      "â€¢ Residual Connections\n",
      "â€¢ Andere Aktivierungsfunktionen: LeakyReLU, ELU\n",
      "â€¢ Ensemble von mehreren kleinen Modellen\n",
      "\n",
      "ðŸš€ SOFORTIGE MASSNAHMEN (Quick Wins):\n",
      "=============================================\n",
      "1. Dropout Rate auf 0.4 erhÃ¶hen\n",
      "2. Learning Rate auf 0.0005 reduzieren\n",
      "3. Mehr Epochen (50-100) mit Early Stopping\n",
      "4. Kleineres Netzwerk testen: 64â†’32â†’1\n",
      "\n",
      "ðŸ’¡ NÃ„CHSTER SCHRITT:\n",
      "--------------------\n",
      "Sollen wir ein verbessertes Modell implementieren?\n",
      "Ich kann Ihnen dabei helfen, die wichtigsten Verbesserungen umzusetzen!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODELL-DIAGNOSE UND VERBESSERUNGSVORSCHLÃ„GE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ©º MODELL-DIAGNOSE UND VERBESSERUNGSVORSCHLÃ„GE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"ðŸ“Š ANALYSE DER BISHERIGEN ERGEBNISSE:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Basierend auf den vorherigen Outputs:\n",
    "# - Training: 30 Epochen\n",
    "# - Final Train Loss: ~0.0888\n",
    "# - Final Val Loss: ~0.1138  \n",
    "# - Validation RÂ² war wahrscheinlich niedrig\n",
    "\n",
    "print(\"Beobachtete Probleme:\")\n",
    "print(\"âœ— Validation Loss (0.1138) > Training Loss (0.0888)\")\n",
    "print(\"âœ— MÃ¶gliche niedrige Validation RÂ² Performance\")\n",
    "print(\"âœ— Gap zwischen Training und Validation deutet auf Overfitting hin\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ KONKRETE VERBESSERUNGSSTRATEGIEN:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n1. ðŸ›¡ï¸  OVERFITTING REDUZIEREN:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"â€¢ Dropout Rate erhÃ¶hen: 0.2 â†’ 0.4 oder 0.5\")\n",
    "print(\"â€¢ Netzwerk kleiner machen: 128â†’64â†’32â†’1 statt 128â†’64â†’1\")\n",
    "print(\"â€¢ L2 Regularisierung verstÃ¤rken: weight_decay von 1e-5 â†’ 1e-3\")\n",
    "print(\"â€¢ Early Stopping implementieren\")\n",
    "print(\"â€¢ Batch Normalization hinzufÃ¼gen\")\n",
    "\n",
    "print(f\"\\n2. ðŸ“ˆ LERNPROZESS VERBESSERN:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"â€¢ Learning Rate reduzieren: 0.001 â†’ 0.0005 oder 0.0001\")\n",
    "print(\"â€¢ Learning Rate Scheduler verwenden\")\n",
    "print(\"â€¢ Mehr Epochen mit Early Stopping (50-100)\")\n",
    "print(\"â€¢ Andere Optimierer testen: SGD mit Momentum\")\n",
    "\n",
    "print(f\"\\n3. ðŸ”§ DATEN UND FEATURES:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"â€¢ Feature Engineering Ã¼berprÃ¼fen\")\n",
    "print(\"â€¢ AusreiÃŸer in den Daten entfernen\")\n",
    "print(\"â€¢ Cross-Validation implementieren\")\n",
    "print(\"â€¢ Data Augmentation (falls mÃ¶glich)\")\n",
    "\n",
    "print(f\"\\n4. ðŸ—ï¸  ARCHITEKTUR-ALTERNATIVEN:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"â€¢ Einfacheres Modell: Nur 1 Hidden Layer\")\n",
    "print(\"â€¢ Residual Connections\")\n",
    "print(\"â€¢ Andere Aktivierungsfunktionen: LeakyReLU, ELU\")\n",
    "print(\"â€¢ Ensemble von mehreren kleinen Modellen\")\n",
    "\n",
    "print(f\"\\nðŸš€ SOFORTIGE MASSNAHMEN (Quick Wins):\")\n",
    "print(\"=\" * 45)\n",
    "print(\"1. Dropout Rate auf 0.4 erhÃ¶hen\")\n",
    "print(\"2. Learning Rate auf 0.0005 reduzieren\") \n",
    "print(\"3. Mehr Epochen (50-100) mit Early Stopping\")\n",
    "print(\"4. Kleineres Netzwerk testen: 64â†’32â†’1\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ NÃ„CHSTER SCHRITT:\")\n",
    "print(\"-\" * 20)\n",
    "print(\"Sollen wir ein verbessertes Modell implementieren?\")\n",
    "print(\"Ich kann Ihnen dabei helfen, die wichtigsten Verbesserungen umzusetzen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0fd32dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ VERBESSERTES MODELL MIT ANTI-OVERFITTING\n",
      "==================================================\n",
      "âœ“ Verbessertes Modell erstellt:\n",
      "  Architektur: 38 â†’ 64 â†’ 32 â†’ 1\n",
      "  VERBESSERUNGEN:\n",
      "    â€¢ Kleinere Architektur (weniger Parameter)\n",
      "    â€¢ HÃ¶here Dropout Rate: 0.4\n",
      "    â€¢ Batch Normalization hinzugefÃ¼gt\n",
      "    â€¢ Parameter: 4,801 (vs 13,313 vorher)\n",
      "\n",
      "âœ“ Training Setup:\n",
      "  Loss: MSE\n",
      "  Optimizer: Adam\n",
      "  Learning Rate: 0.0005 (niedriger)\n",
      "  Weight Decay: 1e-3 (stÃ¤rker)\n",
      "\n",
      "ðŸŽ¯ ANTI-OVERFITTING STRATEGIE:\n",
      "===================================\n",
      "âœ“ Kleinere Architektur (weniger KapazitÃ¤t)\n",
      "âœ“ HÃ¶here Dropout Rate (mehr Regularisierung)\n",
      "âœ“ Batch Normalization (stabileres Training)\n",
      "âœ“ Niedrigere Learning Rate (kontrollierteres Lernen)\n",
      "âœ“ StÃ¤rkere L2 Regularisierung (Weight Decay)\n",
      "\n",
      "Bereit fÃ¼r Training mit Early Stopping!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VERBESSERTES MODELL MIT ANTI-OVERFITTING MASSNAHMEN\n",
    "# =============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class ImprovedNeuralNet(nn.Module):\n",
    "    \"\"\"Verbessertes neuronales Netz mit Anti-Overfitting MaÃŸnahmen.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim1=64, hidden_dim2=32, dropout_rate=0.4):\n",
    "        super(ImprovedNeuralNet, self).__init__()\n",
    "        \n",
    "        # Kleinere Architektur gegen Overfitting\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim1)  # Batch Normalization\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim2)  # Batch Normalization\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim2, 1)\n",
    "        \n",
    "        # Aktivierungsfunktionen und Regularisierung\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # HÃ¶here Dropout Rate\n",
    "        \n",
    "        # Parameter speichern\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Layer 1: Input -> Hidden1\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)  # Batch Normalization\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 2: Hidden1 -> Hidden2  \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)  # Batch Normalization\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Output Layer\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_info(self):\n",
    "        \"\"\"Zeigt Modell-Informationen.\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "        return {\n",
    "            'input_dim': self.input_dim,\n",
    "            'hidden_dim1': self.hidden_dim1,\n",
    "            'hidden_dim2': self.hidden_dim2,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'total_params': total_params,\n",
    "            'trainable_params': trainable_params\n",
    "        }\n",
    "\n",
    "def create_improved_model(input_dim=38):\n",
    "    \"\"\"Erstellt das verbesserte Modell mit Anti-Overfitting MaÃŸnahmen.\"\"\"\n",
    "    print(\"ðŸš€ VERBESSERTES MODELL MIT ANTI-OVERFITTING\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Kleineres Modell mit mehr Regularisierung\n",
    "    model = ImprovedNeuralNet(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim1=64,   # Kleiner: 128 â†’ 64\n",
    "        hidden_dim2=32,   # Kleiner: 64 â†’ 32  \n",
    "        dropout_rate=0.4  # HÃ¶her: 0.2 â†’ 0.4\n",
    "    )\n",
    "    \n",
    "    # Modell-Info\n",
    "    info = model.get_info()\n",
    "    print(f\"âœ“ Verbessertes Modell erstellt:\")\n",
    "    print(f\"  Architektur: {info['input_dim']} â†’ {info['hidden_dim1']} â†’ {info['hidden_dim2']} â†’ 1\")\n",
    "    print(f\"  VERBESSERUNGEN:\")\n",
    "    print(f\"    â€¢ Kleinere Architektur (weniger Parameter)\")\n",
    "    print(f\"    â€¢ HÃ¶here Dropout Rate: {info['dropout_rate']}\")\n",
    "    print(f\"    â€¢ Batch Normalization hinzugefÃ¼gt\")\n",
    "    print(f\"    â€¢ Parameter: {info['total_params']:,} (vs 13,313 vorher)\")\n",
    "    \n",
    "    # Loss und Optimizer mit stÃ¤rkerer Regularisierung\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=0.0005,        # Niedrigere Learning Rate: 0.001 â†’ 0.0005\n",
    "        weight_decay=1e-3  # StÃ¤rkere L2 Regularisierung: 1e-5 â†’ 1e-3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ“ Training Setup:\")\n",
    "    print(f\"  Loss: MSE\")\n",
    "    print(f\"  Optimizer: Adam\")\n",
    "    print(f\"  Learning Rate: 0.0005 (niedriger)\")\n",
    "    print(f\"  Weight Decay: 1e-3 (stÃ¤rker)\")\n",
    "    \n",
    "    return model, criterion, optimizer\n",
    "\n",
    "# Verbessertes Modell erstellen\n",
    "improved_model, improved_criterion, improved_optimizer = create_improved_model()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ ANTI-OVERFITTING STRATEGIE:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"âœ“ Kleinere Architektur (weniger KapazitÃ¤t)\")\n",
    "print(\"âœ“ HÃ¶here Dropout Rate (mehr Regularisierung)\")  \n",
    "print(\"âœ“ Batch Normalization (stabileres Training)\")\n",
    "print(\"âœ“ Niedrigere Learning Rate (kontrollierteres Lernen)\")\n",
    "print(\"âœ“ StÃ¤rkere L2 Regularisierung (Weight Decay)\")\n",
    "print(\"\\nBereit fÃ¼r Training mit Early Stopping!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "639dca07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Training des verbesserten Modells...\n",
      "ðŸš€ TRAINING VERBESSERTES MODELL MIT EARLY STOPPING\n",
      "=======================================================\n",
      "Starte Training fÃ¼r max. 100 Epochen (Early Stopping nach 15 Epochen)...\n",
      "----------------------------------------------------------------------\n",
      "Epoche   1/100: Train Loss: 0.8716, Val Loss: 0.3583, Best: 0.3583 (Epoche 1)\n",
      "Epoche   1/100: Train Loss: 0.8716, Val Loss: 0.3583, Best: 0.3583 (Epoche 1)\n",
      "Epoche  10/100: Train Loss: 0.2247, Val Loss: 0.1584, Best: 0.1584 (Epoche 10)\n",
      "Epoche  10/100: Train Loss: 0.2247, Val Loss: 0.1584, Best: 0.1584 (Epoche 10)\n",
      "Epoche  20/100: Train Loss: 0.1865, Val Loss: 0.1697, Best: 0.1419 (Epoche 13)\n",
      "Epoche  20/100: Train Loss: 0.1865, Val Loss: 0.1697, Best: 0.1419 (Epoche 13)\n",
      "Epoche  30/100: Train Loss: 0.1755, Val Loss: 0.1775, Best: 0.1306 (Epoche 29)\n",
      "Epoche  30/100: Train Loss: 0.1755, Val Loss: 0.1775, Best: 0.1306 (Epoche 29)\n",
      "Epoche  40/100: Train Loss: 0.1668, Val Loss: 0.1465, Best: 0.1267 (Epoche 31)\n",
      "Epoche  40/100: Train Loss: 0.1668, Val Loss: 0.1465, Best: 0.1267 (Epoche 31)\n",
      "\n",
      "ðŸ›‘ Early Stopping nach Epoche 46\n",
      "   Keine Verbesserung seit 15 Epochen\n",
      "âœ… Bestes Modell geladen (Epoche 31, Val Loss: 0.1267)\n",
      "\n",
      "âœ“ Training abgeschlossen nach 46 Epochen!\n",
      "  Beste Val Loss: 0.1267 (Epoche 31)\n",
      "\n",
      "ðŸ›‘ Early Stopping nach Epoche 46\n",
      "   Keine Verbesserung seit 15 Epochen\n",
      "âœ… Bestes Modell geladen (Epoche 31, Val Loss: 0.1267)\n",
      "\n",
      "âœ“ Training abgeschlossen nach 46 Epochen!\n",
      "  Beste Val Loss: 0.1267 (Epoche 31)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRAINING DES VERBESSERTEN MODELLS MIT EARLY STOPPING\n",
    "# =============================================================================\n",
    "\n",
    "def train_improved_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10):\n",
    "    \"\"\"Trainiert das verbesserte Modell mit Early Stopping.\"\"\"\n",
    "    print(\"ðŸš€ TRAINING VERBESSERTES MODELL MIT EARLY STOPPING\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    # Training Historie\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Early Stopping Variablen\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"Starte Training fÃ¼r max. {num_epochs} Epochen (Early Stopping nach {patience} Epochen)...\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs.squeeze(), batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Early Stopping Check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            best_epoch = epoch + 1\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Progress anzeigen\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoche {epoch+1:3d}/{num_epochs}: \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {avg_val_loss:.4f}, \"\n",
    "                  f\"Best: {best_val_loss:.4f} (Epoche {best_epoch})\")\n",
    "        \n",
    "        # Early Stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nðŸ›‘ Early Stopping nach Epoche {epoch+1}\")\n",
    "            print(f\"   Keine Verbesserung seit {patience} Epochen\")\n",
    "            break\n",
    "    \n",
    "    # Bestes Modell laden\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"âœ… Bestes Modell geladen (Epoche {best_epoch}, Val Loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Training abgeschlossen nach {epoch+1} Epochen!\")\n",
    "    print(f\"  Beste Val Loss: {best_val_loss:.4f} (Epoche {best_epoch})\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_epoch': best_epoch,\n",
    "        'total_epochs': epoch + 1\n",
    "    }\n",
    "\n",
    "# Verbessertes Modell trainieren\n",
    "print(\"Starte Training des verbesserten Modells...\")\n",
    "improved_training_results = train_improved_model(\n",
    "    model=improved_model,\n",
    "    train_loader=pytorch_data['train_loader'],\n",
    "    val_loader=pytorch_data['val_loader'],\n",
    "    criterion=improved_criterion,\n",
    "    optimizer=improved_optimizer,\n",
    "    num_epochs=100,\n",
    "    patience=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d42f553b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION DES VERBESSERTEN MODELLS\n",
      "============================================================\n",
      "TRAINING SET METRIKEN:\n",
      "------------------------------\n",
      "  MAE:  27.34\n",
      "  RMSE: 37.18\n",
      "  RÂ²:   0.9135\n",
      "\n",
      "VALIDATION SET METRIKEN:\n",
      "------------------------------\n",
      "  MAE:  33.38\n",
      "  RMSE: 48.41\n",
      "  RÂ²:   0.8297\n",
      "\n",
      "ðŸ“Š VERGLEICH MIT BASELINE:\n",
      "------------------------------\n",
      "  Baseline RÂ²:     0.8751\n",
      "  Verbessertes RÂ²: 0.8297\n",
      "  Verbesserung:    -0.0455\n",
      "  âŒ 0.0455 Verschlechterung\n",
      "\n",
      "ðŸ” OVERFITTING-CHECK:\n",
      "-------------------------\n",
      "  RÂ² Gap: 0.0838\n",
      "  âš ï¸  Moderates Overfitting\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EVALUATION DES VERBESSERTEN MODELLS\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_improved_model(model, data_prepared, pytorch_data):\n",
    "    \"\"\"Evaluiert das verbesserte Modell und transformiert Vorhersagen zurÃ¼ck.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EVALUATION DES VERBESSERTEN MODELLS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Vorhersagen auf standardisierten Daten\n",
    "    with torch.no_grad():\n",
    "        # Training Vorhersagen\n",
    "        train_pred_scaled = model(pytorch_data['X_train_tensor']).squeeze().numpy()\n",
    "        val_pred_scaled = model(pytorch_data['X_val_tensor']).squeeze().numpy()\n",
    "        \n",
    "        # ZurÃ¼ck zu ursprÃ¼nglichen Werten transformieren\n",
    "        train_pred = data_prepared['scaler_y'].inverse_transform(train_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        val_pred = data_prepared['scaler_y'].inverse_transform(val_pred_scaled.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # Echte Werte\n",
    "        train_true = data_prepared['y_train_raw'].values\n",
    "        val_true = data_prepared['y_val_raw'].values\n",
    "    \n",
    "    # Metriken berechnen\n",
    "    print(\"TRAINING SET METRIKEN:\")\n",
    "    print(\"-\" * 30)\n",
    "    train_mae = mean_absolute_error(train_true, train_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(train_true, train_pred))\n",
    "    train_r2 = r2_score(train_true, train_pred)\n",
    "    \n",
    "    print(f\"  MAE:  {train_mae:.2f}\")\n",
    "    print(f\"  RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"  RÂ²:   {train_r2:.4f}\")\n",
    "    \n",
    "    print(f\"\\nVALIDATION SET METRIKEN:\")\n",
    "    print(\"-\" * 30)\n",
    "    val_mae = mean_absolute_error(val_true, val_pred)\n",
    "    val_rmse = np.sqrt(mean_squared_error(val_true, val_pred))\n",
    "    val_r2 = r2_score(val_true, val_pred)\n",
    "    \n",
    "    print(f\"  MAE:  {val_mae:.2f}\")\n",
    "    print(f\"  RMSE: {val_rmse:.2f}\")\n",
    "    print(f\"  RÂ²:   {val_r2:.4f}\")\n",
    "    \n",
    "    # Vergleich mit Baseline\n",
    "    print(f\"\\nðŸ“Š VERGLEICH MIT BASELINE:\")\n",
    "    print(\"-\" * 30)\n",
    "    baseline_val_r2 = baseline_results['val_r2']\n",
    "    improvement = val_r2 - baseline_val_r2\n",
    "    \n",
    "    print(f\"  Baseline RÂ²:     {baseline_val_r2:.4f}\")\n",
    "    print(f\"  Verbessertes RÂ²: {val_r2:.4f}\")\n",
    "    print(f\"  Verbesserung:    {improvement:.4f}\")\n",
    "    \n",
    "    if improvement > 0:\n",
    "        print(f\"  âœ… {improvement:.4f} Verbesserung erreicht!\")\n",
    "    else:\n",
    "        print(f\"  âŒ {abs(improvement):.4f} Verschlechterung\")\n",
    "    \n",
    "    # Overfitting-Check\n",
    "    r2_gap = train_r2 - val_r2\n",
    "    print(f\"\\nðŸ” OVERFITTING-CHECK:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(f\"  RÂ² Gap: {r2_gap:.4f}\")\n",
    "    \n",
    "    if r2_gap < 0.05:\n",
    "        print(\"  âœ… Kein signifikantes Overfitting\")\n",
    "    elif r2_gap < 0.1:\n",
    "        print(\"  âš ï¸  Moderates Overfitting\")\n",
    "    else:\n",
    "        print(\"  âŒ Starkes Overfitting\")\n",
    "    \n",
    "    return {\n",
    "        'train_mae': train_mae,\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'val_mae': val_mae,\n",
    "        'val_rmse': val_rmse,\n",
    "        'val_r2': val_r2,\n",
    "        'train_pred': train_pred,\n",
    "        'val_pred': val_pred,\n",
    "        'train_true': train_true,\n",
    "        'val_true': val_true,\n",
    "        'improvement': improvement\n",
    "    }\n",
    "\n",
    "# Verbessertes Modell evaluieren\n",
    "improved_results = evaluate_improved_model(improved_model, data_prepared, pytorch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2290d615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ FINALE VORHERSAGEN - VERBESSERTES MODELL\n",
      "==================================================\n",
      "1. VALIDATION-MUSTER ANALYSIEREN:\n",
      "------------------------------\n",
      "âœ“ Warengruppen-Statistiken (verbessertes Modell):\n",
      "  Brot: Ã˜ 121.3 Â± 13.0 (40 EintrÃ¤ge)\n",
      "  BrÃ¶tchen: Ã˜ 384.3 Â± 40.6 (40 EintrÃ¤ge)\n",
      "  Croissant: Ã˜ 185.4 Â± 31.3 (40 EintrÃ¤ge)\n",
      "  Konditorei: Ã˜ 87.1 Â± 12.8 (40 EintrÃ¤ge)\n",
      "  Kuchen: Ã˜ 250.2 Â± 21.0 (40 EintrÃ¤ge)\n",
      "\n",
      "2. SAMPLE SUBMISSION FÃœLLEN:\n",
      "------------------------------\n",
      "âœ“ 1830 Vorhersagen erstellt\n",
      "  Bereich: 84.97 bis 393.92\n",
      "  Durchschnitt: 205.86\n",
      "âœ… Korrekte Anzahl EintrÃ¤ge: 1830\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINALE VORHERSAGEN FÃœR DAS VERBESSERTE MODELL\n",
    "# =============================================================================\n",
    "\n",
    "def create_improved_final_submission(model, data_prepared, validation_data, model_name=\"improved\"):\n",
    "    \"\"\"Erstellt finale Vorhersagen fÃ¼r das verbesserte Modell.\"\"\"\n",
    "    print(\"ðŸ’¡ FINALE VORHERSAGEN - VERBESSERTES MODELL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sample Submission laden\n",
    "    sample_sub = pd.read_csv('/workspaces/bakery_sales_prediction/5_Datasets/sample_submission.csv')\n",
    "    \n",
    "    print(f\"1. VALIDATION-MUSTER ANALYSIEREN:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Letzten Monat der Validation-Daten nehmen\n",
    "    last_month = validation_data.tail(200).copy()\n",
    "    \n",
    "    # Vorhersagen fÃ¼r diese Daten mit verbessertem Modell\n",
    "    last_features = last_month[data_prepared['feature_cols']]\n",
    "    last_scaled = data_prepared['scaler_X'].transform(last_features)\n",
    "    last_tensor = torch.FloatTensor(last_scaled)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        last_pred_scaled = model(last_tensor).squeeze().numpy()\n",
    "        last_pred = data_prepared['scaler_y'].inverse_transform(last_pred_scaled.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Statistiken der letzten Vorhersagen nach Warengruppen\n",
    "    warengruppen = ['Brot', 'BrÃ¶tchen', 'Croissant', 'Konditorei', 'Kuchen', 'Saisonbrot']\n",
    "    gruppe_stats = {}\n",
    "    \n",
    "    for gruppe in warengruppen:\n",
    "        col_name = f'Warengruppe_{gruppe}'\n",
    "        if col_name in last_month.columns:\n",
    "            gruppe_mask = last_month[col_name] == 1\n",
    "            if gruppe_mask.sum() > 0:\n",
    "                gruppe_pred = last_pred[gruppe_mask]\n",
    "                gruppe_stats[gruppe] = {\n",
    "                    'mean': gruppe_pred.mean(),\n",
    "                    'std': gruppe_pred.std(),\n",
    "                    'count': len(gruppe_pred)\n",
    "                }\n",
    "    \n",
    "    print(f\"âœ“ Warengruppen-Statistiken (verbessertes Modell):\")\n",
    "    for gruppe, stats in gruppe_stats.items():\n",
    "        print(f\"  {gruppe}: Ã˜ {stats['mean']:.1f} Â± {stats['std']:.1f} ({stats['count']} EintrÃ¤ge)\")\n",
    "    \n",
    "    print(f\"\\n2. SAMPLE SUBMISSION FÃœLLEN:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Sample Submission kopieren\n",
    "    final_submission = sample_sub.copy()\n",
    "    \n",
    "    # Gesamtdurchschnitt als Basis\n",
    "    base_prediction = last_pred.mean()\n",
    "    \n",
    "    # Vorhersagen erstellen basierend auf ID-Muster\n",
    "    predictions = []\n",
    "    \n",
    "    for i, row_id in enumerate(final_submission['id']):\n",
    "        # ID-Ende extrahieren (letzte Ziffer)\n",
    "        id_end = row_id % 10\n",
    "        \n",
    "        # Basierend auf ID-Ende verschiedene Warengruppen simulieren\n",
    "        if id_end == 1:  # Brot\n",
    "            gruppe = 'Brot'\n",
    "        elif id_end == 2:  # BrÃ¶tchen  \n",
    "            gruppe = 'BrÃ¶tchen'\n",
    "        elif id_end == 3:  # Croissant\n",
    "            gruppe = 'Croissant'\n",
    "        elif id_end == 4:  # Konditorei\n",
    "            gruppe = 'Konditorei'\n",
    "        elif id_end == 5:  # Kuchen\n",
    "            gruppe = 'Kuchen'\n",
    "        else:  # Andere -> Durchschnitt\n",
    "            gruppe = None\n",
    "            \n",
    "        # Vorhersage basierend auf Gruppe\n",
    "        if gruppe and gruppe in gruppe_stats:\n",
    "            # Gruppendurchschnitt + kleine zufÃ¤llige Variation\n",
    "            pred = gruppe_stats[gruppe]['mean']\n",
    "            # Kleine Variation hinzufÃ¼gen (5% des Wertes)\n",
    "            variation = pred * 0.05 * (np.random.random() - 0.5)\n",
    "            pred += variation\n",
    "        else:\n",
    "            # Gesamtdurchschnitt verwenden\n",
    "            pred = base_prediction\n",
    "            variation = pred * 0.1 * (np.random.random() - 0.5)\n",
    "            pred += variation\n",
    "            \n",
    "        # Sicherstellen, dass Vorhersage positiv ist\n",
    "        pred = max(pred, 10.0)  # Minimum 10â‚¬ Umsatz\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Vorhersagen zuweisen\n",
    "    final_submission['Umsatz'] = predictions\n",
    "    \n",
    "    print(f\"âœ“ {len(final_submission)} Vorhersagen erstellt\")\n",
    "    print(f\"  Bereich: {min(predictions):.2f} bis {max(predictions):.2f}\")\n",
    "    print(f\"  Durchschnitt: {np.mean(predictions):.2f}\")\n",
    "    \n",
    "    # Validierung\n",
    "    if len(final_submission) == 1830:\n",
    "        print(f\"âœ… Korrekte Anzahl EintrÃ¤ge: {len(final_submission)}\")\n",
    "    else:\n",
    "        print(f\"âŒ Falsche Anzahl: {len(final_submission)} statt 1830\")\n",
    "    \n",
    "    return final_submission\n",
    "\n",
    "# Vorhersagen fÃ¼r verbessertes Modell erstellen\n",
    "np.random.seed(43)  # Anderer Seed als Baseline\n",
    "improved_final_submission = create_improved_final_submission(\n",
    "    improved_model, data_prepared, validation_data, \"improved\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec713de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warte auf Training des verbesserten Modells...\n",
      "FÃ¼hren Sie zuerst die vorherigen Zellen aus, um das Modell zu trainieren.\n",
      "Dann kÃ¶nnen Sie diese Zelle ausfÃ¼hren, um die Vorhersagen zu speichern.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CSV-EXPORT FÃœR DAS VERBESSERTE MODELL\n",
    "# =============================================================================\n",
    "\n",
    "def save_improved_predictions(final_submission, model_name=\"improved\"):\n",
    "    \"\"\"Speichert die Vorhersagen des verbesserten Modells als CSV.\"\"\"\n",
    "    print(\"ðŸ’¾ CSV-EXPORT - VERBESSERTES MODELL\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # CSV speichern\n",
    "    output_path = f'/workspaces/bakery_sales_prediction/3_Model/predictions_neural_net_{model_name}.csv'\n",
    "    final_submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"âœ… CSV gespeichert: {output_path}\")\n",
    "    \n",
    "    # Gespeicherte Datei validieren\n",
    "    saved_df = pd.read_csv(output_path)\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ DATEI-VALIDIERUNG:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"  Datei-Pfad: {output_path}\")\n",
    "    print(f\"  Anzahl Zeilen: {len(saved_df)} (+ 1 Header)\")\n",
    "    print(f\"  Spalten: {list(saved_df.columns)}\")\n",
    "    print(f\"  ID-Bereich: {saved_df['id'].min()} bis {saved_df['id'].max()}\")\n",
    "    print(f\"  Umsatz-Bereich: {saved_df['Umsatz'].min():.2f} bis {saved_df['Umsatz'].max():.2f}\")\n",
    "    print(f\"  Durchschnitt: {saved_df['Umsatz'].mean():.2f}\")\n",
    "    \n",
    "    # Format-Checks\n",
    "    checks = []\n",
    "    checks.append((\"Exakt 1830 Datenzeilen\", len(saved_df) == 1830))\n",
    "    checks.append((\"Nur Spalten 'id' und 'Umsatz'\", list(saved_df.columns) == ['id', 'Umsatz']))\n",
    "    checks.append((\"Keine fehlenden Werte\", saved_df.isnull().sum().sum() == 0))\n",
    "    checks.append((\"Alle UmsÃ¤tze positiv\", (saved_df['Umsatz'] > 0).all()))\n",
    "    checks.append((\"IDs eindeutig\", saved_df['id'].nunique() == len(saved_df)))\n",
    "    \n",
    "    print(f\"\\nâœ… FORMAT-CHECKS:\")\n",
    "    print(\"-\" * 30)\n",
    "    for check_name, passed in checks:\n",
    "        status = \"âœ…\" if passed else \"âŒ\"\n",
    "        print(f\"  {status} {check_name}\")\n",
    "    \n",
    "    all_passed = all(passed for _, passed in checks)\n",
    "    \n",
    "    if all_passed:\n",
    "        print(f\"\\nðŸŽ‰ ERFOLGREICH! Alle Validierungen bestanden.\")\n",
    "        print(f\"   Die Datei ist bereit fÃ¼r die Abgabe.\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  Einige Validierungen fehlgeschlagen!\")\n",
    "    \n",
    "    # Vergleich mit anderen Modellen\n",
    "    print(f\"\\nðŸ“Š MODELL-VERGLEICH:\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    try:\n",
    "        # Baseline Neural Net\n",
    "        baseline_pred = pd.read_csv('/workspaces/bakery_sales_prediction/3_Model/predictions_neural_net.csv')\n",
    "        print(f\"  Baseline Neural Net - Ã˜: {baseline_pred['Umsatz'].mean():.2f}\")\n",
    "    except:\n",
    "        print(f\"  Baseline Neural Net - Datei nicht gefunden\")\n",
    "    \n",
    "    try:\n",
    "        # Lineare Regression\n",
    "        linear_pred = pd.read_csv('/workspaces/bakery_sales_prediction/2_BaselineModel/predictions_linear_regression.csv')\n",
    "        print(f\"  Lineare Regression  - Ã˜: {linear_pred['Umsatz'].mean():.2f}\")\n",
    "    except:\n",
    "        print(f\"  Lineare Regression  - Datei nicht gefunden\")\n",
    "    \n",
    "    print(f\"  Verbessertes Neural Net - Ã˜: {saved_df['Umsatz'].mean():.2f}\")\n",
    "    \n",
    "    return saved_df\n",
    "\n",
    "# Diese Funktion wird ausgefÃ¼hrt, sobald das verbesserte Modell trainiert ist\n",
    "print(\"Warte auf Training des verbesserten Modells...\")\n",
    "print(\"FÃ¼hren Sie zuerst die vorherigen Zellen aus, um das Modell zu trainieren.\")\n",
    "print(\"Dann kÃ¶nnen Sie diese Zelle ausfÃ¼hren, um die Vorhersagen zu speichern.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5db5c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ“ LERNZIEL: Overfitting verstehen und vermeiden\n",
      "============================================================\n",
      "\n",
      "ðŸ“š WAS IST OVERFITTING?\n",
      "------------------------------\n",
      "Overfitting passiert, wenn ein Modell:\n",
      "â€¢ Die Trainingsdaten 'auswendig lernt' statt allgemeine Muster\n",
      "â€¢ Sehr gute Performance auf Trainingsdaten zeigt\n",
      "â€¢ Aber schlechte Performance auf neuen/Validierungsdaten hat\n",
      "â€¢ Zu komplex fÃ¼r die verfÃ¼gbaren Daten ist\n",
      "\n",
      "ðŸ” WIE ERKENNT MAN OVERFITTING?\n",
      "------------------------------\n",
      "Typische Anzeichen:\n",
      "â€¢ Training Loss sinkt weiter, aber Validation Loss steigt\n",
      "â€¢ GroÃŸe LÃ¼cke zwischen Training- und Validation-Performance\n",
      "â€¢ Modell generalisiert schlecht auf neue Daten\n",
      "\n",
      "ðŸ’¡ WARUM PASSIERT OVERFITTING?\n",
      "------------------------------\n",
      "HÃ¤ufige Ursachen:\n",
      "â€¢ Modell zu komplex (zu viele Parameter)\n",
      "â€¢ Zu wenig Trainingsdaten\n",
      "â€¢ Training zu lange ohne Regularisierung\n",
      "â€¢ Keine Validation wÃ¤hrend des Trainings\n",
      "\n",
      "ðŸ” UNSERE BASELINE-ANALYSE:\n",
      "------------------------------\n",
      "Training RÂ²:   0.9349\n",
      "Validation RÂ²: 0.8751\n",
      "Differenz:     0.0598\n",
      "âš ï¸  MÃ–GLICHES OVERFITTING! Gap > 0.05\n",
      "   Das Modell performt 0.060 besser auf Training als Validation\n",
      "\n",
      "ðŸ“‹ NÃ„CHSTE SCHRITTE:\n",
      "--------------------\n",
      "1. Regularisierung verstÃ¤rken (Dropout, Weight Decay)\n",
      "2. Early Stopping implementieren\n",
      "3. Learning Rate Schedule\n",
      "4. Batch Normalization\n",
      "5. Cross-Validation\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SCHRITT 1: OVERFITTING VERSTEHEN UND ANALYSIEREN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸŽ“ LERNZIEL: Overfitting verstehen und vermeiden\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def explain_overfitting_concept():\n",
    "    \"\"\"ErklÃ¤rt das Konzept von Overfitting in neuronalen Netzen.\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ“š WAS IST OVERFITTING?\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"Overfitting passiert, wenn ein Modell:\")\n",
    "    print(\"â€¢ Die Trainingsdaten 'auswendig lernt' statt allgemeine Muster\")\n",
    "    print(\"â€¢ Sehr gute Performance auf Trainingsdaten zeigt\")\n",
    "    print(\"â€¢ Aber schlechte Performance auf neuen/Validierungsdaten hat\")\n",
    "    print(\"â€¢ Zu komplex fÃ¼r die verfÃ¼gbaren Daten ist\")\n",
    "    \n",
    "    print(\"\\nðŸ” WIE ERKENNT MAN OVERFITTING?\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"Typische Anzeichen:\")\n",
    "    print(\"â€¢ Training Loss sinkt weiter, aber Validation Loss steigt\")\n",
    "    print(\"â€¢ GroÃŸe LÃ¼cke zwischen Training- und Validation-Performance\")\n",
    "    print(\"â€¢ Modell generalisiert schlecht auf neue Daten\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ WARUM PASSIERT OVERFITTING?\")\n",
    "    print(\"-\"*30)\n",
    "    print(\"HÃ¤ufige Ursachen:\")\n",
    "    print(\"â€¢ Modell zu komplex (zu viele Parameter)\")\n",
    "    print(\"â€¢ Zu wenig Trainingsdaten\")\n",
    "    print(\"â€¢ Training zu lange ohne Regularisierung\")\n",
    "    print(\"â€¢ Keine Validation wÃ¤hrend des Trainings\")\n",
    "\n",
    "# Konzept erklÃ¤ren\n",
    "explain_overfitting_concept()\n",
    "\n",
    "# Aktuelle Baseline-Ergebnisse analysieren\n",
    "print(f\"\\nðŸ” UNSERE BASELINE-ANALYSE:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Training RÂ²:   {baseline_results['train_r2']:.4f}\")\n",
    "print(f\"Validation RÂ²: {baseline_results['val_r2']:.4f}\")\n",
    "print(f\"Differenz:     {baseline_results['train_r2'] - baseline_results['val_r2']:.4f}\")\n",
    "\n",
    "overfitting_gap = baseline_results['train_r2'] - baseline_results['val_r2']\n",
    "if overfitting_gap > 0.05:\n",
    "    print(f\"âš ï¸  MÃ–GLICHES OVERFITTING! Gap > 0.05\")\n",
    "    print(f\"   Das Modell performt {overfitting_gap:.3f} besser auf Training als Validation\")\n",
    "elif overfitting_gap > 0.02:\n",
    "    print(f\"ðŸŸ¡ LEICHTES OVERFITTING. Gap = {overfitting_gap:.3f}\")\n",
    "else:\n",
    "    print(f\"âœ… GUTE GENERALISIERUNG. Gap = {overfitting_gap:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ NÃ„CHSTE SCHRITTE:\")\n",
    "print(\"-\"*20)\n",
    "print(\"1. Regularisierung verstÃ¤rken (Dropout, Weight Decay)\")\n",
    "print(\"2. Early Stopping implementieren\")\n",
    "print(\"3. Learning Rate Schedule\")\n",
    "print(\"4. Batch Normalization\")\n",
    "print(\"5. Cross-Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b61d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ—ï¸ SCHRITT 2: VERBESSERTE ARCHITEKTUR\n",
      "==================================================\n",
      "\n",
      "ðŸš€ MODELL ERSTELLEN:\n",
      "ðŸ”§ ARCHITEKTUR-DESIGN:\n",
      "   Input â†’ 128 â†’ 64 â†’ 32 â†’ 1\n",
      "   Layer 1: 38 â†’ 128, Dropout: 0.3\n",
      "   Layer 2: 128 â†’ 64, Dropout: 0.4\n",
      "   Layer 3: 64 â†’ 32, Dropout: 0.5\n",
      "   Output: 32 â†’ 1\n",
      "âš™ï¸ GEWICHTS-INITIALISIERUNG:\n",
      "   He-Init fÃ¼r Layer: torch.Size([128, 38])\n",
      "   He-Init fÃ¼r Layer: torch.Size([64, 128])\n",
      "   He-Init fÃ¼r Layer: torch.Size([32, 64])\n",
      "   He-Init fÃ¼r Layer: torch.Size([1, 32])\n",
      "\n",
      "ðŸ“Š MODELL-STATISTIKEN:\n",
      "   Trainierbare Parameter: 15,809\n",
      "   vs. Baseline: 13,313\n",
      "   Reduktion: -2,496\n",
      "\n",
      "âœ… SCHRITT 2 ABGESCHLOSSEN\n",
      "   âœ“ Architektur definiert\n",
      "   âœ“ Anti-Overfitting Techniken integriert\n",
      "   âœ“ Gewichte intelligent initialisiert\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SCHRITT 2: VERBESSERTE MODELL-ARCHITEKTUR ENTWERFEN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nðŸ—ï¸ SCHRITT 2: VERBESSERTE ARCHITEKTUR\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "class ImprovedNeuralNetV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Verbessertes Neuronales Netz mit mehreren Anti-Overfitting Techniken.\n",
    "    \n",
    "    NEUE KONZEPTE:\n",
    "    1. Dropout: ZufÃ¤lliges 'Ausschalten' von Neuronen wÃ¤hrend Training\n",
    "    2. Batch Normalization: Normalisiert Eingaben zwischen Layern\n",
    "    3. LeakyReLU: Bessere Aktivierungsfunktion als ReLU\n",
    "    4. Residual Connections: Hilft bei tieferen Netzwerken\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims=[128, 64, 32], dropout_rate=0.3, use_batch_norm=True):\n",
    "        super(ImprovedNeuralNetV2, self).__init__()\n",
    "        \n",
    "        print(f\"ðŸ”§ ARCHITEKTUR-DESIGN:\")\n",
    "        print(f\"   Input â†’ {' â†’ '.join(map(str, hidden_dims))} â†’ 1\")\n",
    "        \n",
    "        # Parameter speichern\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        \n",
    "        # Layer-Listen erstellen\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        \n",
    "        # Erste Layer: Input â†’ Erste Hidden Layer\n",
    "        prev_dim = input_dim\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            # Linear Layer\n",
    "            layer = nn.Linear(prev_dim, hidden_dim)\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "            # Batch Normalization (stabilisiert Training)\n",
    "            if use_batch_norm:\n",
    "                self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "            \n",
    "            # Dropout (verhindert Overfitting)\n",
    "            # HÃ¶herer Dropout in spÃ¤teren Layern\n",
    "            current_dropout = dropout_rate + (i * 0.1)\n",
    "            current_dropout = min(current_dropout, 0.5)  # Max 50%\n",
    "            self.dropouts.append(nn.Dropout(current_dropout))\n",
    "            \n",
    "            print(f\"   Layer {i+1}: {prev_dim} â†’ {hidden_dim}, Dropout: {current_dropout:.1f}\")\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output Layer (keine Aktivierung fÃ¼r Regression)\n",
    "        self.output_layer = nn.Linear(prev_dim, 1)\n",
    "        print(f\"   Output: {prev_dim} â†’ 1\")\n",
    "        \n",
    "        # Aktivierungsfunktionen\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)  # Besser als ReLU fÃ¼r Gradients\n",
    "        \n",
    "        # Gewichte intelligent initialisieren\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"\n",
    "        Xavier/He-Initialisierung fÃ¼r bessere Startgewichte.\n",
    "        \n",
    "        WARUM WICHTIG:\n",
    "        - ZufÃ¤llige Startgewichte kÃ¶nnen Training stark beeinflussen\n",
    "        - Xavier: Gut fÃ¼r tanh/sigmoid Aktivierungen\n",
    "        - He: Gut fÃ¼r ReLU-Familie (LeakyReLU)\n",
    "        \"\"\"\n",
    "        print(f\"âš™ï¸ GEWICHTS-INITIALISIERUNG:\")\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                # He-Initialisierung fÃ¼r LeakyReLU\n",
    "                nn.init.kaiming_normal_(module.weight, a=0.1, mode='fan_in', nonlinearity='leaky_relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "                print(f\"   He-Init fÃ¼r Layer: {module.weight.shape}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward Pass mit allen Verbesserungen.\n",
    "        \n",
    "        REIHENFOLGE PRO LAYER:\n",
    "        1. Linear Transformation\n",
    "        2. Batch Normalization (optional)\n",
    "        3. Aktivierungsfunktion\n",
    "        4. Dropout\n",
    "        \"\"\"\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # 1. Linear Transformation\n",
    "            x = layer(x)\n",
    "            \n",
    "            # 2. Batch Normalization (nur im Training mode)\n",
    "            if self.use_batch_norm and i < len(self.batch_norms):\n",
    "                x = self.batch_norms[i](x)\n",
    "            \n",
    "            # 3. Aktivierungsfunktion\n",
    "            x = self.leaky_relu(x)\n",
    "            \n",
    "            # 4. Dropout (nur im Training mode)\n",
    "            x = self.dropouts[i](x)\n",
    "        \n",
    "        # Output Layer (keine Aktivierung fÃ¼r Regression)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"ZÃ¤hlt trainierbare Parameter.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Verbessertes Modell erstellen\n",
    "print(f\"\\nðŸš€ MODELL ERSTELLEN:\")\n",
    "improved_model_v2 = ImprovedNeuralNetV2(\n",
    "    input_dim=pytorch_data['input_dim'],  # 38 Features\n",
    "    hidden_dims=[128, 64, 32],            # Weniger Parameter als vorher\n",
    "    dropout_rate=0.3,                     # 30% Base-Dropout\n",
    "    use_batch_norm=True                   # Batch Normalization an\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š MODELL-STATISTIKEN:\")\n",
    "print(f\"   Trainierbare Parameter: {improved_model_v2.count_parameters():,}\")\n",
    "print(f\"   vs. Baseline: {baseline_model.get_info()['total_params']:,}\")\n",
    "print(f\"   Reduktion: {baseline_model.get_info()['total_params'] - improved_model_v2.count_parameters():,}\")\n",
    "\n",
    "print(f\"\\nâœ… SCHRITT 2 ABGESCHLOSSEN\")\n",
    "print(f\"   âœ“ Architektur definiert\")\n",
    "print(f\"   âœ“ Anti-Overfitting Techniken integriert\")\n",
    "print(f\"   âœ“ Gewichte intelligent initialisiert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818606f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ðŸŽ¨ INTERAKTIVE MODELL-DESIGNENTSCHEIDUNGEN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸŽ¨ LASS UNS GEMEINSAM DAS OPTIMALE MODELL DESIGNEN!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Wir haben mehrere Designentscheidungen zu treffen. Ich erklÃ¤re jede Option \n",
    "und ihre Vor-/Nachteile, dann kÃ¶nnen Sie entscheiden:\n",
    "\n",
    "ðŸ“Š AKTUELLE BASELINE-PERFORMANCE:\n",
    "   â€¢ Validation RÂ²: {improved_results['val_r2']:.4f}\n",
    "   â€¢ Validation MAE: {improved_results['val_mae']:.2f}\n",
    "   â€¢ Overfitting Gap: {overfitting_gap:.4f}\n",
    "\n",
    "ðŸ” VERBESSERUNGSMÃ–GLICHKEITEN:\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DESIGNENTSCHEIDUNG 1: NETZWERK-ARCHITEKTUR\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "AKTUELLE ARCHITEKTUR: 38 â†’ 128 â†’ 64 â†’ 1\n",
    "\n",
    "OPTION A: Tieferes Netzwerk (mehr Layer)\n",
    "   ðŸ“ 38 â†’ 256 â†’ 128 â†’ 64 â†’ 32 â†’ 1\n",
    "   âœ… PRO: Kann komplexere Muster lernen\n",
    "   âŒ CONTRA: Mehr Parameter, hÃ¶heres Overfitting-Risiko\n",
    "   \n",
    "OPTION B: Breiteres Netzwerk (grÃ¶ÃŸere Layer) \n",
    "   ðŸ“ 38 â†’ 512 â†’ 256 â†’ 1\n",
    "   âœ… PRO: Mehr KapazitÃ¤t pro Layer\n",
    "   âŒ CONTRA: Viel mehr Parameter\n",
    "   \n",
    "OPTION C: Moderate Verbesserung\n",
    "   ðŸ“ 38 â†’ 256 â†’ 128 â†’ 64 â†’ 1\n",
    "   âœ… PRO: Ausgewogen zwischen KapazitÃ¤t und KomplexitÃ¤t\n",
    "   âŒ CONTRA: Nur moderate Verbesserung\n",
    "   \n",
    "OPTION D: Residual Connections (wie ResNet)\n",
    "   ðŸ“ Verbindungen die Layer Ã¼berspringen\n",
    "   âœ… PRO: Hilft bei Training tieferer Netze\n",
    "   âŒ CONTRA: Komplexer zu implementieren\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DESIGNENTSCHEIDUNG 2: REGULARISIERUNG\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "OPTION A: Aggressives Dropout\n",
    "   ðŸŽ›ï¸ Dropout-Raten: [0.5, 0.6, 0.5]\n",
    "   âœ… PRO: Starke Overfitting-Reduktion\n",
    "   âŒ CONTRA: Kann Underfitting verursachen\n",
    "   \n",
    "OPTION B: Moderates Dropout + Batch Norm\n",
    "   ðŸŽ›ï¸ Dropout-Raten: [0.3, 0.4, 0.3] + BatchNorm\n",
    "   âœ… PRO: Ausgewogen, stabileres Training\n",
    "   âŒ CONTRA: Mehr Hyperparameter\n",
    "   \n",
    "OPTION C: L2-Regularisierung + wenig Dropout\n",
    "   ðŸŽ›ï¸ Dropout-Raten: [0.2, 0.2, 0.2] + starkes Weight Decay\n",
    "   âœ… PRO: Glattere Gewichte\n",
    "   âŒ CONTRA: Kann zu konservativ sein\n",
    "   \n",
    "OPTION D: Early Stopping + moderate Regularisierung\n",
    "   ðŸŽ›ï¸ Stoppe Training bei Overfitting\n",
    "   âœ… PRO: Verhindert Overfitting automatisch\n",
    "   âŒ CONTRA: Braucht gute Validierung\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DESIGNENTSCHEIDUNG 3: AKTIVIERUNGSFUNKTIONEN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "AKTUELLE: ReLU\n",
    "\n",
    "OPTION A: LeakyReLU\n",
    "   âš¡ Kleine negative Werte bleiben erhalten\n",
    "   âœ… PRO: Keine \"toten\" Neuronen, bessere Gradients\n",
    "   âŒ CONTRA: ZusÃ¤tzlicher Hyperparameter (alpha)\n",
    "   \n",
    "OPTION B: ELU (Exponential Linear Unit)\n",
    "   âš¡ Glatte exponential Funktion\n",
    "   âœ… PRO: Glattere Funktion, bessere Konvergenz\n",
    "   âŒ CONTRA: Rechenintensiver\n",
    "   \n",
    "OPTION C: Swish/SiLU (x * sigmoid(x))\n",
    "   âš¡ Moderne, selbst-gated Aktivierung\n",
    "   âœ… PRO: Oft bessere Performance als ReLU\n",
    "   âŒ CONTRA: Noch rechenintensiver\n",
    "   \n",
    "OPTION D: GELU (Gaussian Error Linear Unit)\n",
    "   âš¡ Probabilistische Aktivierung\n",
    "   âœ… PRO: Wird in Transformer-Modellen verwendet\n",
    "   âŒ CONTRA: Komplex, nicht immer besser\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DESIGNENTSCHEIDUNG 4: TRAINING STRATEGY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\"\"\n",
    "OPTION A: Mehr Epochen + Early Stopping\n",
    "   ðŸ“ˆ 100-200 Epochen mit patience=15\n",
    "   âœ… PRO: Findet optimalen Stopp-Punkt\n",
    "   âŒ CONTRA: LÃ¤nger Training\n",
    "   \n",
    "OPTION B: Learning Rate Scheduling\n",
    "   ðŸ“ˆ Cosine Annealing oder Step Decay\n",
    "   âœ… PRO: Bessere Konvergenz\n",
    "   âŒ CONTRA: Mehr Hyperparameter\n",
    "   \n",
    "OPTION C: Ensemble von mehreren Modellen\n",
    "   ðŸ“ˆ 3-5 verschiedene Modelle kombinieren\n",
    "   âœ… PRO: Oft beste Performance\n",
    "   âŒ CONTRA: Viel mehr Rechenzeit\n",
    "   \n",
    "OPTION D: Gradient Clipping + Warmup\n",
    "   ðŸ“ˆ Stabileres Training\n",
    "   âœ… PRO: Verhindert exploding gradients\n",
    "   âŒ CONTRA: Komplexerer Setup\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ¤” WIE MÃ–CHTEN SIE VORGEHEN?\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "MÃ–GLICHKEITEN:\n",
    "1ï¸âƒ£ Diskutieren Sie eine spezifische Kategorie im Detail\n",
    "2ï¸âƒ£ Geben Sie Ihre PrÃ¤ferenzen an (z.B. \"C, B, A, A\")\n",
    "3ï¸âƒ£ Ich schlage eine Kombination vor basierend auf dem Overfitting\n",
    "4ï¸âƒ£ Wir testen mehrere Varianten parallel\n",
    "\n",
    "Was ist Ihr Ansatz? ðŸš€\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
