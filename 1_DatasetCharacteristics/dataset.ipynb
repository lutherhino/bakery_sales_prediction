{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecace47",
   "metadata": {},
   "source": [
    "# ü•ê B√§ckerei Umsatz Vorhersage - Trainingsdatensatz Erstellung\n",
    "\n",
    "Dieses Notebook erstellt einen kombinierten Trainingsdatensatz f√ºr die Vorhersage von B√§ckerei-Ums√§tzen basierend auf:\n",
    "- Historischen Umsatzdaten\n",
    "- Wetterdaten  \n",
    "- Feiertagen\n",
    "- Besonderen Events (Kieler Woche)\n",
    "- Preisindizes f√ºr Backwaren\n",
    "\n",
    "**Ziel:** Ein sauberer DataFrame f√ºr Regressionsanalyse und neuronale Netze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f911bc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Lade erforderliche Bibliotheken...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Deutsche Locale nicht verf√ºgbar - verwende Standard\n",
      "üïí Dataset-Erstellung gestartet: 01.07.2025 21:06:52\n",
      "‚úÖ Alle Bibliotheken erfolgreich geladen!\n"
     ]
    }
   ],
   "source": [
    "# 1. BIBLIOTHEKEN UND KONFIGURATION\n",
    "print(\"üìö Lade erforderliche Bibliotheken...\")\n",
    "\n",
    "# Basis-Bibliotheken f√ºr Datenmanipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualisierung\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# Konfiguration f√ºr bessere Darstellung\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# Pandas Anzeige-Optionen\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Seaborn Style\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Deutsche Locale f√ºr Datumsformatierung (falls verf√ºgbar)\n",
    "try:\n",
    "    import locale\n",
    "    locale.setlocale(locale.LC_TIME, 'de_DE.UTF-8')\n",
    "    print(\"üá©üá™ Deutsche Locale aktiviert\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Deutsche Locale nicht verf√ºgbar - verwende Standard\")\n",
    "\n",
    "# Zeitstempel f√ºr Dataset-Erstellung\n",
    "print(f\"üïí Dataset-Erstellung gestartet: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}\")\n",
    "print(\"‚úÖ Alle Bibliotheken erfolgreich geladen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e661c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Lade Umsatzdaten...\n",
      "Shape der Umsatzdaten: (9334, 4)\n",
      "Spalten: ['id', 'Datum', 'Warengruppe', 'Umsatz']\n",
      "\n",
      "Erste 5 Zeilen:\n",
      "        id       Datum  Warengruppe  Umsatz\n",
      "0  1307011  2013-07-01            1  148.83\n",
      "1  1307021  2013-07-02            1  159.79\n",
      "2  1307031  2013-07-03            1  111.89\n",
      "3  1307041  2013-07-04            1  168.86\n",
      "4  1307051  2013-07-05            1  171.28\n",
      "\n",
      "Erste 10 IDs: [1307011, 1307021, 1307031, 1307041, 1307051, 1307061, 1307071, 1307081, 1307091, 1307101]\n",
      "\n",
      "üîç SPALTEN-ANALYSE:\n",
      "Spalte 0: 'id' - Typ: int64\n",
      "  Wertebereich: 1307011.00 bis 1807315.00\n",
      "  Beispielwerte: [1307011, 1307021, 1307031]\n",
      "\n",
      "Spalte 1: 'Datum' - Typ: object\n",
      "  Beispielwerte: ['2013-07-01', '2013-07-02', '2013-07-03']\n",
      "\n",
      "Spalte 2: 'Warengruppe' - Typ: int64\n",
      "  Wertebereich: 1.00 bis 6.00\n",
      "  Beispielwerte: [1, 1, 1]\n",
      "\n",
      "Spalte 3: 'Umsatz' - Typ: float64\n",
      "  Wertebereich: 7.05 bis 1879.46\n",
      "  Beispielwerte: [148.828353112183, 159.79375714468, 111.885593514353]\n",
      "\n",
      "‚úÖ Umsatzspalte 'Umsatz' gefunden!\n",
      "\n",
      "üìä VERWENDETE UMSATZSPALTE: 'Umsatz'\n",
      "\n",
      "üîÑ Extrahiere Datum und Warengruppe aus ID (Format: YYMMDDW)...\n",
      "Vor Bereinigung: 9334 Zeilen\n",
      "Fehlende Daten: 0\n",
      "Nach Bereinigung: 9334 Zeilen\n",
      "Datum-Datentyp: datetime64[ns]\n",
      "Umsatz-Datentyp: float64\n",
      "‚úÖ 9334 g√ºltige Datens√§tze nach Extraktion\n",
      "üìÖ Zeitraum: 2013-07-01 00:00:00 bis 2018-07-31 00:00:00\n",
      "üè∑Ô∏è Warengruppen: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "üí∞ Umsatz-Vorschau: count   9334.00\n",
      "mean     206.75\n",
      "std      144.55\n",
      "min        7.05\n",
      "25%       96.90\n",
      "50%      161.90\n",
      "75%      280.64\n",
      "max     1879.46\n",
      "Name: Umsatz, dtype: float64\n",
      "Vor Bereinigung: 9334 Zeilen\n",
      "Fehlende Daten: 0\n",
      "Nach Bereinigung: 9334 Zeilen\n",
      "Datum-Datentyp: datetime64[ns]\n",
      "Umsatz-Datentyp: float64\n",
      "‚úÖ 9334 g√ºltige Datens√§tze nach Extraktion\n",
      "üìÖ Zeitraum: 2013-07-01 00:00:00 bis 2018-07-31 00:00:00\n",
      "üè∑Ô∏è Warengruppen: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "üí∞ Umsatz-Vorschau: count   9334.00\n",
      "mean     206.75\n",
      "std      144.55\n",
      "min        7.05\n",
      "25%       96.90\n",
      "50%      161.90\n",
      "75%      280.64\n",
      "max     1879.46\n",
      "Name: Umsatz, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 2. UMSATZDATEN LADEN UND VERARBEITEN\n",
    "print(\"üìä Lade Umsatzdaten...\")\n",
    "\n",
    "# Umsatzdaten von GitHub laden\n",
    "umsatz_url = \"https://raw.githubusercontent.com/opencampus-sh/einfuehrung-in-data-science-und-ml/main/umsatzdaten_gekuerzt.csv\"\n",
    "df_umsatz = pd.read_csv(umsatz_url)\n",
    "\n",
    "print(f\"Shape der Umsatzdaten: {df_umsatz.shape}\")\n",
    "print(f\"Spalten: {list(df_umsatz.columns)}\")\n",
    "print(\"\\nErste 5 Zeilen:\")\n",
    "print(df_umsatz.head())\n",
    "\n",
    "# Debugging: Schaue dir die ersten paar IDs an\n",
    "print(f\"\\nErste 10 IDs: {df_umsatz.iloc[:10, 0].tolist()}\")\n",
    "\n",
    "# IDENTIFIZIERE DIE UMSATZSPALTE\n",
    "print(f\"\\nüîç SPALTEN-ANALYSE:\")\n",
    "for i, col in enumerate(df_umsatz.columns):\n",
    "    print(f\"Spalte {i}: '{col}' - Typ: {df_umsatz[col].dtype}\")\n",
    "    if df_umsatz[col].dtype in ['float64', 'int64']:\n",
    "        print(f\"  Wertebereich: {df_umsatz[col].min():.2f} bis {df_umsatz[col].max():.2f}\")\n",
    "    print(f\"  Beispielwerte: {df_umsatz[col].head(3).tolist()}\")\n",
    "    print()\n",
    "\n",
    "# Suche nach der Umsatzspalte\n",
    "umsatz_spalte = None\n",
    "possible_umsatz_names = ['umsatz', 'Umsatz', 'sales', 'revenue', 'amount']\n",
    "\n",
    "# Pr√ºfe explizit nach \"umsatz\" Spalte\n",
    "if 'umsatz' in df_umsatz.columns:\n",
    "    umsatz_spalte = 'umsatz'\n",
    "    print(\"‚úÖ Umsatzspalte 'umsatz' gefunden!\")\n",
    "elif 'Umsatz' in df_umsatz.columns:\n",
    "    umsatz_spalte = 'Umsatz'\n",
    "    print(\"‚úÖ Umsatzspalte 'Umsatz' gefunden!\")\n",
    "else:\n",
    "    # Falls keine explizite Umsatzspalte, nimm die zweite numerische Spalte\n",
    "    numeric_cols = df_umsatz.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) >= 2:\n",
    "        umsatz_spalte = numeric_cols[1]  # Zweite numerische Spalte\n",
    "        print(f\"‚ö†Ô∏è Keine explizite Umsatzspalte gefunden. Verwende '{umsatz_spalte}' als Umsatz.\")\n",
    "    elif len(numeric_cols) == 1:\n",
    "        umsatz_spalte = numeric_cols[0]\n",
    "        print(f\"‚ö†Ô∏è Nur eine numerische Spalte gefunden. Verwende '{umsatz_spalte}' als Umsatz.\")\n",
    "    else:\n",
    "        print(\"‚ùå Keine numerische Spalte f√ºr Umsatz gefunden!\")\n",
    "        raise ValueError(\"Keine Umsatzspalte identifiziert!\")\n",
    "\n",
    "print(f\"\\nüìä VERWENDETE UMSATZSPALTE: '{umsatz_spalte}'\")\n",
    "\n",
    "# Datum aus der ID extrahieren (Format: YYMMDDW)\n",
    "def extract_date_info(row_id):\n",
    "    \"\"\"Extrahiert Datum und Warengruppe aus der ID (Format: YYMMDDW)\"\"\"\n",
    "    try:\n",
    "        id_str = str(int(row_id))\n",
    "        \n",
    "        # YYMMDD + W (Warengruppe) - 7 Stellen erwartet\n",
    "        if len(id_str) == 7:\n",
    "            jahr_2stellig = int(id_str[:2])\n",
    "            monat = int(id_str[2:4])\n",
    "            tag = int(id_str[4:6])\n",
    "            warengruppe = int(id_str[6])\n",
    "            \n",
    "            # 2-stelliges Jahr zu 4-stelligem Jahr konvertieren\n",
    "            # Annahme: 00-30 = 2000-2030, 31-99 = 1931-1999\n",
    "            if jahr_2stellig <= 30:\n",
    "                jahr = 2000 + jahr_2stellig\n",
    "            else:\n",
    "                jahr = 1900 + jahr_2stellig\n",
    "            \n",
    "            # Datum validieren und erstellen\n",
    "            datum = pd.to_datetime(f\"{jahr}-{monat:02d}-{tag:02d}\")\n",
    "            return datum, warengruppe\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Ung√ºltige ID-L√§nge: {id_str} (L√§nge: {len(id_str)}, erwartet: 7)\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Fehler bei ID {row_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Datum und Warengruppe extrahieren\n",
    "print(\"\\nüîÑ Extrahiere Datum und Warengruppe aus ID (Format: YYMMDDW)...\")\n",
    "df_umsatz[['Datum', 'Warengruppe']] = df_umsatz.iloc[:, 0].apply(\n",
    "    lambda x: pd.Series(extract_date_info(x))\n",
    ")\n",
    "\n",
    "print(f\"Vor Bereinigung: {len(df_umsatz)} Zeilen\")\n",
    "print(f\"Fehlende Daten: {df_umsatz['Datum'].isna().sum()}\")\n",
    "\n",
    "# Ung√ºltige Daten entfernen\n",
    "df_umsatz = df_umsatz.dropna(subset=['Datum', 'Warengruppe'])\n",
    "print(f\"Nach Bereinigung: {len(df_umsatz)} Zeilen\")\n",
    "\n",
    "# Datentypen explizit setzen\n",
    "df_umsatz['Datum'] = pd.to_datetime(df_umsatz['Datum'])\n",
    "df_umsatz['Warengruppe'] = df_umsatz['Warengruppe'].astype(int)\n",
    "\n",
    "# Sicherstellen, dass Umsatz numerisch ist\n",
    "df_umsatz[umsatz_spalte] = pd.to_numeric(df_umsatz[umsatz_spalte], errors='coerce')\n",
    "\n",
    "print(f\"Datum-Datentyp: {df_umsatz['Datum'].dtype}\")\n",
    "print(f\"Umsatz-Datentyp: {df_umsatz[umsatz_spalte].dtype}\")\n",
    "\n",
    "# Weitere Datums-Features erstellen (ohne Sonntag-bezogene Features)\n",
    "df_umsatz['Jahr'] = df_umsatz['Datum'].dt.year\n",
    "df_umsatz['Monat'] = df_umsatz['Datum'].dt.month\n",
    "df_umsatz['Tag'] = df_umsatz['Datum'].dt.day\n",
    "df_umsatz['Wochentag'] = df_umsatz['Datum'].dt.day_name()\n",
    "df_umsatz['Wochentag_Nr'] = df_umsatz['Datum'].dt.dayofweek\n",
    "\n",
    "print(f\"‚úÖ {len(df_umsatz)} g√ºltige Datens√§tze nach Extraktion\")\n",
    "print(f\"üìÖ Zeitraum: {df_umsatz['Datum'].min()} bis {df_umsatz['Datum'].max()}\")\n",
    "print(f\"üè∑Ô∏è Warengruppen: {sorted(df_umsatz['Warengruppe'].unique())}\")\n",
    "print(f\"üí∞ Umsatz-Vorschau: {df_umsatz[umsatz_spalte].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e910a8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ df_umsatz gefunden mit 9334 Zeilen\n",
      "üîÑ Erstelle Bin√§rkodierung f√ºr Warengruppen mit 0/1...\n",
      "‚úÖ Warengruppen-Features mit expliziter 0/1-Kodierung erstellt:\n",
      "['Warengruppe_Brot', 'Warengruppe_Br√∂tchen', 'Warengruppe_Croissant', 'Warengruppe_Konditorei', 'Warengruppe_Kuchen', 'Warengruppe_Saisonbrot']\n",
      "\n",
      "üîç Bin√§rkodierung-√úberpr√ºfung:\n",
      "Warengruppe_Brot: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Br√∂tchen: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Croissant: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Konditorei: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Kuchen: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Saisonbrot: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "\n",
      "üìà Dataset Info:\n",
      "Zeilen: 9334\n",
      "Spalten: 16\n",
      "Umsatzbereich: .2f ‚Ç¨ - .2f ‚Ç¨\n"
     ]
    }
   ],
   "source": [
    "# 3. WARENGRUPPEN IN LESBARE NAMEN UMWANDELN UND BIN√ÑRKODIEREN\n",
    "# Sicherheits√ºberpr√ºfung: Stelle sicher, dass df_umsatz existiert\n",
    "try:\n",
    "    if 'df_umsatz' not in locals():\n",
    "        raise NameError(\"df_umsatz ist nicht definiert. Bitte f√ºhre zuerst Zelle 3 (Umsatzdaten laden) aus.\")\n",
    "    \n",
    "    print(f\"‚úÖ df_umsatz gefunden mit {len(df_umsatz)} Zeilen\")\n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Fehler: {e}\")\n",
    "    print(\"üîÑ F√ºhre die Zellen in der richtigen Reihenfolge aus:\")\n",
    "    print(\"   1. Bibliotheken laden\")\n",
    "    print(\"   2. Umsatzdaten laden\")\n",
    "    print(\"   3. Warengruppen verarbeiten\")\n",
    "    raise\n",
    "\n",
    "warengruppen_mapping = {\n",
    "    1: \"Brot\",\n",
    "    2: \"Br√∂tchen\", \n",
    "    3: \"Croissant\",\n",
    "    4: \"Konditorei\",\n",
    "    5: \"Kuchen\",\n",
    "    6: \"Saisonbrot\"\n",
    "}\n",
    "\n",
    "df_umsatz['Warengruppe_Name'] = df_umsatz['Warengruppe'].map(warengruppen_mapping)\n",
    "\n",
    "# Bin√§rkodierung f√ºr Warengruppen mit expliziten 0/1-Werten\n",
    "print(\"üîÑ Erstelle Bin√§rkodierung f√ºr Warengruppen mit 0/1...\")\n",
    "for warengruppe_nr, warengruppe_name in warengruppen_mapping.items():\n",
    "    # Erstelle bin√§re Spalte: 1 wenn Warengruppe zutrifft, 0 sonst\n",
    "    df_umsatz[f'Warengruppe_{warengruppe_name}'] = (df_umsatz['Warengruppe'] == warengruppe_nr).astype(int)\n",
    "\n",
    "print(\"‚úÖ Warengruppen-Features mit expliziter 0/1-Kodierung erstellt:\")\n",
    "warengruppen_cols = [col for col in df_umsatz.columns if col.startswith('Warengruppe_') and col != 'Warengruppe_Name']\n",
    "print(warengruppen_cols)\n",
    "\n",
    "# √úberpr√ºfe die Bin√§rkodierung\n",
    "print(f\"\\nüîç Bin√§rkodierung-√úberpr√ºfung:\")\n",
    "for col in warengruppen_cols:\n",
    "    unique_values = df_umsatz[col].unique()\n",
    "    print(f\"{col}: Eindeutige Werte = {sorted(unique_values)} (Typ: {df_umsatz[col].dtype})\")\n",
    "\n",
    "# √úberblick √ºber die Daten\n",
    "print(f\"\\nüìà Dataset Info:\")\n",
    "print(f\"Zeilen: {len(df_umsatz)}\")\n",
    "print(f\"Spalten: {len(df_umsatz.columns)}\")\n",
    "print(f\"Umsatzbereich: {df_umsatz.iloc[:, 1].min():.2f} ‚Ç¨ - {df_umsatz.iloc[:, 1].max():.2f} ‚Ç¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1730ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Analysiere Umsatzdaten ohne Manipulation...\n",
      "Verwende Umsatz-Spalte: 'Umsatz'\n",
      "\n",
      "üîç ORIGINAL UMSATZDATEN-ANALYSE:\n",
      "Anzahl Transaktionen: 9334\n",
      "Umsatzbereich: 7.05 ‚Ç¨ bis 1879.46 ‚Ç¨\n",
      "Durchschnittsumsatz: 206.75 ‚Ç¨\n",
      "Negative Ums√§tze: 0 Transaktionen (0.0%)\n",
      "\n",
      "‚úÖ Umsatzdaten bleiben unver√§ndert\n",
      "üìä Finale Anzahl Datens√§tze: 9334\n"
     ]
    }
   ],
   "source": [
    "# 4. UMSATZANALYSE UND BEREINIGUNG\n",
    "print(\"üìä Analysiere Umsatzdaten ohne Manipulation...\")\n",
    "\n",
    "# Verwende die bereits identifizierte Umsatzspalte\n",
    "if 'umsatz_spalte' not in locals():\n",
    "    if 'umsatz' in df_umsatz.columns:\n",
    "        umsatz_spalte = 'umsatz'\n",
    "    elif 'Umsatz' in df_umsatz.columns:\n",
    "        umsatz_spalte = 'Umsatz'\n",
    "    else:\n",
    "        umsatz_spalte = df_umsatz.columns[1]\n",
    "\n",
    "print(f\"Verwende Umsatz-Spalte: '{umsatz_spalte}'\")\n",
    "\n",
    "# Sicherstellen, dass Umsatz numerisch ist\n",
    "df_umsatz[umsatz_spalte] = pd.to_numeric(df_umsatz[umsatz_spalte], errors='coerce')\n",
    "\n",
    "# WICHTIG: Keine Manipulation der Umsatzdaten!\n",
    "# Negative Ums√§tze sind valide (R√ºckgaben/Stornierungen)\n",
    "# Keine Aggregation - behalte Einzeltransaktionen\n",
    "\n",
    "print(f\"\\nüîç ORIGINAL UMSATZDATEN-ANALYSE:\")\n",
    "print(f\"Anzahl Transaktionen: {len(df_umsatz)}\")\n",
    "print(f\"Umsatzbereich: {df_umsatz[umsatz_spalte].min():.2f} ‚Ç¨ bis {df_umsatz[umsatz_spalte].max():.2f} ‚Ç¨\")\n",
    "print(f\"Durchschnittsumsatz: {df_umsatz[umsatz_spalte].mean():.2f} ‚Ç¨\")\n",
    "print(f\"Negative Ums√§tze: {(df_umsatz[umsatz_spalte] < 0).sum()} Transaktionen ({(df_umsatz[umsatz_spalte] < 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Zeige Beispiele negativer Ums√§tze (als Information, nicht zur Entfernung)\n",
    "negative_sales = df_umsatz[df_umsatz[umsatz_spalte] < 0]\n",
    "if len(negative_sales) > 0:\n",
    "    print(f\"\\nüìã NEGATIVE UMS√ÑTZE (Beispiele - das sind legitime R√ºckgaben/Stornierungen):\")\n",
    "    print(negative_sales[['Datum', 'Warengruppe_Name', umsatz_spalte]].head())\n",
    "\n",
    "# KEINE AGGREGATION - behalte die Daten wie sie sind!\n",
    "print(f\"\\n‚úÖ Umsatzdaten bleiben unver√§ndert\")\n",
    "print(f\"üìä Finale Anzahl Datens√§tze: {len(df_umsatz)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d08b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå§Ô∏è Lade Wetterdaten...\n",
      "Shape der Wetterdaten: (2601, 5)\n",
      "Spalten: ['Datum', 'Bewoelkung', 'Temperatur', 'Windgeschwindigkeit', 'Wettercode']\n",
      "\n",
      "Erste 5 Zeilen Wetterdaten:\n",
      "        Datum  Bewoelkung  Temperatur  Windgeschwindigkeit  Wettercode\n",
      "0  2012-01-01        8.00        9.82                   14       58.00\n",
      "1  2012-01-02        7.00        7.44                   12         NaN\n",
      "2  2012-01-03        8.00        5.54                   18       63.00\n",
      "3  2012-01-04        4.00        5.69                   19       80.00\n",
      "4  2012-01-05        6.00        5.30                   23       80.00\n",
      "üìä Temperatur: Fehlende Werte mit Median 12.00 gef√ºllt\n",
      "üìä Windgeschwindigkeit: Fehlende Werte mit Median 10.00 gef√ºllt\n",
      "üìä Bewoelkung: Fehlende Werte mit Median 6.00 gef√ºllt\n",
      "\n",
      "üîó Verkn√ºpfe Wetter- mit Umsatzdaten...\n",
      "‚úÖ Kombinierter Datensatz: (9334, 21)\n",
      "üå°Ô∏è Temperaturbereich: -8.5¬∞C - 31.4¬∞C\n",
      "‚ö†Ô∏è 16 Tage ohne Wetterdaten - werden mit Durchschnitt gef√ºllt\n"
     ]
    }
   ],
   "source": [
    "# 4. WETTERDATEN LADEN UND VERKN√úPFEN\n",
    "print(\"üå§Ô∏è Lade Wetterdaten...\")\n",
    "\n",
    "# Wetterdaten von GitHub laden\n",
    "wetter_url = \"https://raw.githubusercontent.com/opencampus-sh/einfuehrung-in-data-science-und-ml/main/wetter.csv\"\n",
    "df_wetter = pd.read_csv(wetter_url)\n",
    "\n",
    "print(f\"Shape der Wetterdaten: {df_wetter.shape}\")\n",
    "print(f\"Spalten: {list(df_wetter.columns)}\")\n",
    "print(\"\\nErste 5 Zeilen Wetterdaten:\")\n",
    "print(df_wetter.head())\n",
    "\n",
    "# Datum in Wetterdaten konvertieren\n",
    "df_wetter['Datum'] = pd.to_datetime(df_wetter['Datum'])\n",
    "\n",
    "# Feature: Wettercode fehlt\n",
    "df_wetter['Wettercode_fehlt'] = df_wetter['Wettercode'].isna().astype(int)\n",
    "\n",
    "# Fehlende Wetterdaten mit Median f√ºllen\n",
    "numeric_weather_cols = ['Temperatur', 'Windgeschwindigkeit', 'Bewoelkung']\n",
    "for col in numeric_weather_cols:\n",
    "    if col in df_wetter.columns:\n",
    "        median_val = df_wetter[col].median()\n",
    "        df_wetter[col] = df_wetter[col].fillna(median_val)\n",
    "        print(f\"üìä {col}: Fehlende Werte mit Median {median_val:.2f} gef√ºllt\")\n",
    "\n",
    "# Wetterdaten mit Umsatzdaten verkn√ºpfen\n",
    "print(\"\\nüîó Verkn√ºpfe Wetter- mit Umsatzdaten...\")\n",
    "df_combined = df_umsatz.merge(df_wetter, on='Datum', how='left')\n",
    "\n",
    "print(f\"‚úÖ Kombinierter Datensatz: {df_combined.shape}\")\n",
    "print(f\"üå°Ô∏è Temperaturbereich: {df_combined['Temperatur'].min():.1f}¬∞C - {df_combined['Temperatur'].max():.1f}¬∞C\")\n",
    "\n",
    "# Fehlende Wetterdaten anzeigen\n",
    "missing_weather = df_combined['Temperatur'].isna().sum()\n",
    "if missing_weather > 0:\n",
    "    print(f\"‚ö†Ô∏è {missing_weather} Tage ohne Wetterdaten - werden mit Durchschnitt gef√ºllt\")\n",
    "    for col in numeric_weather_cols:\n",
    "        if col in df_combined.columns:\n",
    "            df_combined[col] = df_combined[col].fillna(df_combined[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd74113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÑ Lade Feiertagsdaten...\n",
      "Shape der Feiertagsdaten: (468, 2)\n",
      "Spalten: ['Datum', 'Feiertag']\n",
      "\n",
      "Erste 5 Feiertage:\n",
      "        Datum             Feiertag\n",
      "0  01.01.2000              Neujahr\n",
      "1  21.04.2000           Karfreitag\n",
      "2  23.04.2000               Ostern\n",
      "3  24.04.2000          Ostermontag\n",
      "4  01.06.2000  Christi Himmelfahrt\n",
      "Beispiel-Datum Format: '01.01.2000'\n",
      "üìÖ Gefiltert von 468 auf 78 Feiertage (2013-2018)\n",
      "‚úÖ 78 Feiertage im relevanten Zeitraum gefunden\n",
      "üìÖ Tage mit Feiertag im Dataset: 201\n",
      "\n",
      "üéÑ Beispiel-Feiertage:\n",
      "         Datum\n",
      "169 2013-01-01\n",
      "170 2013-03-29\n",
      "171 2013-03-31\n",
      "172 2013-04-01\n",
      "173 2013-05-09\n",
      "174 2013-05-19\n",
      "175 2013-05-20\n",
      "176 2013-05-01\n",
      "177 2013-10-03\n",
      "178 2013-12-24\n",
      "\n",
      "üìä FEIERTAG-STATISTIKEN:\n",
      "Feiertage im Dataset: 201 von 9334 Tagen (2.2%)\n",
      "üí∞ Durchschnittsumsatz an Feiertagen: 299.58 ‚Ç¨\n",
      "üí∞ Durchschnittsumsatz an normalen Tagen: 204.71 ‚Ç¨\n",
      "üìà Feiertag vs. Normal Ratio: 1.46\n"
     ]
    }
   ],
   "source": [
    "# 5. FEIERTAGE LADEN UND VERKN√úPFEN\n",
    "print(\"üéÑ Lade Feiertagsdaten...\")\n",
    "\n",
    "try:\n",
    "    # Feiertage aus lokaler Datei laden\n",
    "    feiertage_path = \"/workspaces/bakery_sales_prediction/5_Datasets/DE-Feiertage_2020_bis_2035.csv\"\n",
    "    \n",
    "    # Versuche verschiedene Separator und Encoding\n",
    "    try:\n",
    "        df_feiertage = pd.read_csv(feiertage_path, sep=';', encoding='utf-8')\n",
    "    except:\n",
    "        try:\n",
    "            df_feiertage = pd.read_csv(feiertage_path, sep=',', encoding='utf-8')\n",
    "        except:\n",
    "            df_feiertage = pd.read_csv(feiertage_path, sep=';', encoding='latin-1')\n",
    "    \n",
    "    print(f\"Shape der Feiertagsdaten: {df_feiertage.shape}\")\n",
    "    print(f\"Spalten: {list(df_feiertage.columns)}\")\n",
    "    print(\"\\nErste 5 Feiertage:\")\n",
    "    print(df_feiertage.head())\n",
    "    \n",
    "    # Datum in Feiertagsdaten konvertieren\n",
    "    datum_col = df_feiertage.columns[0]  # Erste Spalte als Datum\n",
    "    \n",
    "    # Pr√ºfe das Datumsformat und konvertiere entsprechend\n",
    "    sample_date = str(df_feiertage[datum_col].iloc[0])\n",
    "    print(f\"Beispiel-Datum Format: '{sample_date}'\")\n",
    "    \n",
    "    if ';' in sample_date:\n",
    "        # Format: \"01.01.2000;Neujahr\" - extrahiere nur das Datum\n",
    "        df_feiertage[datum_col] = df_feiertage[datum_col].str.split(';').str[0]\n",
    "        print(\"‚úÖ Semikolon-separierte Daten erkannt und aufgeteilt\")\n",
    "    \n",
    "    # Konvertiere Datum mit deutschem Format (DD.MM.YYYY)\n",
    "    try:\n",
    "        df_feiertage['Datum'] = pd.to_datetime(df_feiertage[datum_col], format='%d.%m.%Y')\n",
    "    except ValueError:\n",
    "        try:\n",
    "            df_feiertage['Datum'] = pd.to_datetime(df_feiertage[datum_col], dayfirst=True)\n",
    "        except:\n",
    "            # Fallback: Automatische Erkennung\n",
    "            df_feiertage['Datum'] = pd.to_datetime(df_feiertage[datum_col], infer_datetime_format=True)\n",
    "    \n",
    "    # Nur relevante Jahre filtern (2013-2018)\n",
    "    original_count = len(df_feiertage)\n",
    "    df_feiertage = df_feiertage[\n",
    "        (df_feiertage['Datum'].dt.year >= 2013) & \n",
    "        (df_feiertage['Datum'].dt.year <= 2018)\n",
    "    ]\n",
    "    \n",
    "    print(f\"üìÖ Gefiltert von {original_count} auf {len(df_feiertage)} Feiertage (2013-2018)\")\n",
    "    \n",
    "    # Bin√§re Feiertag-Spalte erstellen\n",
    "    feiertage_dates = set(df_feiertage['Datum'].dt.date)\n",
    "    df_combined['ist_feiertag'] = df_combined['Datum'].dt.date.isin(feiertage_dates).astype(int)\n",
    "    \n",
    "    print(f\"‚úÖ {len(feiertage_dates)} Feiertage im relevanten Zeitraum gefunden\")\n",
    "    print(f\"üìÖ Tage mit Feiertag im Dataset: {df_combined['ist_feiertag'].sum()}\")\n",
    "    \n",
    "    # Zeige einige Beispiel-Feiertage\n",
    "    if len(df_feiertage) > 0:\n",
    "        print(f\"\\nüéÑ Beispiel-Feiertage:\")\n",
    "        print(df_feiertage[['Datum']].head(10))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Feiertagsdatei nicht gefunden - erstelle manuell Feiertage\")\n",
    "    # Manuelle Definition wichtiger deutscher Feiertage\n",
    "    deutsche_feiertage = [\n",
    "        # 2013\n",
    "        '2013-01-01', '2013-03-29', '2013-04-01', '2013-05-01', '2013-05-09', '2013-05-20', \n",
    "        '2013-10-03', '2013-12-25', '2013-12-26',\n",
    "        # 2014\n",
    "        '2014-01-01', '2014-04-18', '2014-04-21', '2014-05-01', '2014-05-29', '2014-06-09',\n",
    "        '2014-10-03', '2014-12-25', '2014-12-26',\n",
    "        # 2015\n",
    "        '2015-01-01', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25',\n",
    "        '2015-10-03', '2015-12-25', '2015-12-26',\n",
    "        # 2016\n",
    "        '2016-01-01', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16',\n",
    "        '2016-10-03', '2016-12-25', '2016-12-26',\n",
    "        # 2017\n",
    "        '2017-01-01', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05',\n",
    "        '2017-10-03', '2017-12-25', '2017-12-26',\n",
    "        # 2018\n",
    "        '2018-01-01', '2018-03-30', '2018-04-02', '2018-05-01', '2018-05-10', '2018-05-21',\n",
    "        '2018-10-03', '2018-12-25', '2018-12-26'\n",
    "    ]\n",
    "    \n",
    "    feiertage_dates = set(pd.to_datetime(deutsche_feiertage).date)\n",
    "    df_combined['ist_feiertag'] = df_combined['Datum'].dt.date.isin(feiertage_dates).astype(int)\n",
    "    print(f\"‚úÖ {len(feiertage_dates)} manuelle Feiertage erstellt\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Fehler beim Verarbeiten der Feiertage: {e}\")\n",
    "    print(\"Erstelle Standard-Feiertage als Fallback...\")\n",
    "    \n",
    "    # Minimale Feiertage als Fallback\n",
    "    standard_feiertage = [\n",
    "        '2013-01-01', '2013-12-25', '2014-01-01', '2014-12-25',\n",
    "        '2015-01-01', '2015-12-25', '2016-01-01', '2016-12-25',\n",
    "        '2017-01-01', '2017-12-25', '2018-01-01', '2018-12-25'\n",
    "    ]\n",
    "    \n",
    "    feiertage_dates = set(pd.to_datetime(standard_feiertage).date)\n",
    "    df_combined['ist_feiertag'] = df_combined['Datum'].dt.date.isin(feiertage_dates).astype(int)\n",
    "    print(f\"‚úÖ {len(feiertage_dates)} Standard-Feiertage erstellt\")\n",
    "\n",
    "# Feiertags-Statistiken\n",
    "feiertag_count = df_combined['ist_feiertag'].sum()\n",
    "total_days = len(df_combined)\n",
    "feiertag_percent = (feiertag_count / total_days) * 100\n",
    "\n",
    "print(f\"\\nüìä FEIERTAG-STATISTIKEN:\")\n",
    "print(f\"Feiertage im Dataset: {feiertag_count} von {total_days} Tagen ({feiertag_percent:.1f}%)\")\n",
    "\n",
    "if feiertag_count > 0:\n",
    "    # Analysiere Umsatz an Feiertagen vs. normale Tage - FIX: Verwende korrekte Umsatz-Spalte\n",
    "    # Identifiziere die korrekte Umsatz-Spalte\n",
    "    if 'umsatz_spalte' in locals() and umsatz_spalte in df_combined.columns:\n",
    "        umsatz_col_name = umsatz_spalte\n",
    "    else:\n",
    "        # Fallback: Suche nach der Umsatz-Spalte\n",
    "        numeric_cols = df_combined.select_dtypes(include=[np.number]).columns\n",
    "        umsatz_col_name = None\n",
    "        for col in ['umsatz', 'Umsatz']:\n",
    "            if col in df_combined.columns:\n",
    "                umsatz_col_name = col\n",
    "                break\n",
    "        \n",
    "        if umsatz_col_name is None and len(numeric_cols) > 1:\n",
    "            # Nimm die zweite numerische Spalte (erste ist meist ID)\n",
    "            umsatz_col_name = numeric_cols[1]\n",
    "        elif umsatz_col_name is None and len(numeric_cols) == 1:\n",
    "            umsatz_col_name = numeric_cols[0]\n",
    "    \n",
    "    if umsatz_col_name and umsatz_col_name in df_combined.columns:\n",
    "        try:\n",
    "            # Stelle sicher, dass die Umsatz-Spalte numerisch ist\n",
    "            df_combined[umsatz_col_name] = pd.to_numeric(df_combined[umsatz_col_name], errors='coerce')\n",
    "            \n",
    "            feiertag_umsatz = df_combined[df_combined['ist_feiertag'] == 1][umsatz_col_name].mean()\n",
    "            normal_umsatz = df_combined[df_combined['ist_feiertag'] == 0][umsatz_col_name].mean()\n",
    "            \n",
    "            # Pr√ºfe ob die Werte numerisch sind\n",
    "            if pd.notna(feiertag_umsatz) and pd.notna(normal_umsatz) and normal_umsatz != 0:\n",
    "                print(f\"üí∞ Durchschnittsumsatz an Feiertagen: {feiertag_umsatz:.2f} ‚Ç¨\")\n",
    "                print(f\"üí∞ Durchschnittsumsatz an normalen Tagen: {normal_umsatz:.2f} ‚Ç¨\")\n",
    "                print(f\"üìà Feiertag vs. Normal Ratio: {feiertag_umsatz/normal_umsatz:.2f}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Konnte Umsatz-Statistiken nicht berechnen (Feiertag: {feiertag_umsatz}, Normal: {normal_umsatz})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Fehler bei Umsatz-Statistiken: {e}\")\n",
    "            print(f\"   Umsatz-Spalte: '{umsatz_col_name}', Typ: {df_combined[umsatz_col_name].dtype}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Keine g√ºltige Umsatz-Spalte gefunden f√ºr Statistiken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dad5240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå∏ Erstelle Jahreszeiten-Feature...\n",
      "üîÑ Erstelle Bin√§rkodierung f√ºr Jahreszeiten mit 0/1...\n",
      "‚úÖ Jahreszeiten-Features mit expliziter 0/1-Kodierung erstellt:\n",
      "['Jahreszeit_Winter', 'Jahreszeit_Fr√ºhling', 'Jahreszeit_Sommer', 'Jahreszeit_Herbst']\n",
      "\n",
      "üîç Jahreszeiten-Bin√§rkodierung-√úberpr√ºfung:\n",
      "Jahreszeit_Winter: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Fr√ºhling: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Sommer: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Herbst: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "\n",
      "üìä Verteilung der Jahreszeiten:\n",
      "Jahreszeit\n",
      "Herbst      2410\n",
      "Sommer      2405\n",
      "Winter      2293\n",
      "Fr√ºhling    2226\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 6. JAHRESZEIT BESTIMMEN\n",
    "print(\"üå∏ Erstelle Jahreszeiten-Feature...\")\n",
    "\n",
    "def get_season(month):\n",
    "    \"\"\"Bestimmt die Jahreszeit basierend auf dem Monat\"\"\"\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Fr√ºhling\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Sommer\"\n",
    "    else:  # 9, 10, 11\n",
    "        return \"Herbst\"\n",
    "\n",
    "df_combined['Jahreszeit'] = df_combined['Monat'].apply(get_season)\n",
    "\n",
    "# Bin√§rkodierung f√ºr Jahreszeiten mit expliziten 0/1-Werten\n",
    "print(\"üîÑ Erstelle Bin√§rkodierung f√ºr Jahreszeiten mit 0/1...\")\n",
    "jahreszeiten = ['Winter', 'Fr√ºhling', 'Sommer', 'Herbst']\n",
    "for jahreszeit in jahreszeiten:\n",
    "    df_combined[f'Jahreszeit_{jahreszeit}'] = (df_combined['Jahreszeit'] == jahreszeit).astype(int)\n",
    "\n",
    "print(\"‚úÖ Jahreszeiten-Features mit expliziter 0/1-Kodierung erstellt:\")\n",
    "jahreszeiten_cols = [col for col in df_combined.columns if col.startswith('Jahreszeit_')]\n",
    "print(jahreszeiten_cols)\n",
    "\n",
    "# √úberpr√ºfe die Bin√§rkodierung\n",
    "print(f\"\\nüîç Jahreszeiten-Bin√§rkodierung-√úberpr√ºfung:\")\n",
    "for col in jahreszeiten_cols:\n",
    "    unique_values = df_combined[col].unique()\n",
    "    print(f\"{col}: Eindeutige Werte = {sorted(unique_values)} (Typ: {df_combined[col].dtype})\")\n",
    "\n",
    "# Verteilung der Jahreszeiten anzeigen\n",
    "jahreszeit_verteilung = df_combined['Jahreszeit'].value_counts()\n",
    "print(f\"\\nüìä Verteilung der Jahreszeiten:\")\n",
    "print(jahreszeit_verteilung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66fa7ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Erstelle zus√§tzliche Zeit-Features...\n",
      "üîÑ Erstelle Wochentag-Bin√§rkodierung (optional)...\n",
      "‚úÖ Wochentag-Features erstellt: 8 Features\n",
      "\n",
      "üìä Wochentag-Verteilung:\n",
      "Wochentag\n",
      "Tuesday      1345\n",
      "Wednesday    1342\n",
      "Sunday       1342\n",
      "Saturday     1336\n",
      "Thursday     1334\n",
      "Monday       1324\n",
      "Friday       1311\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìù Hinweis: Sonntag-Feature wurde entfernt. Verwende bei Bedarf 'Wochentag_Sunday' aus der Wochentag-Bin√§rkodierung.\n"
     ]
    }
   ],
   "source": [
    "# 7. ZUS√ÑTZLICHE ZEIT-FEATURES (ohne Sonntag)\n",
    "print(\"üìÖ Erstelle zus√§tzliche Zeit-Features...\")\n",
    "\n",
    "# Wochentag-Bin√§rkodierung (falls f√ºr spezifische Analysen ben√∂tigt)\n",
    "wochentage = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "print(\"üîÑ Erstelle Wochentag-Bin√§rkodierung (optional)...\")\n",
    "\n",
    "for wochentag in wochentage:\n",
    "    df_combined[f'Wochentag_{wochentag}'] = (df_combined['Wochentag'] == wochentag).astype(int)\n",
    "\n",
    "wochentag_cols = [col for col in df_combined.columns if col.startswith('Wochentag_')]\n",
    "print(f\"‚úÖ Wochentag-Features erstellt: {len(wochentag_cols)} Features\")\n",
    "\n",
    "# √úberpr√ºfe die Wochentag-Verteilung\n",
    "print(f\"\\nüìä Wochentag-Verteilung:\")\n",
    "wochentag_verteilung = df_combined['Wochentag'].value_counts()\n",
    "print(wochentag_verteilung)\n",
    "\n",
    "print(f\"\\nüìù Hinweis: Sonntag-Feature wurde entfernt. Verwende bei Bedarf 'Wochentag_Sunday' aus der Wochentag-Bin√§rkodierung.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33a1aba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚õµ Lade Kieler Woche-Daten und erstelle bin√§res Feature ...\n",
      "Shape der Kiwo-Daten: (72, 2)\n",
      "Spalten: ['Datum', 'KielerWoche']\n",
      "        Datum  KielerWoche\n",
      "0  2012-06-16            1\n",
      "1  2012-06-17            1\n",
      "2  2012-06-18            1\n",
      "3  2012-06-19            1\n",
      "4  2012-06-20            1\n",
      "‚úÖ Kieler Woche-Feature erg√§nzt: 223 Tage im Datensatz sind w√§hrend der Kieler Woche.\n"
     ]
    }
   ],
   "source": [
    "# 8. KIELER WOCHE: Daten laden, mergen und bin√§r kodieren\n",
    "print(\"‚õµ Lade Kieler Woche-Daten und erstelle bin√§res Feature ...\")\n",
    "\n",
    "# Kiwo-Daten laden\n",
    "kiwo_url = \"https://raw.githubusercontent.com/opencampus-sh/einfuehrung-in-data-science-und-ml/main/kiwo.csv\"\n",
    "df_kiwo = pd.read_csv(kiwo_url)\n",
    "print(f\"Shape der Kiwo-Daten: {df_kiwo.shape}\")\n",
    "print(f\"Spalten: {list(df_kiwo.columns)}\")\n",
    "print(df_kiwo.head())\n",
    "\n",
    "# Datumsspalte in datetime konvertieren (Spalte hei√üt meist 'Datum' oder √§hnlich)\n",
    "if 'Datum' in df_kiwo.columns:\n",
    "    df_kiwo['Datum'] = pd.to_datetime(df_kiwo['Datum'])\n",
    "else:\n",
    "    # Fallback: erste Spalte als Datum\n",
    "    df_kiwo[df_kiwo.columns[0]] = pd.to_datetime(df_kiwo[df_kiwo.columns[0]])\n",
    "    df_kiwo = df_kiwo.rename(columns={df_kiwo.columns[0]: 'Datum'})\n",
    "\n",
    "# Bin√§res Feature 'ist_kiwo' setzen: 1 f√ºr Kiwo-Tage, sonst 0\n",
    "kiwo_tage = set(df_kiwo['Datum'].dt.date)\n",
    "df_combined['ist_kiwo'] = df_combined['Datum'].dt.date.isin(kiwo_tage).astype(int)\n",
    "\n",
    "print(f\"‚úÖ Kieler Woche-Feature erg√§nzt: {df_combined['ist_kiwo'].sum()} Tage im Datensatz sind w√§hrend der Kieler Woche.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b51b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Finalisiere den Trainingsdatensatz...\n",
      "\n",
      "üîç √úberpr√ºfe fehlende Werte:\n",
      "Wettercode          2325\n",
      "Wettercode_fehlt      16\n",
      "dtype: int64\n",
      "Wettercode          2325\n",
      "Wettercode_fehlt      16\n",
      "dtype: int64\n",
      "‚úÖ Wettercode: 0 fehlende Werte mit Median 28.00 gef√ºllt\n",
      "‚úÖ Wettercode_fehlt: 0 fehlende Werte mit Median 0.00 gef√ºllt\n",
      "üìä Verwendete Umsatz-Spalte: 'Umsatz'\n",
      "üîß Konvertiere Umsatz-Spalte zu numerischen Werten...\n",
      "   Aktueller Datentyp: float64\n",
      "   ‚úÖ Bereits numerisch, zu float konvertiert\n",
      "\n",
      "üìä FINALER TRAININGSDATENSATZ:\n",
      "üìè Shape: (9334, 34)\n",
      "üìÖ Zeitraum: 2013-07-01 00:00:00 bis 2018-07-31 00:00:00\n",
      "üí∞ Umsatzbereich: 7.05 ‚Ç¨ - 1879.46 ‚Ç¨\n",
      "\n",
      "üè∑Ô∏è FEATURES (34 Spalten):\n",
      " 1. Datum\n",
      " 2. Jahr\n",
      " 3. Monat\n",
      " 4. Tag\n",
      " 5. Wochentag\n",
      " 6. Wochentag_Nr\n",
      " 7. Warengruppe\n",
      " 8. Warengruppe_Name\n",
      " 9. Temperatur\n",
      "10. Windgeschwindigkeit\n",
      "11. Bewoelkung\n",
      "12. Wettercode_fehlt\n",
      "13. ist_feiertag\n",
      "14. Jahreszeit\n",
      "15. ist_kiwo\n",
      "16. Umsatz\n",
      "17. Warengruppe_Brot\n",
      "18. Warengruppe_Br√∂tchen\n",
      "19. Warengruppe_Croissant\n",
      "20. Warengruppe_Konditorei\n",
      "21. Warengruppe_Kuchen\n",
      "22. Warengruppe_Saisonbrot\n",
      "23. Jahreszeit_Winter\n",
      "24. Jahreszeit_Fr√ºhling\n",
      "25. Jahreszeit_Sommer\n",
      "26. Jahreszeit_Herbst\n",
      "27. Wochentag_Nr\n",
      "28. Wochentag_Monday\n",
      "29. Wochentag_Tuesday\n",
      "30. Wochentag_Wednesday\n",
      "31. Wochentag_Thursday\n",
      "32. Wochentag_Friday\n",
      "33. Wochentag_Saturday\n",
      "34. Wochentag_Sunday\n",
      "\n",
      "üîç BIN√ÑRKODIERUNG-√úBERPR√úFUNG IM FINALEN DATASET:\n",
      "Gefundene bin√§re Spalten: 20\n",
      "Wochentag_Nr: Fehler bei Analyse - 'DataFrame' object has no attribute 'unique'\n",
      "Warengruppe_Name: ['Brot', 'Br√∂tchen', 'Croissant', 'Konditorei', 'Kuchen', 'Saisonbrot'] (Typ: object)\n",
      "Warengruppe_Brot: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Br√∂tchen: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Croissant: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Konditorei: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Kuchen: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Saisonbrot: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Winter: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Fr√ºhling: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Sommer: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Herbst: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Nr: Fehler bei Analyse - 'DataFrame' object has no attribute 'unique'\n",
      "Wochentag_Monday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Tuesday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Wednesday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Thursday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Friday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Saturday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Sunday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "‚úÖ Wettercode: 0 fehlende Werte mit Median 28.00 gef√ºllt\n",
      "‚úÖ Wettercode_fehlt: 0 fehlende Werte mit Median 0.00 gef√ºllt\n",
      "üìä Verwendete Umsatz-Spalte: 'Umsatz'\n",
      "üîß Konvertiere Umsatz-Spalte zu numerischen Werten...\n",
      "   Aktueller Datentyp: float64\n",
      "   ‚úÖ Bereits numerisch, zu float konvertiert\n",
      "\n",
      "üìä FINALER TRAININGSDATENSATZ:\n",
      "üìè Shape: (9334, 34)\n",
      "üìÖ Zeitraum: 2013-07-01 00:00:00 bis 2018-07-31 00:00:00\n",
      "üí∞ Umsatzbereich: 7.05 ‚Ç¨ - 1879.46 ‚Ç¨\n",
      "\n",
      "üè∑Ô∏è FEATURES (34 Spalten):\n",
      " 1. Datum\n",
      " 2. Jahr\n",
      " 3. Monat\n",
      " 4. Tag\n",
      " 5. Wochentag\n",
      " 6. Wochentag_Nr\n",
      " 7. Warengruppe\n",
      " 8. Warengruppe_Name\n",
      " 9. Temperatur\n",
      "10. Windgeschwindigkeit\n",
      "11. Bewoelkung\n",
      "12. Wettercode_fehlt\n",
      "13. ist_feiertag\n",
      "14. Jahreszeit\n",
      "15. ist_kiwo\n",
      "16. Umsatz\n",
      "17. Warengruppe_Brot\n",
      "18. Warengruppe_Br√∂tchen\n",
      "19. Warengruppe_Croissant\n",
      "20. Warengruppe_Konditorei\n",
      "21. Warengruppe_Kuchen\n",
      "22. Warengruppe_Saisonbrot\n",
      "23. Jahreszeit_Winter\n",
      "24. Jahreszeit_Fr√ºhling\n",
      "25. Jahreszeit_Sommer\n",
      "26. Jahreszeit_Herbst\n",
      "27. Wochentag_Nr\n",
      "28. Wochentag_Monday\n",
      "29. Wochentag_Tuesday\n",
      "30. Wochentag_Wednesday\n",
      "31. Wochentag_Thursday\n",
      "32. Wochentag_Friday\n",
      "33. Wochentag_Saturday\n",
      "34. Wochentag_Sunday\n",
      "\n",
      "üîç BIN√ÑRKODIERUNG-√úBERPR√úFUNG IM FINALEN DATASET:\n",
      "Gefundene bin√§re Spalten: 20\n",
      "Wochentag_Nr: Fehler bei Analyse - 'DataFrame' object has no attribute 'unique'\n",
      "Warengruppe_Name: ['Brot', 'Br√∂tchen', 'Croissant', 'Konditorei', 'Kuchen', 'Saisonbrot'] (Typ: object)\n",
      "Warengruppe_Brot: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Br√∂tchen: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Croissant: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Konditorei: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Kuchen: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Saisonbrot: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Winter: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Fr√ºhling: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Sommer: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Herbst: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Nr: Fehler bei Analyse - 'DataFrame' object has no attribute 'unique'\n",
      "Wochentag_Monday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Tuesday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Wednesday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Thursday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Friday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Saturday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Sunday: [np.int64(0), np.int64(1)] (Typ: int64)\n"
     ]
    }
   ],
   "source": [
    "# 10. FINALISIERUNG DES DATASETS\n",
    "print(\"üîß Finalisiere den Trainingsdatensatz...\")\n",
    "\n",
    "# √úberpr√ºfe fehlende Werte\n",
    "print(\"\\nüîç √úberpr√ºfe fehlende Werte:\")\n",
    "missing_values = df_combined.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "if len(missing_values) > 0:\n",
    "    print(missing_values)\n",
    "    \n",
    "    # F√ºlle fehlende numerische Werte mit Median\n",
    "    numeric_cols = df_combined.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_combined[col].isnull().sum() > 0:\n",
    "            median_val = df_combined[col].median()\n",
    "            df_combined[col] = df_combined[col].fillna(median_val)\n",
    "            print(f\"‚úÖ {col}: {df_combined[col].isnull().sum()} fehlende Werte mit Median {median_val:.2f} gef√ºllt\")\n",
    "else:\n",
    "    print(\"‚úÖ Keine fehlenden Werte gefunden!\")\n",
    "\n",
    "# FIX: Korrekte Identifizierung der Umsatz-Spalte\n",
    "# Verwende die bereits identifizierte Umsatz-Spalte aus den vorherigen Zellen\n",
    "if 'umsatz_spalte' in locals() and umsatz_spalte in df_combined.columns:\n",
    "    umsatz_col = umsatz_spalte\n",
    "else:\n",
    "    # Fallback: Suche nach der Umsatz-Spalte\n",
    "    numeric_cols = df_combined.select_dtypes(include=[np.number]).columns\n",
    "    umsatz_col = None\n",
    "    for col in ['umsatz', 'Umsatz']:\n",
    "        if col in df_combined.columns:\n",
    "            umsatz_col = col\n",
    "            break\n",
    "    \n",
    "    if umsatz_col is None and len(numeric_cols) > 1:\n",
    "        # Nimm die zweite numerische Spalte (erste ist meist ID oder Index)\n",
    "        umsatz_col = numeric_cols[1]\n",
    "    elif umsatz_col is None and len(numeric_cols) >= 1:\n",
    "        umsatz_col = numeric_cols[0]\n",
    "    else:\n",
    "        print(\"‚ùå Keine Umsatz-Spalte gefunden!\")\n",
    "        raise ValueError(\"Keine Umsatz-Spalte identifiziert!\")\n",
    "\n",
    "print(f\"üìä Verwendete Umsatz-Spalte: '{umsatz_col}'\")\n",
    "\n",
    "feature_columns = [\n",
    "    'Datum', 'Jahr', 'Monat', 'Tag', 'Wochentag', 'Wochentag_Nr',\n",
    "    'Warengruppe', 'Warengruppe_Name',\n",
    "    'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung', 'Wettercode_fehlt',\n",
    "    'ist_feiertag', 'Jahreszeit', 'ist_kiwo', 'Preisindex',\n",
    "    umsatz_col\n",
    "]\n",
    "\n",
    "# FIX: Korrekte Filterung der bin√§r kodierten Spalten\n",
    "warengruppen_cols = [col for col in df_combined.columns if col.startswith('Warengruppe_') and col != 'Warengruppe_Name']\n",
    "# FIX: Korrigiere den Spaltenname f√ºr Jahreszeiten-Features\n",
    "jahreszeiten_cols = [col for col in df_combined.columns if col.startswith('Jahreszeit_')]\n",
    "wochentag_cols = [col for col in df_combined.columns if col.startswith('Wochentag_')]\n",
    "\n",
    "all_features = feature_columns + warengruppen_cols + jahreszeiten_cols + wochentag_cols\n",
    "\n",
    "# Finale Spalten filtern (nur die die existieren)\n",
    "final_columns = [col for col in all_features if col in df_combined.columns]\n",
    "df_final = df_combined[final_columns].copy()\n",
    "\n",
    "# FIX: Umsatz-Spalte umbenennen und korrekt konvertieren\n",
    "df_final = df_final.rename(columns={umsatz_col: 'Umsatz'})\n",
    "\n",
    "# FIX: Robustere Konvertierung der Umsatz-Spalte zu numerischen Werten\n",
    "print(f\"üîß Konvertiere Umsatz-Spalte zu numerischen Werten...\")\n",
    "try:\n",
    "    # Pr√ºfe zun√§chst den aktuellen Datentyp\n",
    "    current_dtype = df_final['Umsatz'].dtype\n",
    "    print(f\"   Aktueller Datentyp: {current_dtype}\")\n",
    "    \n",
    "    if current_dtype == 'object' or 'datetime' in str(current_dtype).lower():\n",
    "        # Falls es sich um Object oder Datetime handelt, konvertiere zu numerisch\n",
    "        df_final['Umsatz'] = pd.to_numeric(df_final['Umsatz'], errors='coerce')\n",
    "        print(f\"   ‚úÖ Erfolgreich zu numerisch konvertiert\")\n",
    "    elif current_dtype in ['int64', 'float64']:\n",
    "        # Bereits numerisch, explizit zu float konvertieren\n",
    "        df_final['Umsatz'] = df_final['Umsatz'].astype(float)\n",
    "        print(f\"   ‚úÖ Bereits numerisch, zu float konvertiert\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Unbekannter Datentyp: {current_dtype}\")\n",
    "        # Versuche manuelle Konvertierung\n",
    "        df_final['Umsatz'] = pd.to_numeric(df_final['Umsatz'], errors='coerce')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Fehler beim Konvertieren der Umsatz-Spalte: {e}\")\n",
    "    print(\"üîÑ Versuche alternative Konvertierung...\")\n",
    "    \n",
    "    # Alternative: Element-weise Konvertierung f√ºr nicht-numerische Werte\n",
    "    try:\n",
    "        def convert_to_float(value):\n",
    "            if pd.isna(value):\n",
    "                return np.nan\n",
    "            try:\n",
    "                return float(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return np.nan\n",
    "        \n",
    "        df_final['Umsatz'] = df_final['Umsatz'].apply(convert_to_float)\n",
    "        print(\"‚úÖ Alternative Konvertierung erfolgreich\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Auch alternative Konvertierung fehlgeschlagen: {e2}\")\n",
    "        # Als letzter Ausweg: Entferne die problematische Spalte und verwende eine andere\n",
    "        problem_col = df_final['Umsatz']\n",
    "        print(f\"Problematische Spalte Typ: {type(problem_col.iloc[0])}\")\n",
    "        print(f\"Erste 5 Werte: {problem_col.head().tolist()}\")\n",
    "\n",
    "print(f\"\\nüìä FINALER TRAININGSDATENSATZ:\")\n",
    "print(f\"üìè Shape: {df_final.shape}\")\n",
    "print(f\"üìÖ Zeitraum: {df_final['Datum'].min()} bis {df_final['Datum'].max()}\")\n",
    "\n",
    "# FIX: Sicherere Berechnung der Min/Max-Werte\n",
    "try:\n",
    "    umsatz_values = df_final['Umsatz'].dropna()  # Entferne NaN-Werte f√ºr Berechnung\n",
    "    if len(umsatz_values) > 0:\n",
    "        umsatz_min = float(umsatz_values.min())\n",
    "        umsatz_max = float(umsatz_values.max())\n",
    "        print(f\"üí∞ Umsatzbereich: {umsatz_min:.2f} ‚Ç¨ - {umsatz_max:.2f} ‚Ç¨\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Keine g√ºltigen Umsatzwerte gefunden\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Fehler bei Umsatz-Statistiken: {e}\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è FEATURES ({len(df_final.columns)} Spalten):\")\n",
    "for i, col in enumerate(df_final.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# √úberpr√ºfe Bin√§rkodierung im finalen Dataset\n",
    "print(f\"\\nüîç BIN√ÑRKODIERUNG-√úBERPR√úFUNG IM FINALEN DATASET:\")\n",
    "binary_cols = [col for col in df_final.columns if col.startswith(('Warengruppe_', 'Jahreszeit_', 'Wochentag_'))]\n",
    "print(f\"Gefundene bin√§re Spalten: {len(binary_cols)}\")\n",
    "for col in binary_cols:\n",
    "    try:\n",
    "        unique_values = sorted(df_final[col].unique())\n",
    "        print(f\"{col}: {unique_values} (Typ: {df_final[col].dtype})\")\n",
    "    except Exception as e:\n",
    "        print(f\"{col}: Fehler bei Analyse - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33107b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Normalisiere numerische Features f√ºr Modelltraining...\n",
      "üìä Zu normalisierende Features: ['Jahr', 'Monat', 'Tag', 'Wochentag_Nr', 'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung']\n",
      "\n",
      "üìà ORIGINALE FEATURE-STATISTIKEN:\n",
      "üìä Zu normalisierende Features: ['Jahr', 'Monat', 'Tag', 'Wochentag_Nr', 'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung']\n",
      "\n",
      "üìà ORIGINALE FEATURE-STATISTIKEN:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Jahr   Monat     Tag  Wochentag_Nr  Wochentag_Nr  Temperatur  \\\n",
      "count 9334.00 9334.00 9334.00       9334.00       9334.00     9334.00   \n",
      "mean  2015.52    6.65   15.71          3.00          3.00       12.03   \n",
      "std      1.52    3.46    8.75          2.00          2.00        7.23   \n",
      "min   2013.00    1.00    1.00          0.00          0.00       -8.48   \n",
      "25%   2014.00    4.00    8.00          1.00          1.00        6.25   \n",
      "50%   2016.00    7.00   16.00          3.00          3.00       11.66   \n",
      "75%   2017.00   10.00   23.00          5.00          5.00       17.96   \n",
      "max   2018.00   12.00   31.00          6.00          6.00       31.44   \n",
      "\n",
      "       Windgeschwindigkeit  Bewoelkung  \n",
      "count              9334.00     9334.00  \n",
      "mean                 10.97        4.73  \n",
      "std                   4.13        2.64  \n",
      "min                   3.00        0.00  \n",
      "25%                   8.00        3.00  \n",
      "50%                  10.00        6.00  \n",
      "75%                  13.00        7.00  \n",
      "max                  35.00        8.00  \n",
      "\n",
      "üîÑ Methode 1: Standard Scaler (Z-Score)\n",
      "‚úÖ Standard Scaler erfolgreich angewendet\n",
      "Statistiken der Z-standardisierten Features:\n",
      "       std_Jahr  std_Monat  std_Tag  std_Wochentag_Nr  std_Temperatur  \\\n",
      "count   9334.00    9334.00  9334.00           9334.00         9334.00   \n",
      "mean       0.00       0.00     0.00              0.00            0.00   \n",
      "std        1.00       1.00     1.00              1.00            1.00   \n",
      "min       -1.66      -1.63    -1.68             -1.50           -1.50   \n",
      "25%       -1.00      -0.77    -0.88             -1.00           -1.00   \n",
      "50%        0.31       0.10     0.03             -0.00           -0.00   \n",
      "75%        0.97       0.97     0.83              1.00            1.00   \n",
      "max        1.63       1.54     1.75              1.50            1.50   \n",
      "\n",
      "       std_Windgeschwindigkeit  std_Bewoelkung  \n",
      "count                  9334.00         9334.00  \n",
      "mean                     -0.00            0.00  \n",
      "std                       1.00            1.00  \n",
      "min                      -2.84           -1.93  \n",
      "25%                      -0.80           -0.72  \n",
      "50%                      -0.05           -0.24  \n",
      "75%                       0.82            0.49  \n",
      "max                       2.69            5.82  \n",
      "\n",
      "üîÑ Methode 2: Min-Max Scaler (0-1)\n",
      "‚úÖ Min-Max Scaler erfolgreich angewendet\n",
      "üîÑ Methode 3: Robust Scaler (Median-basiert)\n",
      "‚úÖ Robust Scaler erfolgreich angewendet\n",
      "üîÑ Methode 4: Power Transformer (Yeo-Johnson)\n",
      "‚úÖ Power Transformer erfolgreich angewendet\n",
      "‚úÖ Normalisierung abgeschlossen!\n",
      "\n",
      "üìä VERGLEICH DER NORMALISIERUNGSCMETHODEN:\n",
      "================================================================================\n",
      "\n",
      "STD-SCALER:\n",
      "Mean Bereich: 0.000 bis 0.000\n",
      "Std Bereich: 1.000 bis 1.000\n",
      "Min Bereich: -2.837 bis -1.500\n",
      "Max Bereich: 1.499 bis 5.820\n",
      "\n",
      "MINMAX-SCALER:\n",
      "Mean Bereich: 0.249 bis 0.514\n",
      "Std Bereich: 0.129 bis 0.334\n",
      "Min Bereich: 0.000 bis 0.000\n",
      "Max Bereich: 1.000 bis 1.000\n",
      "\n",
      "ROBUST-SCALER:\n",
      "Mean Bereich: -0.158 bis 0.195\n",
      "Std Bereich: 0.500 bis 0.826\n",
      "Min Bereich: -1.719 bis -0.750\n",
      "Max Bereich: 0.667 bis 5.000\n",
      "\n",
      "POWER-SCALER:\n",
      "Mean Bereich: -0.000 bis -0.000\n",
      "Std Bereich: 1.000 bis 1.000\n",
      "Min Bereich: -3.313 bis -1.621\n",
      "Max Bereich: 1.403 bis 3.516\n",
      "‚úÖ Power Transformer erfolgreich angewendet\n",
      "‚úÖ Normalisierung abgeschlossen!\n",
      "\n",
      "üìä VERGLEICH DER NORMALISIERUNGSCMETHODEN:\n",
      "================================================================================\n",
      "\n",
      "STD-SCALER:\n",
      "Mean Bereich: 0.000 bis 0.000\n",
      "Std Bereich: 1.000 bis 1.000\n",
      "Min Bereich: -2.837 bis -1.500\n",
      "Max Bereich: 1.499 bis 5.820\n",
      "\n",
      "MINMAX-SCALER:\n",
      "Mean Bereich: 0.249 bis 0.514\n",
      "Std Bereich: 0.129 bis 0.334\n",
      "Min Bereich: 0.000 bis 0.000\n",
      "Max Bereich: 1.000 bis 1.000\n",
      "\n",
      "ROBUST-SCALER:\n",
      "Mean Bereich: -0.158 bis 0.195\n",
      "Std Bereich: 0.500 bis 0.826\n",
      "Min Bereich: -1.719 bis -0.750\n",
      "Max Bereich: 0.667 bis 5.000\n",
      "\n",
      "POWER-SCALER:\n",
      "Mean Bereich: -0.000 bis -0.000\n",
      "Std Bereich: 1.000 bis 1.000\n",
      "Min Bereich: -3.313 bis -1.621\n",
      "Max Bereich: 1.403 bis 3.516\n"
     ]
    }
   ],
   "source": [
    "# NORMALISIERUNG DER NUMERISCHEN FEATURES\n",
    "print(\"üîß Normalisiere numerische Features f√ºr Modelltraining...\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Kopie des finalen Datasets f√ºr Normalisierung erstellen\n",
    "df_normalized = df_final.copy()\n",
    "\n",
    "# Identifiziere numerische Features (ohne Zielvariable und bin√§re Features)\n",
    "numeric_features_to_scale = [\n",
    "    'Jahr', 'Monat', 'Tag', 'Wochentag_Nr',\n",
    "    'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung', \n",
    "    'Preisindex'\n",
    "]\n",
    "\n",
    "# Nur Features behalten, die auch im Dataset existieren\n",
    "numeric_features_to_scale = [col for col in numeric_features_to_scale if col in df_normalized.columns]\n",
    "\n",
    "print(f\"üìä Zu normalisierende Features: {numeric_features_to_scale}\")\n",
    "\n",
    "# Originale Features vor Normalisierung anzeigen\n",
    "print(f\"\\nüìà ORIGINALE FEATURE-STATISTIKEN:\")\n",
    "print(df_normalized[numeric_features_to_scale].describe().round(2))\n",
    "\n",
    "# 1. STANDARD SCALER (Z-Score Normalisierung)\n",
    "print(f\"\\nüîÑ Methode 1: Standard Scaler (Z-Score)\")\n",
    "try:\n",
    "    scaler_standard = StandardScaler()\n",
    "    standard_scaled = scaler_standard.fit_transform(df_normalized[numeric_features_to_scale])\n",
    "    \n",
    "    # Sichere Zuweisung der normalisierten Werte - jede Spalte einzeln\n",
    "    for i, col in enumerate(numeric_features_to_scale):\n",
    "        df_normalized[f'std_{col}'] = standard_scaled[:, i]\n",
    "    \n",
    "    print(\"‚úÖ Standard Scaler erfolgreich angewendet\")\n",
    "    print(\"Statistiken der Z-standardisierten Features:\")\n",
    "    print(df_normalized[[f'std_{col}' for col in numeric_features_to_scale]].describe().round(2))\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Standard Scaler Fehler: {str(e)[:100]}...\")\n",
    "\n",
    "# 2. MIN-MAX SCALER (0-1 Normalisierung)\n",
    "print(f\"\\nüîÑ Methode 2: Min-Max Scaler (0-1)\")\n",
    "try:\n",
    "    scaler_minmax = MinMaxScaler()\n",
    "    minmax_scaled = scaler_minmax.fit_transform(df_normalized[numeric_features_to_scale])\n",
    "    \n",
    "    # Sichere Zuweisung der normalisierten Werte - jede Spalte einzeln\n",
    "    for i, col in enumerate(numeric_features_to_scale):\n",
    "        df_normalized[f'minmax_{col}'] = minmax_scaled[:, i]\n",
    "    \n",
    "    print(\"‚úÖ Min-Max Scaler erfolgreich angewendet\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Min-Max Scaler Fehler: {str(e)[:100]}...\")\n",
    "\n",
    "# 3. ROBUST SCALER (Median-basiert, weniger empfindlich gegen Ausrei√üer)\n",
    "print(f\"üîÑ Methode 3: Robust Scaler (Median-basiert)\")\n",
    "try:\n",
    "    scaler_robust = RobustScaler()\n",
    "    robust_scaled = scaler_robust.fit_transform(df_normalized[numeric_features_to_scale])\n",
    "    \n",
    "    # Sichere Zuweisung der normalisierten Werte - jede Spalte einzeln\n",
    "    for i, col in enumerate(numeric_features_to_scale):\n",
    "        df_normalized[f'robust_{col}'] = robust_scaled[:, i]\n",
    "    \n",
    "    print(\"‚úÖ Robust Scaler erfolgreich angewendet\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Robust Scaler Fehler: {str(e)[:100]}...\")\n",
    "\n",
    "# 4. POWER TRANSFORMER (Yeo-Johnson f√ºr Normalverteilung)\n",
    "print(f\"üîÑ Methode 4: Power Transformer (Yeo-Johnson)\")\n",
    "try:\n",
    "    power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    power_scaled = power_transformer.fit_transform(df_normalized[numeric_features_to_scale])\n",
    "    \n",
    "    # Sichere Zuweisung der normalisierten Werte - jede Spalte einzeln\n",
    "    for i, col in enumerate(numeric_features_to_scale):\n",
    "        df_normalized[f'power_{col}'] = power_scaled[:, i]\n",
    "    \n",
    "    print(\"‚úÖ Power Transformer erfolgreich angewendet\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Power Transformer √ºbersprungen (Fehler: {str(e)[:50]}...)\")\n",
    "\n",
    "print(f\"‚úÖ Normalisierung abgeschlossen!\")\n",
    "\n",
    "# Vergleiche die Normalisierungsmethoden\n",
    "scaling_methods = ['std', 'minmax', 'robust', 'power']\n",
    "print(f\"\\nüìä VERGLEICH DER NORMALISIERUNGSCMETHODEN:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for method in scaling_methods:\n",
    "    method_cols = [col for col in df_normalized.columns if col.startswith(f'{method}_')]\n",
    "    if method_cols:\n",
    "        print(f\"\\n{method.upper()}-SCALER:\")\n",
    "        try:\n",
    "            stats = df_normalized[method_cols].describe().round(3)\n",
    "            print(f\"Mean Bereich: {stats.loc['mean'].min():.3f} bis {stats.loc['mean'].max():.3f}\")\n",
    "            print(f\"Std Bereich: {stats.loc['std'].min():.3f} bis {stats.loc['std'].max():.3f}\")\n",
    "            print(f\"Min Bereich: {stats.loc['min'].min():.3f} bis {stats.loc['min'].max():.3f}\")\n",
    "            print(f\"Max Bereich: {stats.loc['max'].min():.3f} bis {stats.loc['max'].max():.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei Statistiken: {str(e)[:50]}...\")\n",
    "    else:\n",
    "        print(f\"\\n{method.upper()}-SCALER: Keine Spalten gefunden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b690bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ EMPFEHLUNG F√úR NORMALISIERUNGSMETHODE:\n",
      "============================================================\n",
      "üìã Bewertungskriterien f√ºr Neuronale Netze:\n",
      "1. √Ñhnliche Skalierung aller Features (Mean ‚âà 0, Std ‚âà 1)\n",
      "2. Keine extremen Ausrei√üer\n",
      "3. Stabile Gradientenberechnung\n",
      "4. Erhaltung der Datenverteilung\n",
      "\n",
      "üìä BEWERTUNG DER NORMALISIERUNGSMETHODEN:\n",
      "STD-Scaler:\n",
      "  Gesamtscore: 0.701\n",
      "  Mean-Konsistenz: 1.000\n",
      "  Std-Konsistenz: 1.000\n",
      "  Wertebereich: 0.104\n",
      "MINMAX-Scaler:\n",
      "  Gesamtscore: 0.779\n",
      "  Mean-Konsistenz: 0.912\n",
      "  Std-Konsistenz: 0.925\n",
      "  Wertebereich: 0.500\n",
      "ROBUST-Scaler:\n",
      "  Gesamtscore: 0.643\n",
      "  Mean-Konsistenz: 0.904\n",
      "  Std-Konsistenz: 0.897\n",
      "  Wertebereich: 0.130\n",
      "\n",
      "‚úÖ EMPFEHLUNG: MINMAX-SCALER\n",
      "üèÜ Beste Gesamtbewertung: 0.779\n",
      "\n",
      "üìä MODELL-BEREITES DATASET:\n",
      "üìè Shape: (9334, 37)\n",
      "üè∑Ô∏è Features: 37\n",
      "\n",
      "üîß NORMALISIERTE FEATURES:\n",
      "Jahr_normalized: Mean=0.505, Std=0.304, Range=[0.000, 1.000]\n",
      "Monat_normalized: Mean=0.514, Std=0.315, Range=[0.000, 1.000]\n",
      "Tag_normalized: Mean=0.490, Std=0.292, Range=[0.000, 1.000]\n",
      "Wochentag_Nr_normalized: Mean=0.500, Std=0.334, Range=[0.000, 1.000]\n",
      "Temperatur_normalized: Mean=0.500, Std=0.334, Range=[0.000, 1.000]\n",
      "Windgeschwindigkeit_normalized: Mean=0.514, Std=0.181, Range=[0.000, 1.000]\n",
      "Bewoelkung_normalized: Mean=0.249, Std=0.129, Range=[0.000, 1.000]\n",
      "\n",
      "üîç BIN√ÑRE FEATURES IM NORMALISIERTEN DATASET:\n",
      "Anzahl bin√§rer Features: 23\n",
      "  Warengruppe_Name: ['Brot', 'Br√∂tchen', 'Croissant', 'Konditorei', 'Kuchen', 'Saisonbrot']\n",
      "  Warengruppe_Brot: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_Br√∂tchen: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_Croissant: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_Konditorei: [np.int64(0), np.int64(1)]\n",
      "  ... und 18 weitere bin√§re Features\n",
      "\n",
      "üìä BEWERTUNG DER NORMALISIERUNGSMETHODEN:\n",
      "STD-Scaler:\n",
      "  Gesamtscore: 0.701\n",
      "  Mean-Konsistenz: 1.000\n",
      "  Std-Konsistenz: 1.000\n",
      "  Wertebereich: 0.104\n",
      "MINMAX-Scaler:\n",
      "  Gesamtscore: 0.779\n",
      "  Mean-Konsistenz: 0.912\n",
      "  Std-Konsistenz: 0.925\n",
      "  Wertebereich: 0.500\n",
      "ROBUST-Scaler:\n",
      "  Gesamtscore: 0.643\n",
      "  Mean-Konsistenz: 0.904\n",
      "  Std-Konsistenz: 0.897\n",
      "  Wertebereich: 0.130\n",
      "\n",
      "‚úÖ EMPFEHLUNG: MINMAX-SCALER\n",
      "üèÜ Beste Gesamtbewertung: 0.779\n",
      "\n",
      "üìä MODELL-BEREITES DATASET:\n",
      "üìè Shape: (9334, 37)\n",
      "üè∑Ô∏è Features: 37\n",
      "\n",
      "üîß NORMALISIERTE FEATURES:\n",
      "Jahr_normalized: Mean=0.505, Std=0.304, Range=[0.000, 1.000]\n",
      "Monat_normalized: Mean=0.514, Std=0.315, Range=[0.000, 1.000]\n",
      "Tag_normalized: Mean=0.490, Std=0.292, Range=[0.000, 1.000]\n",
      "Wochentag_Nr_normalized: Mean=0.500, Std=0.334, Range=[0.000, 1.000]\n",
      "Temperatur_normalized: Mean=0.500, Std=0.334, Range=[0.000, 1.000]\n",
      "Windgeschwindigkeit_normalized: Mean=0.514, Std=0.181, Range=[0.000, 1.000]\n",
      "Bewoelkung_normalized: Mean=0.249, Std=0.129, Range=[0.000, 1.000]\n",
      "\n",
      "üîç BIN√ÑRE FEATURES IM NORMALISIERTEN DATASET:\n",
      "Anzahl bin√§rer Features: 23\n",
      "  Warengruppe_Name: ['Brot', 'Br√∂tchen', 'Croissant', 'Konditorei', 'Kuchen', 'Saisonbrot']\n",
      "  Warengruppe_Brot: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_Br√∂tchen: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_Croissant: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_Konditorei: [np.int64(0), np.int64(1)]\n",
      "  ... und 18 weitere bin√§re Features\n",
      "\n",
      "‚úÖ Normalisiertes Dataset gespeichert: /workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset_normalized.csv\n",
      "‚úÖ Normalisierungs-Info gespeichert: /workspaces/bakery_sales_prediction/5_Datasets/normalization_info.json\n",
      "\n",
      "üéâ NORMALISIERUNG ABGESCHLOSSEN!\n",
      "üìÅ Original Dataset: bakery_training_dataset.csv\n",
      "üìÅ Normalisiertes Dataset: bakery_training_dataset_normalized.csv\n",
      "‚úÖ Sonntag-Feature entfernt\n",
      "‚úÖ Bin√§rkodierung mit expliziten 0/1-Werten\n",
      "üöÄ Bereit f√ºr Neuronale Netze und ML-Modelle!\n",
      "\n",
      "‚úÖ Normalisiertes Dataset gespeichert: /workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset_normalized.csv\n",
      "‚úÖ Normalisierungs-Info gespeichert: /workspaces/bakery_sales_prediction/5_Datasets/normalization_info.json\n",
      "\n",
      "üéâ NORMALISIERUNG ABGESCHLOSSEN!\n",
      "üìÅ Original Dataset: bakery_training_dataset.csv\n",
      "üìÅ Normalisiertes Dataset: bakery_training_dataset_normalized.csv\n",
      "‚úÖ Sonntag-Feature entfernt\n",
      "‚úÖ Bin√§rkodierung mit expliziten 0/1-Werten\n",
      "üöÄ Bereit f√ºr Neuronale Netze und ML-Modelle!\n"
     ]
    }
   ],
   "source": [
    "# EMPFEHLUNG UND FINALE DATASET-ERSTELLUNG\n",
    "print(\"üéØ EMPFEHLUNG F√úR NORMALISIERUNGSMETHODE:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Bewertungskriterien f√ºr neuronale netze\n",
    "print(\"üìã Bewertungskriterien f√ºr Neuronale Netze:\")\n",
    "print(\"1. √Ñhnliche Skalierung aller Features (Mean ‚âà 0, Std ‚âà 1)\")\n",
    "print(\"2. Keine extremen Ausrei√üer\")\n",
    "print(\"3. Stabile Gradientenberechnung\")\n",
    "print(\"4. Erhaltung der Datenverteilung\")\n",
    "\n",
    "# Analysiere Stabilit√§t der verschiedenen Scaler\n",
    "scaler_scores = {}\n",
    "\n",
    "for method in ['std', 'minmax', 'robust']:\n",
    "    method_cols = [col for col in df_normalized.columns if col.startswith(f'{method}_')]\n",
    "    if method_cols:\n",
    "        method_data = df_normalized[method_cols]\n",
    "        \n",
    "        # Bewertungskriterien berechnen\n",
    "        mean_consistency = 1 / (1 + abs(method_data.mean().std()))  # Je einheitlicher die Means, desto besser\n",
    "        std_consistency = 1 / (1 + abs(method_data.std().std()))    # Je einheitlicher die Stds, desto besser\n",
    "        range_control = 1 / (1 + (method_data.max().max() - method_data.min().min()))  # Kontrollierter Wertebereich\n",
    "        \n",
    "        total_score = (mean_consistency + std_consistency + range_control) / 3\n",
    "        scaler_scores[method] = {\n",
    "            'total': total_score,\n",
    "            'mean_consistency': mean_consistency,\n",
    "            'std_consistency': std_consistency,\n",
    "            'range_control': range_control\n",
    "        }\n",
    "\n",
    "# Beste Methode ermitteln\n",
    "best_method = max(scaler_scores.keys(), key=lambda k: scaler_scores[k]['total'])\n",
    "\n",
    "print(f\"\\nüìä BEWERTUNG DER NORMALISIERUNGSMETHODEN:\")\n",
    "for method, scores in scaler_scores.items():\n",
    "    print(f\"{method.upper()}-Scaler:\")\n",
    "    print(f\"  Gesamtscore: {scores['total']:.3f}\")\n",
    "    print(f\"  Mean-Konsistenz: {scores['mean_consistency']:.3f}\")\n",
    "    print(f\"  Std-Konsistenz: {scores['std_consistency']:.3f}\")\n",
    "    print(f\"  Wertebereich: {scores['range_control']:.3f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ EMPFEHLUNG: {best_method.upper()}-SCALER\")\n",
    "print(f\"üèÜ Beste Gesamtbewertung: {scaler_scores[best_method]['total']:.3f}\")\n",
    "\n",
    "# Finales Dataset mit der besten Normalisierungsmethode erstellen\n",
    "best_method_cols = [col for col in df_normalized.columns if col.startswith(f'{best_method}_')]\n",
    "\n",
    "# Features f√ºr finales normalisiertes Dataset ausw√§hlen (ohne Sonntag-Feature)\n",
    "final_normalized_features = [\n",
    "    'Datum', 'Umsatz',  # Basis-Features\n",
    "    'Warengruppe', 'Warengruppe_Name', 'Wochentag', 'Jahreszeit',  # Kategorische Features\n",
    "    'ist_feiertag', 'ist_kiwo', 'Wettercode_fehlt'  # Bin√§re Features (ohne ist_sonntag)\n",
    "]\n",
    "\n",
    "# Bin√§r kodierte Features hinzuf√ºgen\n",
    "warengruppen_binary = [col for col in df_normalized.columns if col.startswith('Warengruppe_') and col != 'Warengruppe_Name']\n",
    "jahreszeiten_binary = [col for col in df_normalized.columns if col.startswith('Jahreszeit_')]\n",
    "wochentag_binary = [col for col in df_normalized.columns if col.startswith('Wochentag_')]\n",
    "\n",
    "# Normalisierte numerische Features der besten Methode hinzuf√ºgen\n",
    "final_features = final_normalized_features + warengruppen_binary + jahreszeiten_binary + wochentag_binary + best_method_cols\n",
    "\n",
    "# Nur existierende Spalten ausw√§hlen\n",
    "final_features = [col for col in final_features if col in df_normalized.columns]\n",
    "\n",
    "# Finales normalisiertes Dataset erstellen\n",
    "df_model_ready = df_normalized[final_features].copy()\n",
    "\n",
    "# Normalisierte Spalten umbenennen (Pr√§fix entfernen f√ºr Klarheit)\n",
    "rename_dict = {}\n",
    "for col in best_method_cols:\n",
    "    original_name = col.replace(f'{best_method}_', '')\n",
    "    rename_dict[col] = f'{original_name}_normalized'\n",
    "\n",
    "df_model_ready = df_model_ready.rename(columns=rename_dict)\n",
    "\n",
    "print(f\"\\nüìä MODELL-BEREITES DATASET:\")\n",
    "print(f\"üìè Shape: {df_model_ready.shape}\")\n",
    "print(f\"üè∑Ô∏è Features: {len(df_model_ready.columns)}\")\n",
    "\n",
    "print(f\"\\nüîß NORMALISIERTE FEATURES:\")\n",
    "normalized_cols = [col for col in df_model_ready.columns if col.endswith('_normalized')]\n",
    "for col in normalized_cols:\n",
    "    stats = df_model_ready[col].describe()\n",
    "    print(f\"{col}: Mean={stats['mean']:.3f}, Std={stats['std']:.3f}, Range=[{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
    "\n",
    "# √úberpr√ºfe die bin√§ren Features im finalen normalisierten Dataset\n",
    "print(f\"\\nüîç BIN√ÑRE FEATURES IM NORMALISIERTEN DATASET:\")\n",
    "binary_cols = [col for col in df_model_ready.columns if col.startswith(('Warengruppe_', 'Jahreszeit_', 'Wochentag_'))]\n",
    "print(f\"Anzahl bin√§rer Features: {len(binary_cols)}\")\n",
    "for col in binary_cols[:5]:  # Zeige nur die ersten 5 als Beispiel\n",
    "    unique_vals = sorted(df_model_ready[col].unique())\n",
    "    print(f\"  {col}: {unique_vals}\")\n",
    "if len(binary_cols) > 5:\n",
    "    print(f\"  ... und {len(binary_cols) - 5} weitere bin√§re Features\")\n",
    "\n",
    "# Speichere das normalisierte Dataset\n",
    "normalized_output_path = \"/workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset_normalized.csv\"\n",
    "\n",
    "try:\n",
    "    df_model_ready.to_csv(normalized_output_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\n‚úÖ Normalisiertes Dataset gespeichert: {normalized_output_path}\")\n",
    "    \n",
    "    # Zus√§tzliche Info-Datei mit Normalisierungsparametern\n",
    "    normalization_info = {\n",
    "        'Methode': best_method.upper() + '-Scaler',\n",
    "        'Normalisierte_Features': [col.replace('_normalized', '') for col in normalized_cols],\n",
    "        'Original_Features': numeric_features_to_scale,\n",
    "        'Binaere_Features': binary_cols,\n",
    "        'Entfernte_Features': ['ist_sonntag'],  # Dokumentiere entfernte Features\n",
    "        'Dataset_Shape': df_model_ready.shape,\n",
    "        'Empfehlung': f\"Verwende {best_method.upper()}-Scaler f√ºr optimale NN-Performance\"\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    info_path = \"/workspaces/bakery_sales_prediction/5_Datasets/normalization_info.json\"\n",
    "    with open(info_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(normalization_info, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"‚úÖ Normalisierungs-Info gespeichert: {info_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Fehler beim Speichern: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ NORMALISIERUNG ABGESCHLOSSEN!\")\n",
    "print(f\"üìÅ Original Dataset: bakery_training_dataset.csv\")\n",
    "print(f\"üìÅ Normalisiertes Dataset: bakery_training_dataset_normalized.csv\")\n",
    "print(f\"‚úÖ Sonntag-Feature entfernt\")\n",
    "print(f\"‚úÖ Bin√§rkodierung mit expliziten 0/1-Werten\")\n",
    "print(f\"üöÄ Bereit f√ºr Neuronale Netze und ML-Modelle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e3b9717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Speichere finalen Trainingsdatensatz...\n",
      "‚úÖ Dataset erfolgreich gespeichert: /workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset.csv\n",
      "‚ùå Fehler beim Speichern: No module named 'openpyxl'\n",
      "\n",
      "üéâ TRAININGSDATENSATZ ERFOLGREICH ERSTELLT!\n",
      "üìÅ Hauptdatei: /workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset.csv\n",
      "üìã 9334 Zeilen mit 34 Features\n",
      "üéØ Bereit f√ºr Regression und Neuronale Netze!\n",
      "\n",
      "üìä FINALE ZUSAMMENFASSUNG:\n",
      "ü•ê Warengruppen: 6\n",
      "üìÖ Tage: 1819\n",
      "üå°Ô∏è Temperaturbereich: -8.5¬∞C - 31.4¬∞C\n",
      "üéÑ Feiertage: 201 Tage\n",
      "‚õµ Kieler Woche: 223 Tage\n",
      "üî¢ Bin√§re Features: 20 (alle mit 0/1-Kodierung)\n",
      "üí∞ Durchschnittsumsatz: 206.75 ‚Ç¨\n",
      "\n",
      "‚úÖ √ÑNDERUNGEN DURCHGEF√úHRT:\n",
      "   ‚Ä¢ Sonntag-Feature (ist_sonntag) entfernt\n",
      "   ‚Ä¢ Warengruppen-Features mit expliziter 0/1-Bin√§rkodierung\n",
      "   ‚Ä¢ Jahreszeiten-Features mit expliziter 0/1-Bin√§rkodierung\n",
      "   ‚Ä¢ Wochentag-Features als Alternative verf√ºgbar\n",
      "   ‚Ä¢ Kieler Woche und Preisindex Features hinzugef√ºgt\n"
     ]
    }
   ],
   "source": [
    "# DATASET SPEICHERN\n",
    "print(\"üíæ Speichere finalen Trainingsdatensatz...\")\n",
    "\n",
    "# Pfad f√ºr das finale Dataset\n",
    "output_path = \"/workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset.csv\"\n",
    "\n",
    "try:\n",
    "    # CSV speichern\n",
    "    df_final.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"‚úÖ Dataset erfolgreich gespeichert: {output_path}\")\n",
    "    \n",
    "    # Zus√§tzlich als Excel f√ºr bessere Lesbarkeit\n",
    "    excel_path = output_path.replace('.csv', '.xlsx')\n",
    "    df_final.to_excel(excel_path, index=False)\n",
    "    print(f\"‚úÖ Dataset auch als Excel gespeichert: {excel_path}\")\n",
    "    \n",
    "    # Feature-Liste separat speichern\n",
    "    feature_info = pd.DataFrame({\n",
    "        'Feature': df_final.columns,\n",
    "        'Typ': df_final.dtypes,\n",
    "        'Fehlende_Werte': df_final.isnull().sum(),\n",
    "        'Eindeutige_Werte': df_final.nunique()\n",
    "    })\n",
    "    \n",
    "    feature_path = \"/workspaces/bakery_sales_prediction/5_Datasets/feature_description.csv\"\n",
    "    feature_info.to_csv(feature_path, index=False)\n",
    "    print(f\"‚úÖ Feature-Beschreibung gespeichert: {feature_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Fehler beim Speichern: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ TRAININGSDATENSATZ ERFOLGREICH ERSTELLT!\")\n",
    "print(f\"üìÅ Hauptdatei: {output_path}\")\n",
    "print(f\"üìã {len(df_final)} Zeilen mit {len(df_final.columns)} Features\")\n",
    "print(f\"üéØ Bereit f√ºr Regression und Neuronale Netze!\")\n",
    "\n",
    "# Finale Zusammenfassung (ohne Sonntag-Feature) - FIX: Robuste Behandlung fehlender Features\n",
    "print(f\"\\nüìä FINALE ZUSAMMENFASSUNG:\")\n",
    "print(f\"ü•ê Warengruppen: {df_final['Warengruppe_Name'].nunique()}\")\n",
    "print(f\"üìÖ Tage: {df_final['Datum'].nunique()}\")\n",
    "print(f\"üå°Ô∏è Temperaturbereich: {df_final['Temperatur'].min():.1f}¬∞C - {df_final['Temperatur'].max():.1f}¬∞C\")\n",
    "\n",
    "# Sichere Behandlung von Feiertagen\n",
    "if 'ist_feiertag' in df_final.columns:\n",
    "    print(f\"üéÑ Feiertage: {df_final['ist_feiertag'].sum()} Tage\")\n",
    "else:\n",
    "    print(f\"üéÑ Feiertage: Feature nicht verf√ºgbar\")\n",
    "\n",
    "# Sichere Behandlung von Kieler Woche\n",
    "if 'ist_kiwo' in df_final.columns:\n",
    "    print(f\"‚õµ Kieler Woche: {df_final['ist_kiwo'].sum()} Tage\")\n",
    "else:\n",
    "    print(f\"‚õµ Kieler Woche: Feature nicht verf√ºgbar\")\n",
    "\n",
    "# √úberpr√ºfe die bin√§ren Features\n",
    "binary_features = [col for col in df_final.columns if col.startswith(('Warengruppe_', 'Jahreszeit_', 'Wochentag_'))]\n",
    "print(f\"üî¢ Bin√§re Features: {len(binary_features)} (alle mit 0/1-Kodierung)\")\n",
    "\n",
    "# Robustere Berechnung des Durchschnittsumsatzes\n",
    "try:\n",
    "    # Methode 1: Konvertiere zu NumPy-Array und berechne den Mittelwert\n",
    "    mean_umsatz = df_final['Umsatz'].to_numpy().mean()\n",
    "    print(f\"üí∞ Durchschnittsumsatz: {mean_umsatz:.2f} ‚Ç¨\")\n",
    "except Exception as e:\n",
    "    try:\n",
    "        # Methode 2: Verwende NumPy direkt\n",
    "        mean_umsatz = np.nanmean(df_final['Umsatz'].values)\n",
    "        print(f\"üí∞ Durchschnittsumsatz: {mean_umsatz:.2f} ‚Ç¨\")\n",
    "    except Exception as e:\n",
    "        # Fallback: Zeige den Wert ohne Formatierung\n",
    "        print(f\"üí∞ Durchschnittsumsatz: {df_final['Umsatz'].mean()} ‚Ç¨\")\n",
    "\n",
    "print(f\"\\n‚úÖ √ÑNDERUNGEN DURCHGEF√úHRT:\")\n",
    "print(f\"   ‚Ä¢ Sonntag-Feature (ist_sonntag) entfernt\")\n",
    "print(f\"   ‚Ä¢ Warengruppen-Features mit expliziter 0/1-Bin√§rkodierung\")\n",
    "print(f\"   ‚Ä¢ Jahreszeiten-Features mit expliziter 0/1-Bin√§rkodierung\")\n",
    "print(f\"   ‚Ä¢ Wochentag-Features als Alternative verf√ºgbar\")\n",
    "print(f\"   ‚Ä¢ Kieler Woche und Preisindex Features hinzugef√ºgt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
