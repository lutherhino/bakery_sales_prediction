{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ecace47",
   "metadata": {},
   "source": [
    "# ğŸ¥ BÃ¤ckerei Umsatz Vorhersage - Trainingsdatensatz Erstellung\n",
    "\n",
    "Dieses Notebook erstellt einen kombinierten Trainingsdatensatz fÃ¼r die Vorhersage von BÃ¤ckerei-UmsÃ¤tzen basierend auf:\n",
    "- Historischen Umsatzdaten\n",
    "- Wetterdaten  \n",
    "- Feiertagen\n",
    "- Besonderen Events (Kieler Woche)\n",
    "- Preisindizes fÃ¼r Backwaren\n",
    "\n",
    "**Ziel:** Ein sauberer DataFrame fÃ¼r Regressionsanalyse und neuronale Netze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f911bc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Lade erforderliche Bibliotheken...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Deutsche Locale nicht verfÃ¼gbar - verwende Standard\n",
      "ğŸ•’ Dataset-Erstellung gestartet: 01.07.2025 21:06:52\n",
      "âœ… Alle Bibliotheken erfolgreich geladen!\n"
     ]
    }
   ],
   "source": [
    "# 1. BIBLIOTHEKEN UND KONFIGURATION\n",
    "print(\"ğŸ“š Lade erforderliche Bibliotheken...\")\n",
    "\n",
    "# Basis-Bibliotheken fÃ¼r Datenmanipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualisierung\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# Konfiguration fÃ¼r bessere Darstellung\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# Pandas Anzeige-Optionen\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "# Seaborn Style\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Deutsche Locale fÃ¼r Datumsformatierung (falls verfÃ¼gbar)\n",
    "try:\n",
    "    import locale\n",
    "    locale.setlocale(locale.LC_TIME, 'de_DE.UTF-8')\n",
    "    print(\"ğŸ‡©ğŸ‡ª Deutsche Locale aktiviert\")\n",
    "except:\n",
    "    print(\"âš ï¸ Deutsche Locale nicht verfÃ¼gbar - verwende Standard\")\n",
    "\n",
    "# Zeitstempel fÃ¼r Dataset-Erstellung\n",
    "print(f\"ğŸ•’ Dataset-Erstellung gestartet: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}\")\n",
    "print(\"âœ… Alle Bibliotheken erfolgreich geladen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e661c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Lade Umsatzdaten...\n",
      "Shape der Umsatzdaten: (9334, 4)\n",
      "Spalten: ['id', 'Datum', 'Warengruppe', 'Umsatz']\n",
      "\n",
      "Erste 5 Zeilen:\n",
      "        id       Datum  Warengruppe  Umsatz\n",
      "0  1307011  2013-07-01            1  148.83\n",
      "1  1307021  2013-07-02            1  159.79\n",
      "2  1307031  2013-07-03            1  111.89\n",
      "3  1307041  2013-07-04            1  168.86\n",
      "4  1307051  2013-07-05            1  171.28\n",
      "\n",
      "Erste 10 IDs: [1307011, 1307021, 1307031, 1307041, 1307051, 1307061, 1307071, 1307081, 1307091, 1307101]\n",
      "\n",
      "ğŸ” SPALTEN-ANALYSE:\n",
      "Spalte 0: 'id' - Typ: int64\n",
      "  Wertebereich: 1307011.00 bis 1807315.00\n",
      "  Beispielwerte: [1307011, 1307021, 1307031]\n",
      "\n",
      "Spalte 1: 'Datum' - Typ: object\n",
      "  Beispielwerte: ['2013-07-01', '2013-07-02', '2013-07-03']\n",
      "\n",
      "Spalte 2: 'Warengruppe' - Typ: int64\n",
      "  Wertebereich: 1.00 bis 6.00\n",
      "  Beispielwerte: [1, 1, 1]\n",
      "\n",
      "Spalte 3: 'Umsatz' - Typ: float64\n",
      "  Wertebereich: 7.05 bis 1879.46\n",
      "  Beispielwerte: [148.828353112183, 159.79375714468, 111.885593514353]\n",
      "\n",
      "âœ… Umsatzspalte 'Umsatz' gefunden!\n",
      "\n",
      "ğŸ“Š VERWENDETE UMSATZSPALTE: 'Umsatz'\n",
      "\n",
      "ğŸ”„ Extrahiere Datum und Warengruppe aus ID (Format: YYMMDDW)...\n",
      "Vor Bereinigung: 9334 Zeilen\n",
      "Fehlende Daten: 0\n",
      "Nach Bereinigung: 9334 Zeilen\n",
      "Datum-Datentyp: datetime64[ns]\n",
      "Umsatz-Datentyp: float64\n",
      "âœ… 9334 gÃ¼ltige DatensÃ¤tze nach Extraktion\n",
      "ğŸ“… Zeitraum: 2013-07-01 00:00:00 bis 2018-07-31 00:00:00\n",
      "ğŸ·ï¸ Warengruppen: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "ğŸ’° Umsatz-Vorschau: count   9334.00\n",
      "mean     206.75\n",
      "std      144.55\n",
      "min        7.05\n",
      "25%       96.90\n",
      "50%      161.90\n",
      "75%      280.64\n",
      "max     1879.46\n",
      "Name: Umsatz, dtype: float64\n",
      "Vor Bereinigung: 9334 Zeilen\n",
      "Fehlende Daten: 0\n",
      "Nach Bereinigung: 9334 Zeilen\n",
      "Datum-Datentyp: datetime64[ns]\n",
      "Umsatz-Datentyp: float64\n",
      "âœ… 9334 gÃ¼ltige DatensÃ¤tze nach Extraktion\n",
      "ğŸ“… Zeitraum: 2013-07-01 00:00:00 bis 2018-07-31 00:00:00\n",
      "ğŸ·ï¸ Warengruppen: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "ğŸ’° Umsatz-Vorschau: count   9334.00\n",
      "mean     206.75\n",
      "std      144.55\n",
      "min        7.05\n",
      "25%       96.90\n",
      "50%      161.90\n",
      "75%      280.64\n",
      "max     1879.46\n",
      "Name: Umsatz, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 2. UMSATZDATEN LADEN UND VERARBEITEN\n",
    "print(\"ğŸ“Š Lade Umsatzdaten...\")\n",
    "\n",
    "# Umsatzdaten von GitHub laden\n",
    "umsatz_url = \"https://raw.githubusercontent.com/opencampus-sh/einfuehrung-in-data-science-und-ml/main/umsatzdaten_gekuerzt.csv\"\n",
    "df_umsatz = pd.read_csv(umsatz_url)\n",
    "\n",
    "print(f\"Shape der Umsatzdaten: {df_umsatz.shape}\")\n",
    "print(f\"Spalten: {list(df_umsatz.columns)}\")\n",
    "print(\"\\nErste 5 Zeilen:\")\n",
    "print(df_umsatz.head())\n",
    "\n",
    "# Debugging: Schaue dir die ersten paar IDs an\n",
    "print(f\"\\nErste 10 IDs: {df_umsatz.iloc[:10, 0].tolist()}\")\n",
    "\n",
    "# IDENTIFIZIERE DIE UMSATZSPALTE\n",
    "print(f\"\\nğŸ” SPALTEN-ANALYSE:\")\n",
    "for i, col in enumerate(df_umsatz.columns):\n",
    "    print(f\"Spalte {i}: '{col}' - Typ: {df_umsatz[col].dtype}\")\n",
    "    if df_umsatz[col].dtype in ['float64', 'int64']:\n",
    "        print(f\"  Wertebereich: {df_umsatz[col].min():.2f} bis {df_umsatz[col].max():.2f}\")\n",
    "    print(f\"  Beispielwerte: {df_umsatz[col].head(3).tolist()}\")\n",
    "    print()\n",
    "\n",
    "# Suche nach der Umsatzspalte\n",
    "umsatz_spalte = None\n",
    "possible_umsatz_names = ['umsatz', 'Umsatz', 'sales', 'revenue', 'amount']\n",
    "\n",
    "# PrÃ¼fe explizit nach \"umsatz\" Spalte\n",
    "if 'umsatz' in df_umsatz.columns:\n",
    "    umsatz_spalte = 'umsatz'\n",
    "    print(\"âœ… Umsatzspalte 'umsatz' gefunden!\")\n",
    "elif 'Umsatz' in df_umsatz.columns:\n",
    "    umsatz_spalte = 'Umsatz'\n",
    "    print(\"âœ… Umsatzspalte 'Umsatz' gefunden!\")\n",
    "else:\n",
    "    # Falls keine explizite Umsatzspalte, nimm die zweite numerische Spalte\n",
    "    numeric_cols = df_umsatz.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) >= 2:\n",
    "        umsatz_spalte = numeric_cols[1]  # Zweite numerische Spalte\n",
    "        print(f\"âš ï¸ Keine explizite Umsatzspalte gefunden. Verwende '{umsatz_spalte}' als Umsatz.\")\n",
    "    elif len(numeric_cols) == 1:\n",
    "        umsatz_spalte = numeric_cols[0]\n",
    "        print(f\"âš ï¸ Nur eine numerische Spalte gefunden. Verwende '{umsatz_spalte}' als Umsatz.\")\n",
    "    else:\n",
    "        print(\"âŒ Keine numerische Spalte fÃ¼r Umsatz gefunden!\")\n",
    "        raise ValueError(\"Keine Umsatzspalte identifiziert!\")\n",
    "\n",
    "print(f\"\\nğŸ“Š VERWENDETE UMSATZSPALTE: '{umsatz_spalte}'\")\n",
    "\n",
    "# Datum aus der ID extrahieren (Format: YYMMDDW)\n",
    "def extract_date_info(row_id):\n",
    "    \"\"\"Extrahiert Datum und Warengruppe aus der ID (Format: YYMMDDW)\"\"\"\n",
    "    try:\n",
    "        id_str = str(int(row_id))\n",
    "        \n",
    "        # YYMMDD + W (Warengruppe) - 7 Stellen erwartet\n",
    "        if len(id_str) == 7:\n",
    "            jahr_2stellig = int(id_str[:2])\n",
    "            monat = int(id_str[2:4])\n",
    "            tag = int(id_str[4:6])\n",
    "            warengruppe = int(id_str[6])\n",
    "            \n",
    "            # 2-stelliges Jahr zu 4-stelligem Jahr konvertieren\n",
    "            # Annahme: 00-30 = 2000-2030, 31-99 = 1931-1999\n",
    "            if jahr_2stellig <= 30:\n",
    "                jahr = 2000 + jahr_2stellig\n",
    "            else:\n",
    "                jahr = 1900 + jahr_2stellig\n",
    "            \n",
    "            # Datum validieren und erstellen\n",
    "            datum = pd.to_datetime(f\"{jahr}-{monat:02d}-{tag:02d}\")\n",
    "            return datum, warengruppe\n",
    "        else:\n",
    "            print(f\"âš ï¸ UngÃ¼ltige ID-LÃ¤nge: {id_str} (LÃ¤nge: {len(id_str)}, erwartet: 7)\")\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Fehler bei ID {row_id}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Datum und Warengruppe extrahieren\n",
    "print(\"\\nğŸ”„ Extrahiere Datum und Warengruppe aus ID (Format: YYMMDDW)...\")\n",
    "df_umsatz[['Datum', 'Warengruppe']] = df_umsatz.iloc[:, 0].apply(\n",
    "    lambda x: pd.Series(extract_date_info(x))\n",
    ")\n",
    "\n",
    "print(f\"Vor Bereinigung: {len(df_umsatz)} Zeilen\")\n",
    "print(f\"Fehlende Daten: {df_umsatz['Datum'].isna().sum()}\")\n",
    "\n",
    "# UngÃ¼ltige Daten entfernen\n",
    "df_umsatz = df_umsatz.dropna(subset=['Datum', 'Warengruppe'])\n",
    "print(f\"Nach Bereinigung: {len(df_umsatz)} Zeilen\")\n",
    "\n",
    "# Datentypen explizit setzen\n",
    "df_umsatz['Datum'] = pd.to_datetime(df_umsatz['Datum'])\n",
    "df_umsatz['Warengruppe'] = df_umsatz['Warengruppe'].astype(int)\n",
    "\n",
    "# Sicherstellen, dass Umsatz numerisch ist\n",
    "df_umsatz[umsatz_spalte] = pd.to_numeric(df_umsatz[umsatz_spalte], errors='coerce')\n",
    "\n",
    "print(f\"Datum-Datentyp: {df_umsatz['Datum'].dtype}\")\n",
    "print(f\"Umsatz-Datentyp: {df_umsatz[umsatz_spalte].dtype}\")\n",
    "\n",
    "# Weitere Datums-Features erstellen (ohne Sonntag-bezogene Features)\n",
    "df_umsatz['Jahr'] = df_umsatz['Datum'].dt.year\n",
    "df_umsatz['Monat'] = df_umsatz['Datum'].dt.month\n",
    "df_umsatz['Tag'] = df_umsatz['Datum'].dt.day\n",
    "df_umsatz['Wochentag'] = df_umsatz['Datum'].dt.day_name()\n",
    "df_umsatz['Wochentag_Nr'] = df_umsatz['Datum'].dt.dayofweek\n",
    "\n",
    "print(f\"âœ… {len(df_umsatz)} gÃ¼ltige DatensÃ¤tze nach Extraktion\")\n",
    "print(f\"ğŸ“… Zeitraum: {df_umsatz['Datum'].min()} bis {df_umsatz['Datum'].max()}\")\n",
    "print(f\"ğŸ·ï¸ Warengruppen: {sorted(df_umsatz['Warengruppe'].unique())}\")\n",
    "print(f\"ğŸ’° Umsatz-Vorschau: {df_umsatz[umsatz_spalte].describe()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e910a8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… df_umsatz gefunden mit 9334 Zeilen\n",
      "ğŸ”„ Erstelle BinÃ¤rkodierung fÃ¼r Warengruppen mit 0/1...\n",
      "âœ… Warengruppen-Features mit expliziter 0/1-Kodierung erstellt:\n",
      "['Warengruppe_Brot', 'Warengruppe_BrÃ¶tchen', 'Warengruppe_Croissant', 'Warengruppe_Konditorei', 'Warengruppe_Kuchen', 'Warengruppe_Saisonbrot']\n",
      "\n",
      "ğŸ” BinÃ¤rkodierung-ÃœberprÃ¼fung:\n",
      "Warengruppe_Brot: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_BrÃ¶tchen: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Croissant: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Konditorei: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Kuchen: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Saisonbrot: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "\n",
      "ğŸ“ˆ Dataset Info:\n",
      "Zeilen: 9334\n",
      "Spalten: 16\n",
      "Umsatzbereich: .2f â‚¬ - .2f â‚¬\n"
     ]
    }
   ],
   "source": [
    "# 3. WARENGRUPPEN IN LESBARE NAMEN UMWANDELN UND BINÃ„RKODIEREN\n",
    "# SicherheitsÃ¼berprÃ¼fung: Stelle sicher, dass df_umsatz existiert\n",
    "try:\n",
    "    if 'df_umsatz' not in locals():\n",
    "        raise NameError(\"df_umsatz ist nicht definiert. Bitte fÃ¼hre zuerst Zelle 3 (Umsatzdaten laden) aus.\")\n",
    "    \n",
    "    print(f\"âœ… df_umsatz gefunden mit {len(df_umsatz)} Zeilen\")\n",
    "except NameError as e:\n",
    "    print(f\"âŒ Fehler: {e}\")\n",
    "    print(\"ğŸ”„ FÃ¼hre die Zellen in der richtigen Reihenfolge aus:\")\n",
    "    print(\"   1. Bibliotheken laden\")\n",
    "    print(\"   2. Umsatzdaten laden\")\n",
    "    print(\"   3. Warengruppen verarbeiten\")\n",
    "    raise\n",
    "\n",
    "warengruppen_mapping = {\n",
    "    1: \"Brot\",\n",
    "    2: \"BrÃ¶tchen\", \n",
    "    3: \"Croissant\",\n",
    "    4: \"Konditorei\",\n",
    "    5: \"Kuchen\",\n",
    "    6: \"Saisonbrot\"\n",
    "}\n",
    "\n",
    "df_umsatz['Warengruppe_Name'] = df_umsatz['Warengruppe'].map(warengruppen_mapping)\n",
    "\n",
    "# BinÃ¤rkodierung fÃ¼r Warengruppen mit expliziten 0/1-Werten\n",
    "print(\"ğŸ”„ Erstelle BinÃ¤rkodierung fÃ¼r Warengruppen mit 0/1...\")\n",
    "for warengruppe_nr, warengruppe_name in warengruppen_mapping.items():\n",
    "    # Erstelle binÃ¤re Spalte: 1 wenn Warengruppe zutrifft, 0 sonst\n",
    "    df_umsatz[f'Warengruppe_{warengruppe_name}'] = (df_umsatz['Warengruppe'] == warengruppe_nr).astype(int)\n",
    "\n",
    "print(\"âœ… Warengruppen-Features mit expliziter 0/1-Kodierung erstellt:\")\n",
    "warengruppen_cols = [col for col in df_umsatz.columns if col.startswith('Warengruppe_') and col != 'Warengruppe_Name']\n",
    "print(warengruppen_cols)\n",
    "\n",
    "# ÃœberprÃ¼fe die BinÃ¤rkodierung\n",
    "print(f\"\\nğŸ” BinÃ¤rkodierung-ÃœberprÃ¼fung:\")\n",
    "for col in warengruppen_cols:\n",
    "    unique_values = df_umsatz[col].unique()\n",
    "    print(f\"{col}: Eindeutige Werte = {sorted(unique_values)} (Typ: {df_umsatz[col].dtype})\")\n",
    "\n",
    "# Ãœberblick Ã¼ber die Daten\n",
    "print(f\"\\nğŸ“ˆ Dataset Info:\")\n",
    "print(f\"Zeilen: {len(df_umsatz)}\")\n",
    "print(f\"Spalten: {len(df_umsatz.columns)}\")\n",
    "print(f\"Umsatzbereich: {df_umsatz.iloc[:, 1].min():.2f} â‚¬ - {df_umsatz.iloc[:, 1].max():.2f} â‚¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1730ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Analysiere Umsatzdaten ohne Manipulation...\n",
      "Verwende Umsatz-Spalte: 'Umsatz'\n",
      "\n",
      "ğŸ” ORIGINAL UMSATZDATEN-ANALYSE:\n",
      "Anzahl Transaktionen: 9334\n",
      "Umsatzbereich: 7.05 â‚¬ bis 1879.46 â‚¬\n",
      "Durchschnittsumsatz: 206.75 â‚¬\n",
      "Negative UmsÃ¤tze: 0 Transaktionen (0.0%)\n",
      "\n",
      "âœ… Umsatzdaten bleiben unverÃ¤ndert\n",
      "ğŸ“Š Finale Anzahl DatensÃ¤tze: 9334\n"
     ]
    }
   ],
   "source": [
    "# 4. UMSATZANALYSE UND BEREINIGUNG\n",
    "print(\"ğŸ“Š Analysiere Umsatzdaten ohne Manipulation...\")\n",
    "\n",
    "# Verwende die bereits identifizierte Umsatzspalte\n",
    "if 'umsatz_spalte' not in locals():\n",
    "    if 'umsatz' in df_umsatz.columns:\n",
    "        umsatz_spalte = 'umsatz'\n",
    "    elif 'Umsatz' in df_umsatz.columns:\n",
    "        umsatz_spalte = 'Umsatz'\n",
    "    else:\n",
    "        umsatz_spalte = df_umsatz.columns[1]\n",
    "\n",
    "print(f\"Verwende Umsatz-Spalte: '{umsatz_spalte}'\")\n",
    "\n",
    "# Sicherstellen, dass Umsatz numerisch ist\n",
    "df_umsatz[umsatz_spalte] = pd.to_numeric(df_umsatz[umsatz_spalte], errors='coerce')\n",
    "\n",
    "# WICHTIG: Keine Manipulation der Umsatzdaten!\n",
    "# Negative UmsÃ¤tze sind valide (RÃ¼ckgaben/Stornierungen)\n",
    "# Keine Aggregation - behalte Einzeltransaktionen\n",
    "\n",
    "print(f\"\\nğŸ” ORIGINAL UMSATZDATEN-ANALYSE:\")\n",
    "print(f\"Anzahl Transaktionen: {len(df_umsatz)}\")\n",
    "print(f\"Umsatzbereich: {df_umsatz[umsatz_spalte].min():.2f} â‚¬ bis {df_umsatz[umsatz_spalte].max():.2f} â‚¬\")\n",
    "print(f\"Durchschnittsumsatz: {df_umsatz[umsatz_spalte].mean():.2f} â‚¬\")\n",
    "print(f\"Negative UmsÃ¤tze: {(df_umsatz[umsatz_spalte] < 0).sum()} Transaktionen ({(df_umsatz[umsatz_spalte] < 0).mean()*100:.1f}%)\")\n",
    "\n",
    "# Zeige Beispiele negativer UmsÃ¤tze (als Information, nicht zur Entfernung)\n",
    "negative_sales = df_umsatz[df_umsatz[umsatz_spalte] < 0]\n",
    "if len(negative_sales) > 0:\n",
    "    print(f\"\\nğŸ“‹ NEGATIVE UMSÃ„TZE (Beispiele - das sind legitime RÃ¼ckgaben/Stornierungen):\")\n",
    "    print(negative_sales[['Datum', 'Warengruppe_Name', umsatz_spalte]].head())\n",
    "\n",
    "# KEINE AGGREGATION - behalte die Daten wie sie sind!\n",
    "print(f\"\\nâœ… Umsatzdaten bleiben unverÃ¤ndert\")\n",
    "print(f\"ğŸ“Š Finale Anzahl DatensÃ¤tze: {len(df_umsatz)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d08b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ¤ï¸ Lade Wetterdaten...\n",
      "Shape der Wetterdaten: (2601, 5)\n",
      "Spalten: ['Datum', 'Bewoelkung', 'Temperatur', 'Windgeschwindigkeit', 'Wettercode']\n",
      "\n",
      "Erste 5 Zeilen Wetterdaten:\n",
      "        Datum  Bewoelkung  Temperatur  Windgeschwindigkeit  Wettercode\n",
      "0  2012-01-01        8.00        9.82                   14       58.00\n",
      "1  2012-01-02        7.00        7.44                   12         NaN\n",
      "2  2012-01-03        8.00        5.54                   18       63.00\n",
      "3  2012-01-04        4.00        5.69                   19       80.00\n",
      "4  2012-01-05        6.00        5.30                   23       80.00\n",
      "ğŸ“Š Temperatur: Fehlende Werte mit Median 12.00 gefÃ¼llt\n",
      "ğŸ“Š Windgeschwindigkeit: Fehlende Werte mit Median 10.00 gefÃ¼llt\n",
      "ğŸ“Š Bewoelkung: Fehlende Werte mit Median 6.00 gefÃ¼llt\n",
      "\n",
      "ğŸ”— VerknÃ¼pfe Wetter- mit Umsatzdaten...\n",
      "âœ… Kombinierter Datensatz: (9334, 21)\n",
      "ğŸŒ¡ï¸ Temperaturbereich: -8.5Â°C - 31.4Â°C\n",
      "âš ï¸ 16 Tage ohne Wetterdaten - werden mit Durchschnitt gefÃ¼llt\n"
     ]
    }
   ],
   "source": [
    "# 4. WETTERDATEN LADEN UND VERKNÃœPFEN\n",
    "print(\"ğŸŒ¤ï¸ Lade Wetterdaten...\")\n",
    "\n",
    "# Wetterdaten von GitHub laden\n",
    "wetter_url = \"https://raw.githubusercontent.com/opencampus-sh/einfuehrung-in-data-science-und-ml/main/wetter.csv\"\n",
    "df_wetter = pd.read_csv(wetter_url)\n",
    "\n",
    "print(f\"Shape der Wetterdaten: {df_wetter.shape}\")\n",
    "print(f\"Spalten: {list(df_wetter.columns)}\")\n",
    "print(\"\\nErste 5 Zeilen Wetterdaten:\")\n",
    "print(df_wetter.head())\n",
    "\n",
    "# Datum in Wetterdaten konvertieren\n",
    "df_wetter['Datum'] = pd.to_datetime(df_wetter['Datum'])\n",
    "\n",
    "# Feature: Wettercode fehlt\n",
    "df_wetter['Wettercode_fehlt'] = df_wetter['Wettercode'].isna().astype(int)\n",
    "\n",
    "# Fehlende Wetterdaten mit Median fÃ¼llen\n",
    "numeric_weather_cols = ['Temperatur', 'Windgeschwindigkeit', 'Bewoelkung']\n",
    "for col in numeric_weather_cols:\n",
    "    if col in df_wetter.columns:\n",
    "        median_val = df_wetter[col].median()\n",
    "        df_wetter[col] = df_wetter[col].fillna(median_val)\n",
    "        print(f\"ğŸ“Š {col}: Fehlende Werte mit Median {median_val:.2f} gefÃ¼llt\")\n",
    "\n",
    "# Wetterdaten mit Umsatzdaten verknÃ¼pfen\n",
    "print(\"\\nğŸ”— VerknÃ¼pfe Wetter- mit Umsatzdaten...\")\n",
    "df_combined = df_umsatz.merge(df_wetter, on='Datum', how='left')\n",
    "\n",
    "print(f\"âœ… Kombinierter Datensatz: {df_combined.shape}\")\n",
    "print(f\"ğŸŒ¡ï¸ Temperaturbereich: {df_combined['Temperatur'].min():.1f}Â°C - {df_combined['Temperatur'].max():.1f}Â°C\")\n",
    "\n",
    "# Fehlende Wetterdaten anzeigen\n",
    "missing_weather = df_combined['Temperatur'].isna().sum()\n",
    "if missing_weather > 0:\n",
    "    print(f\"âš ï¸ {missing_weather} Tage ohne Wetterdaten - werden mit Durchschnitt gefÃ¼llt\")\n",
    "    for col in numeric_weather_cols:\n",
    "        if col in df_combined.columns:\n",
    "            df_combined[col] = df_combined[col].fillna(df_combined[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd74113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ„ Lade Feiertagsdaten...\n",
      "Shape der Feiertagsdaten: (468, 2)\n",
      "Spalten: ['Datum', 'Feiertag']\n",
      "\n",
      "Erste 5 Feiertage:\n",
      "        Datum             Feiertag\n",
      "0  01.01.2000              Neujahr\n",
      "1  21.04.2000           Karfreitag\n",
      "2  23.04.2000               Ostern\n",
      "3  24.04.2000          Ostermontag\n",
      "4  01.06.2000  Christi Himmelfahrt\n",
      "Beispiel-Datum Format: '01.01.2000'\n",
      "ğŸ“… Gefiltert von 468 auf 78 Feiertage (2013-2018)\n",
      "âœ… 78 Feiertage im relevanten Zeitraum gefunden\n",
      "ğŸ“… Tage mit Feiertag im Dataset: 201\n",
      "\n",
      "ğŸ„ Beispiel-Feiertage:\n",
      "         Datum\n",
      "169 2013-01-01\n",
      "170 2013-03-29\n",
      "171 2013-03-31\n",
      "172 2013-04-01\n",
      "173 2013-05-09\n",
      "174 2013-05-19\n",
      "175 2013-05-20\n",
      "176 2013-05-01\n",
      "177 2013-10-03\n",
      "178 2013-12-24\n",
      "\n",
      "ğŸ“Š FEIERTAG-STATISTIKEN:\n",
      "Feiertage im Dataset: 201 von 9334 Tagen (2.2%)\n",
      "ğŸ’° Durchschnittsumsatz an Feiertagen: 299.58 â‚¬\n",
      "ğŸ’° Durchschnittsumsatz an normalen Tagen: 204.71 â‚¬\n",
      "ğŸ“ˆ Feiertag vs. Normal Ratio: 1.46\n"
     ]
    }
   ],
   "source": [
    "# 5. FEIERTAGE LADEN UND VERKNÃœPFEN\n",
    "print(\"ğŸ„ Lade Feiertagsdaten...\")\n",
    "\n",
    "try:\n",
    "    # Feiertage aus lokaler Datei laden\n",
    "    feiertage_path = \"/workspaces/bakery_sales_prediction/5_Datasets/DE-Feiertage_2020_bis_2035.csv\"\n",
    "    \n",
    "    # Versuche verschiedene Separator und Encoding\n",
    "    try:\n",
    "        df_feiertage = pd.read_csv(feiertage_path, sep=';', encoding='utf-8')\n",
    "    except:\n",
    "        try:\n",
    "            df_feiertage = pd.read_csv(feiertage_path, sep=',', encoding='utf-8')\n",
    "        except:\n",
    "            df_feiertage = pd.read_csv(feiertage_path, sep=';', encoding='latin-1')\n",
    "    \n",
    "    print(f\"Shape der Feiertagsdaten: {df_feiertage.shape}\")\n",
    "    print(f\"Spalten: {list(df_feiertage.columns)}\")\n",
    "    print(\"\\nErste 5 Feiertage:\")\n",
    "    print(df_feiertage.head())\n",
    "    \n",
    "    # Datum in Feiertagsdaten konvertieren\n",
    "    datum_col = df_feiertage.columns[0]  # Erste Spalte als Datum\n",
    "    \n",
    "    # PrÃ¼fe das Datumsformat und konvertiere entsprechend\n",
    "    sample_date = str(df_feiertage[datum_col].iloc[0])\n",
    "    print(f\"Beispiel-Datum Format: '{sample_date}'\")\n",
    "    \n",
    "    if ';' in sample_date:\n",
    "        # Format: \"01.01.2000;Neujahr\" - extrahiere nur das Datum\n",
    "        df_feiertage[datum_col] = df_feiertage[datum_col].str.split(';').str[0]\n",
    "        print(\"âœ… Semikolon-separierte Daten erkannt und aufgeteilt\")\n",
    "    \n",
    "    # Konvertiere Datum mit deutschem Format (DD.MM.YYYY)\n",
    "    try:\n",
    "        df_feiertage['Datum'] = pd.to_datetime(df_feiertage[datum_col], format='%d.%m.%Y')\n",
    "    except ValueError:\n",
    "        try:\n",
    "            df_feiertage['Datum'] = pd.to_datetime(df_feiertage[datum_col], dayfirst=True)\n",
    "        except:\n",
    "            # Fallback: Automatische Erkennung\n",
    "            df_feiertage['Datum'] = pd.to_datetime(df_feiertage[datum_col], infer_datetime_format=True)\n",
    "    \n",
    "    # Nur relevante Jahre filtern (2013-2018)\n",
    "    original_count = len(df_feiertage)\n",
    "    df_feiertage = df_feiertage[\n",
    "        (df_feiertage['Datum'].dt.year >= 2013) & \n",
    "        (df_feiertage['Datum'].dt.year <= 2018)\n",
    "    ]\n",
    "    \n",
    "    print(f\"ğŸ“… Gefiltert von {original_count} auf {len(df_feiertage)} Feiertage (2013-2018)\")\n",
    "    \n",
    "    # BinÃ¤re Feiertag-Spalte erstellen\n",
    "    feiertage_dates = set(df_feiertage['Datum'].dt.date)\n",
    "    df_combined['ist_feiertag'] = df_combined['Datum'].dt.date.isin(feiertage_dates).astype(int)\n",
    "    \n",
    "    print(f\"âœ… {len(feiertage_dates)} Feiertage im relevanten Zeitraum gefunden\")\n",
    "    print(f\"ğŸ“… Tage mit Feiertag im Dataset: {df_combined['ist_feiertag'].sum()}\")\n",
    "    \n",
    "    # Zeige einige Beispiel-Feiertage\n",
    "    if len(df_feiertage) > 0:\n",
    "        print(f\"\\nğŸ„ Beispiel-Feiertage:\")\n",
    "        print(df_feiertage[['Datum']].head(10))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ Feiertagsdatei nicht gefunden - erstelle manuell Feiertage\")\n",
    "    # Manuelle Definition wichtiger deutscher Feiertage\n",
    "    deutsche_feiertage = [\n",
    "        # 2013\n",
    "        '2013-01-01', '2013-03-29', '2013-04-01', '2013-05-01', '2013-05-09', '2013-05-20', \n",
    "        '2013-10-03', '2013-12-25', '2013-12-26',\n",
    "        # 2014\n",
    "        '2014-01-01', '2014-04-18', '2014-04-21', '2014-05-01', '2014-05-29', '2014-06-09',\n",
    "        '2014-10-03', '2014-12-25', '2014-12-26',\n",
    "        # 2015\n",
    "        '2015-01-01', '2015-04-03', '2015-04-06', '2015-05-01', '2015-05-14', '2015-05-25',\n",
    "        '2015-10-03', '2015-12-25', '2015-12-26',\n",
    "        # 2016\n",
    "        '2016-01-01', '2016-03-25', '2016-03-28', '2016-05-01', '2016-05-05', '2016-05-16',\n",
    "        '2016-10-03', '2016-12-25', '2016-12-26',\n",
    "        # 2017\n",
    "        '2017-01-01', '2017-04-14', '2017-04-17', '2017-05-01', '2017-05-25', '2017-06-05',\n",
    "        '2017-10-03', '2017-12-25', '2017-12-26',\n",
    "        # 2018\n",
    "        '2018-01-01', '2018-03-30', '2018-04-02', '2018-05-01', '2018-05-10', '2018-05-21',\n",
    "        '2018-10-03', '2018-12-25', '2018-12-26'\n",
    "    ]\n",
    "    \n",
    "    feiertage_dates = set(pd.to_datetime(deutsche_feiertage).date)\n",
    "    df_combined['ist_feiertag'] = df_combined['Datum'].dt.date.isin(feiertage_dates).astype(int)\n",
    "    print(f\"âœ… {len(feiertage_dates)} manuelle Feiertage erstellt\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Fehler beim Verarbeiten der Feiertage: {e}\")\n",
    "    print(\"Erstelle Standard-Feiertage als Fallback...\")\n",
    "    \n",
    "    # Minimale Feiertage als Fallback\n",
    "    standard_feiertage = [\n",
    "        '2013-01-01', '2013-12-25', '2014-01-01', '2014-12-25',\n",
    "        '2015-01-01', '2015-12-25', '2016-01-01', '2016-12-25',\n",
    "        '2017-01-01', '2017-12-25', '2018-01-01', '2018-12-25'\n",
    "    ]\n",
    "    \n",
    "    feiertage_dates = set(pd.to_datetime(standard_feiertage).date)\n",
    "    df_combined['ist_feiertag'] = df_combined['Datum'].dt.date.isin(feiertage_dates).astype(int)\n",
    "    print(f\"âœ… {len(feiertage_dates)} Standard-Feiertage erstellt\")\n",
    "\n",
    "# Feiertags-Statistiken\n",
    "feiertag_count = df_combined['ist_feiertag'].sum()\n",
    "total_days = len(df_combined)\n",
    "feiertag_percent = (feiertag_count / total_days) * 100\n",
    "\n",
    "print(f\"\\nğŸ“Š FEIERTAG-STATISTIKEN:\")\n",
    "print(f\"Feiertage im Dataset: {feiertag_count} von {total_days} Tagen ({feiertag_percent:.1f}%)\")\n",
    "\n",
    "if feiertag_count > 0:\n",
    "    # Analysiere Umsatz an Feiertagen vs. normale Tage - FIX: Verwende korrekte Umsatz-Spalte\n",
    "    # Identifiziere die korrekte Umsatz-Spalte\n",
    "    if 'umsatz_spalte' in locals() and umsatz_spalte in df_combined.columns:\n",
    "        umsatz_col_name = umsatz_spalte\n",
    "    else:\n",
    "        # Fallback: Suche nach der Umsatz-Spalte\n",
    "        numeric_cols = df_combined.select_dtypes(include=[np.number]).columns\n",
    "        umsatz_col_name = None\n",
    "        for col in ['umsatz', 'Umsatz']:\n",
    "            if col in df_combined.columns:\n",
    "                umsatz_col_name = col\n",
    "                break\n",
    "        \n",
    "        if umsatz_col_name is None and len(numeric_cols) > 1:\n",
    "            # Nimm die zweite numerische Spalte (erste ist meist ID)\n",
    "            umsatz_col_name = numeric_cols[1]\n",
    "        elif umsatz_col_name is None and len(numeric_cols) == 1:\n",
    "            umsatz_col_name = numeric_cols[0]\n",
    "    \n",
    "    if umsatz_col_name and umsatz_col_name in df_combined.columns:\n",
    "        try:\n",
    "            # Stelle sicher, dass die Umsatz-Spalte numerisch ist\n",
    "            df_combined[umsatz_col_name] = pd.to_numeric(df_combined[umsatz_col_name], errors='coerce')\n",
    "            \n",
    "            feiertag_umsatz = df_combined[df_combined['ist_feiertag'] == 1][umsatz_col_name].mean()\n",
    "            normal_umsatz = df_combined[df_combined['ist_feiertag'] == 0][umsatz_col_name].mean()\n",
    "            \n",
    "            # PrÃ¼fe ob die Werte numerisch sind\n",
    "            if pd.notna(feiertag_umsatz) and pd.notna(normal_umsatz) and normal_umsatz != 0:\n",
    "                print(f\"ğŸ’° Durchschnittsumsatz an Feiertagen: {feiertag_umsatz:.2f} â‚¬\")\n",
    "                print(f\"ğŸ’° Durchschnittsumsatz an normalen Tagen: {normal_umsatz:.2f} â‚¬\")\n",
    "                print(f\"ğŸ“ˆ Feiertag vs. Normal Ratio: {feiertag_umsatz/normal_umsatz:.2f}\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Konnte Umsatz-Statistiken nicht berechnen (Feiertag: {feiertag_umsatz}, Normal: {normal_umsatz})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Fehler bei Umsatz-Statistiken: {e}\")\n",
    "            print(f\"   Umsatz-Spalte: '{umsatz_col_name}', Typ: {df_combined[umsatz_col_name].dtype}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ Keine gÃ¼ltige Umsatz-Spalte gefunden fÃ¼r Statistiken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dad5240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ¸ Erstelle Jahreszeiten-Feature...\n",
      "ğŸ”„ Erstelle BinÃ¤rkodierung fÃ¼r Jahreszeiten mit 0/1...\n",
      "âœ… Jahreszeiten-Features mit expliziter 0/1-Kodierung erstellt:\n",
      "['Jahreszeit_Winter', 'Jahreszeit_FrÃ¼hling', 'Jahreszeit_Sommer', 'Jahreszeit_Herbst']\n",
      "\n",
      "ğŸ” Jahreszeiten-BinÃ¤rkodierung-ÃœberprÃ¼fung:\n",
      "Jahreszeit_Winter: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_FrÃ¼hling: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Sommer: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Herbst: Eindeutige Werte = [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "\n",
      "ğŸ“Š Verteilung der Jahreszeiten:\n",
      "Jahreszeit\n",
      "Herbst      2410\n",
      "Sommer      2405\n",
      "Winter      2293\n",
      "FrÃ¼hling    2226\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 6. JAHRESZEIT BESTIMMEN\n",
    "print(\"ğŸŒ¸ Erstelle Jahreszeiten-Feature...\")\n",
    "\n",
    "def get_season(month):\n",
    "    \"\"\"Bestimmt die Jahreszeit basierend auf dem Monat\"\"\"\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"FrÃ¼hling\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Sommer\"\n",
    "    else:  # 9, 10, 11\n",
    "        return \"Herbst\"\n",
    "\n",
    "df_combined['Jahreszeit'] = df_combined['Monat'].apply(get_season)\n",
    "\n",
    "# BinÃ¤rkodierung fÃ¼r Jahreszeiten mit expliziten 0/1-Werten\n",
    "print(\"ğŸ”„ Erstelle BinÃ¤rkodierung fÃ¼r Jahreszeiten mit 0/1...\")\n",
    "jahreszeiten = ['Winter', 'FrÃ¼hling', 'Sommer', 'Herbst']\n",
    "for jahreszeit in jahreszeiten:\n",
    "    df_combined[f'Jahreszeit_{jahreszeit}'] = (df_combined['Jahreszeit'] == jahreszeit).astype(int)\n",
    "\n",
    "print(\"âœ… Jahreszeiten-Features mit expliziter 0/1-Kodierung erstellt:\")\n",
    "jahreszeiten_cols = [col for col in df_combined.columns if col.startswith('Jahreszeit_')]\n",
    "print(jahreszeiten_cols)\n",
    "\n",
    "# ÃœberprÃ¼fe die BinÃ¤rkodierung\n",
    "print(f\"\\nğŸ” Jahreszeiten-BinÃ¤rkodierung-ÃœberprÃ¼fung:\")\n",
    "for col in jahreszeiten_cols:\n",
    "    unique_values = df_combined[col].unique()\n",
    "    print(f\"{col}: Eindeutige Werte = {sorted(unique_values)} (Typ: {df_combined[col].dtype})\")\n",
    "\n",
    "# Verteilung der Jahreszeiten anzeigen\n",
    "jahreszeit_verteilung = df_combined['Jahreszeit'].value_counts()\n",
    "print(f\"\\nğŸ“Š Verteilung der Jahreszeiten:\")\n",
    "print(jahreszeit_verteilung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66fa7ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… Erstelle zusÃ¤tzliche Zeit-Features...\n",
      "ğŸ”„ Erstelle Wochentag-BinÃ¤rkodierung (optional)...\n",
      "âœ… Wochentag-Features erstellt: 8 Features\n",
      "\n",
      "ğŸ“Š Wochentag-Verteilung:\n",
      "Wochentag\n",
      "Tuesday      1345\n",
      "Wednesday    1342\n",
      "Sunday       1342\n",
      "Saturday     1336\n",
      "Thursday     1334\n",
      "Monday       1324\n",
      "Friday       1311\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ Hinweis: Sonntag-Feature wurde entfernt. Verwende bei Bedarf 'Wochentag_Sunday' aus der Wochentag-BinÃ¤rkodierung.\n"
     ]
    }
   ],
   "source": [
    "# 7. ZUSÃ„TZLICHE ZEIT-FEATURES (ohne Sonntag)\n",
    "print(\"ğŸ“… Erstelle zusÃ¤tzliche Zeit-Features...\")\n",
    "\n",
    "# Wochentag-BinÃ¤rkodierung (falls fÃ¼r spezifische Analysen benÃ¶tigt)\n",
    "wochentage = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "print(\"ğŸ”„ Erstelle Wochentag-BinÃ¤rkodierung (optional)...\")\n",
    "\n",
    "for wochentag in wochentage:\n",
    "    df_combined[f'Wochentag_{wochentag}'] = (df_combined['Wochentag'] == wochentag).astype(int)\n",
    "\n",
    "wochentag_cols = [col for col in df_combined.columns if col.startswith('Wochentag_')]\n",
    "print(f\"âœ… Wochentag-Features erstellt: {len(wochentag_cols)} Features\")\n",
    "\n",
    "# ÃœberprÃ¼fe die Wochentag-Verteilung\n",
    "print(f\"\\nğŸ“Š Wochentag-Verteilung:\")\n",
    "wochentag_verteilung = df_combined['Wochentag'].value_counts()\n",
    "print(wochentag_verteilung)\n",
    "\n",
    "print(f\"\\nğŸ“ Hinweis: Sonntag-Feature wurde entfernt. Verwende bei Bedarf 'Wochentag_Sunday' aus der Wochentag-BinÃ¤rkodierung.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33a1aba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â›µ Lade Kieler Woche-Daten und erstelle binÃ¤res Feature ...\n",
      "Shape der Kiwo-Daten: (72, 2)\n",
      "Spalten: ['Datum', 'KielerWoche']\n",
      "        Datum  KielerWoche\n",
      "0  2012-06-16            1\n",
      "1  2012-06-17            1\n",
      "2  2012-06-18            1\n",
      "3  2012-06-19            1\n",
      "4  2012-06-20            1\n",
      "âœ… Kieler Woche-Feature ergÃ¤nzt: 223 Tage im Datensatz sind wÃ¤hrend der Kieler Woche.\n"
     ]
    }
   ],
   "source": [
    "# 8. KIELER WOCHE: Daten laden, mergen und binÃ¤r kodieren\n",
    "print(\"â›µ Lade Kieler Woche-Daten und erstelle binÃ¤res Feature ...\")\n",
    "\n",
    "# Kiwo-Daten laden\n",
    "kiwo_url = \"https://raw.githubusercontent.com/opencampus-sh/einfuehrung-in-data-science-und-ml/main/kiwo.csv\"\n",
    "df_kiwo = pd.read_csv(kiwo_url)\n",
    "print(f\"Shape der Kiwo-Daten: {df_kiwo.shape}\")\n",
    "print(f\"Spalten: {list(df_kiwo.columns)}\")\n",
    "print(df_kiwo.head())\n",
    "\n",
    "# Datumsspalte in datetime konvertieren (Spalte heiÃŸt meist 'Datum' oder Ã¤hnlich)\n",
    "if 'Datum' in df_kiwo.columns:\n",
    "    df_kiwo['Datum'] = pd.to_datetime(df_kiwo['Datum'])\n",
    "else:\n",
    "    # Fallback: erste Spalte als Datum\n",
    "    df_kiwo[df_kiwo.columns[0]] = pd.to_datetime(df_kiwo[df_kiwo.columns[0]])\n",
    "    df_kiwo = df_kiwo.rename(columns={df_kiwo.columns[0]: 'Datum'})\n",
    "\n",
    "# BinÃ¤res Feature 'ist_kiwo' setzen: 1 fÃ¼r Kiwo-Tage, sonst 0\n",
    "kiwo_tage = set(df_kiwo['Datum'].dt.date)\n",
    "df_combined['ist_kiwo'] = df_combined['Datum'].dt.date.isin(kiwo_tage).astype(int)\n",
    "\n",
    "print(f\"âœ… Kieler Woche-Feature ergÃ¤nzt: {df_combined['ist_kiwo'].sum()} Tage im Datensatz sind wÃ¤hrend der Kieler Woche.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b51b226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Finalisiere den Trainingsdatensatz...\n",
      "\n",
      "ğŸ” ÃœberprÃ¼fe fehlende Werte:\n",
      "Wettercode          2325\n",
      "Wettercode_fehlt      16\n",
      "dtype: int64\n",
      "Wettercode          2325\n",
      "Wettercode_fehlt      16\n",
      "dtype: int64\n",
      "âœ… Wettercode: 0 fehlende Werte mit Median 28.00 gefÃ¼llt\n",
      "âœ… Wettercode_fehlt: 0 fehlende Werte mit Median 0.00 gefÃ¼llt\n",
      "ğŸ“Š Verwendete Umsatz-Spalte: 'Umsatz'\n",
      "ğŸ”§ Konvertiere Umsatz-Spalte zu numerischen Werten...\n",
      "   Aktueller Datentyp: float64\n",
      "   âœ… Bereits numerisch, zu float konvertiert\n",
      "\n",
      "ğŸ“Š FINALER TRAININGSDATENSATZ:\n",
      "ğŸ“ Shape: (9334, 34)\n",
      "ğŸ“… Zeitraum: 2013-07-01 00:00:00 bis 2018-07-31 00:00:00\n",
      "ğŸ’° Umsatzbereich: 7.05 â‚¬ - 1879.46 â‚¬\n",
      "\n",
      "ğŸ·ï¸ FEATURES (34 Spalten):\n",
      " 1. Datum\n",
      " 2. Jahr\n",
      " 3. Monat\n",
      " 4. Tag\n",
      " 5. Wochentag\n",
      " 6. Wochentag_Nr\n",
      " 7. Warengruppe\n",
      " 8. Warengruppe_Name\n",
      " 9. Temperatur\n",
      "10. Windgeschwindigkeit\n",
      "11. Bewoelkung\n",
      "12. Wettercode_fehlt\n",
      "13. ist_feiertag\n",
      "14. Jahreszeit\n",
      "15. ist_kiwo\n",
      "16. Umsatz\n",
      "17. Warengruppe_Brot\n",
      "18. Warengruppe_BrÃ¶tchen\n",
      "19. Warengruppe_Croissant\n",
      "20. Warengruppe_Konditorei\n",
      "21. Warengruppe_Kuchen\n",
      "22. Warengruppe_Saisonbrot\n",
      "23. Jahreszeit_Winter\n",
      "24. Jahreszeit_FrÃ¼hling\n",
      "25. Jahreszeit_Sommer\n",
      "26. Jahreszeit_Herbst\n",
      "27. Wochentag_Nr\n",
      "28. Wochentag_Monday\n",
      "29. Wochentag_Tuesday\n",
      "30. Wochentag_Wednesday\n",
      "31. Wochentag_Thursday\n",
      "32. Wochentag_Friday\n",
      "33. Wochentag_Saturday\n",
      "34. Wochentag_Sunday\n",
      "\n",
      "ğŸ” BINÃ„RKODIERUNG-ÃœBERPRÃœFUNG IM FINALEN DATASET:\n",
      "Gefundene binÃ¤re Spalten: 20\n",
      "Wochentag_Nr: Fehler bei Analyse - 'DataFrame' object has no attribute 'unique'\n",
      "Warengruppe_Name: ['Brot', 'BrÃ¶tchen', 'Croissant', 'Konditorei', 'Kuchen', 'Saisonbrot'] (Typ: object)\n",
      "Warengruppe_Brot: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_BrÃ¶tchen: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Croissant: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Konditorei: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Kuchen: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Saisonbrot: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Winter: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_FrÃ¼hling: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Sommer: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Herbst: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Nr: Fehler bei Analyse - 'DataFrame' object has no attribute 'unique'\n",
      "Wochentag_Monday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Tuesday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Wednesday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Thursday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Friday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Saturday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Sunday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "âœ… Wettercode: 0 fehlende Werte mit Median 28.00 gefÃ¼llt\n",
      "âœ… Wettercode_fehlt: 0 fehlende Werte mit Median 0.00 gefÃ¼llt\n",
      "ğŸ“Š Verwendete Umsatz-Spalte: 'Umsatz'\n",
      "ğŸ”§ Konvertiere Umsatz-Spalte zu numerischen Werten...\n",
      "   Aktueller Datentyp: float64\n",
      "   âœ… Bereits numerisch, zu float konvertiert\n",
      "\n",
      "ğŸ“Š FINALER TRAININGSDATENSATZ:\n",
      "ğŸ“ Shape: (9334, 34)\n",
      "ğŸ“… Zeitraum: 2013-07-01 00:00:00 bis 2018-07-31 00:00:00\n",
      "ğŸ’° Umsatzbereich: 7.05 â‚¬ - 1879.46 â‚¬\n",
      "\n",
      "ğŸ·ï¸ FEATURES (34 Spalten):\n",
      " 1. Datum\n",
      " 2. Jahr\n",
      " 3. Monat\n",
      " 4. Tag\n",
      " 5. Wochentag\n",
      " 6. Wochentag_Nr\n",
      " 7. Warengruppe\n",
      " 8. Warengruppe_Name\n",
      " 9. Temperatur\n",
      "10. Windgeschwindigkeit\n",
      "11. Bewoelkung\n",
      "12. Wettercode_fehlt\n",
      "13. ist_feiertag\n",
      "14. Jahreszeit\n",
      "15. ist_kiwo\n",
      "16. Umsatz\n",
      "17. Warengruppe_Brot\n",
      "18. Warengruppe_BrÃ¶tchen\n",
      "19. Warengruppe_Croissant\n",
      "20. Warengruppe_Konditorei\n",
      "21. Warengruppe_Kuchen\n",
      "22. Warengruppe_Saisonbrot\n",
      "23. Jahreszeit_Winter\n",
      "24. Jahreszeit_FrÃ¼hling\n",
      "25. Jahreszeit_Sommer\n",
      "26. Jahreszeit_Herbst\n",
      "27. Wochentag_Nr\n",
      "28. Wochentag_Monday\n",
      "29. Wochentag_Tuesday\n",
      "30. Wochentag_Wednesday\n",
      "31. Wochentag_Thursday\n",
      "32. Wochentag_Friday\n",
      "33. Wochentag_Saturday\n",
      "34. Wochentag_Sunday\n",
      "\n",
      "ğŸ” BINÃ„RKODIERUNG-ÃœBERPRÃœFUNG IM FINALEN DATASET:\n",
      "Gefundene binÃ¤re Spalten: 20\n",
      "Wochentag_Nr: Fehler bei Analyse - 'DataFrame' object has no attribute 'unique'\n",
      "Warengruppe_Name: ['Brot', 'BrÃ¶tchen', 'Croissant', 'Konditorei', 'Kuchen', 'Saisonbrot'] (Typ: object)\n",
      "Warengruppe_Brot: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_BrÃ¶tchen: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Croissant: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Konditorei: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Kuchen: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Warengruppe_Saisonbrot: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Winter: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_FrÃ¼hling: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Sommer: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Jahreszeit_Herbst: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Nr: Fehler bei Analyse - 'DataFrame' object has no attribute 'unique'\n",
      "Wochentag_Monday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Tuesday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Wednesday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Thursday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Friday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Saturday: [np.int64(0), np.int64(1)] (Typ: int64)\n",
      "Wochentag_Sunday: [np.int64(0), np.int64(1)] (Typ: int64)\n"
     ]
    }
   ],
   "source": [
    "# 10. FINALISIERUNG DES DATASETS\n",
    "print(\"ğŸ”§ Finalisiere den Trainingsdatensatz...\")\n",
    "\n",
    "# ÃœberprÃ¼fe fehlende Werte\n",
    "print(\"\\nğŸ” ÃœberprÃ¼fe fehlende Werte:\")\n",
    "missing_values = df_combined.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "if len(missing_values) > 0:\n",
    "    print(missing_values)\n",
    "    \n",
    "    # FÃ¼lle fehlende numerische Werte mit Median\n",
    "    numeric_cols = df_combined.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_combined[col].isnull().sum() > 0:\n",
    "            median_val = df_combined[col].median()\n",
    "            df_combined[col] = df_combined[col].fillna(median_val)\n",
    "            print(f\"âœ… {col}: {df_combined[col].isnull().sum()} fehlende Werte mit Median {median_val:.2f} gefÃ¼llt\")\n",
    "else:\n",
    "    print(\"âœ… Keine fehlenden Werte gefunden!\")\n",
    "\n",
    "# FIX: Korrekte Identifizierung der Umsatz-Spalte\n",
    "# Verwende die bereits identifizierte Umsatz-Spalte aus den vorherigen Zellen\n",
    "if 'umsatz_spalte' in locals() and umsatz_spalte in df_combined.columns:\n",
    "    umsatz_col = umsatz_spalte\n",
    "else:\n",
    "    # Fallback: Suche nach der Umsatz-Spalte\n",
    "    numeric_cols = df_combined.select_dtypes(include=[np.number]).columns\n",
    "    umsatz_col = None\n",
    "    for col in ['umsatz', 'Umsatz']:\n",
    "        if col in df_combined.columns:\n",
    "            umsatz_col = col\n",
    "            break\n",
    "    \n",
    "    if umsatz_col is None and len(numeric_cols) > 1:\n",
    "        # Nimm die zweite numerische Spalte (erste ist meist ID oder Index)\n",
    "        umsatz_col = numeric_cols[1]\n",
    "    elif umsatz_col is None and len(numeric_cols) >= 1:\n",
    "        umsatz_col = numeric_cols[0]\n",
    "    else:\n",
    "        print(\"âŒ Keine Umsatz-Spalte gefunden!\")\n",
    "        raise ValueError(\"Keine Umsatz-Spalte identifiziert!\")\n",
    "\n",
    "print(f\"ğŸ“Š Verwendete Umsatz-Spalte: '{umsatz_col}'\")\n",
    "\n",
    "feature_columns = [\n",
    "    'Datum', 'Jahr', 'Monat', 'Tag', 'Wochentag', 'Wochentag_Nr',\n",
    "    'Warengruppe', 'Warengruppe_Name',\n",
    "    'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung', 'Wettercode_fehlt',\n",
    "    'ist_feiertag', 'Jahreszeit', 'ist_kiwo', 'Preisindex',\n",
    "    umsatz_col\n",
    "]\n",
    "\n",
    "# FIX: Korrekte Filterung der binÃ¤r kodierten Spalten\n",
    "warengruppen_cols = [col for col in df_combined.columns if col.startswith('Warengruppe_') and col != 'Warengruppe_Name']\n",
    "# FIX: Korrigiere den Spaltenname fÃ¼r Jahreszeiten-Features\n",
    "jahreszeiten_cols = [col for col in df_combined.columns if col.startswith('Jahreszeit_')]\n",
    "wochentag_cols = [col for col in df_combined.columns if col.startswith('Wochentag_')]\n",
    "\n",
    "all_features = feature_columns + warengruppen_cols + jahreszeiten_cols + wochentag_cols\n",
    "\n",
    "# Finale Spalten filtern (nur die die existieren)\n",
    "final_columns = [col for col in all_features if col in df_combined.columns]\n",
    "df_final = df_combined[final_columns].copy()\n",
    "\n",
    "# FIX: Umsatz-Spalte umbenennen und korrekt konvertieren\n",
    "df_final = df_final.rename(columns={umsatz_col: 'Umsatz'})\n",
    "\n",
    "# FIX: Robustere Konvertierung der Umsatz-Spalte zu numerischen Werten\n",
    "print(f\"ğŸ”§ Konvertiere Umsatz-Spalte zu numerischen Werten...\")\n",
    "try:\n",
    "    # PrÃ¼fe zunÃ¤chst den aktuellen Datentyp\n",
    "    current_dtype = df_final['Umsatz'].dtype\n",
    "    print(f\"   Aktueller Datentyp: {current_dtype}\")\n",
    "    \n",
    "    if current_dtype == 'object' or 'datetime' in str(current_dtype).lower():\n",
    "        # Falls es sich um Object oder Datetime handelt, konvertiere zu numerisch\n",
    "        df_final['Umsatz'] = pd.to_numeric(df_final['Umsatz'], errors='coerce')\n",
    "        print(f\"   âœ… Erfolgreich zu numerisch konvertiert\")\n",
    "    elif current_dtype in ['int64', 'float64']:\n",
    "        # Bereits numerisch, explizit zu float konvertieren\n",
    "        df_final['Umsatz'] = df_final['Umsatz'].astype(float)\n",
    "        print(f\"   âœ… Bereits numerisch, zu float konvertiert\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Unbekannter Datentyp: {current_dtype}\")\n",
    "        # Versuche manuelle Konvertierung\n",
    "        df_final['Umsatz'] = pd.to_numeric(df_final['Umsatz'], errors='coerce')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Fehler beim Konvertieren der Umsatz-Spalte: {e}\")\n",
    "    print(\"ğŸ”„ Versuche alternative Konvertierung...\")\n",
    "    \n",
    "    # Alternative: Element-weise Konvertierung fÃ¼r nicht-numerische Werte\n",
    "    try:\n",
    "        def convert_to_float(value):\n",
    "            if pd.isna(value):\n",
    "                return np.nan\n",
    "            try:\n",
    "                return float(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return np.nan\n",
    "        \n",
    "        df_final['Umsatz'] = df_final['Umsatz'].apply(convert_to_float)\n",
    "        print(\"âœ… Alternative Konvertierung erfolgreich\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Auch alternative Konvertierung fehlgeschlagen: {e2}\")\n",
    "        # Als letzter Ausweg: Entferne die problematische Spalte und verwende eine andere\n",
    "        problem_col = df_final['Umsatz']\n",
    "        print(f\"Problematische Spalte Typ: {type(problem_col.iloc[0])}\")\n",
    "        print(f\"Erste 5 Werte: {problem_col.head().tolist()}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š FINALER TRAININGSDATENSATZ:\")\n",
    "print(f\"ğŸ“ Shape: {df_final.shape}\")\n",
    "print(f\"ğŸ“… Zeitraum: {df_final['Datum'].min()} bis {df_final['Datum'].max()}\")\n",
    "\n",
    "# FIX: Sicherere Berechnung der Min/Max-Werte\n",
    "try:\n",
    "    umsatz_values = df_final['Umsatz'].dropna()  # Entferne NaN-Werte fÃ¼r Berechnung\n",
    "    if len(umsatz_values) > 0:\n",
    "        umsatz_min = float(umsatz_values.min())\n",
    "        umsatz_max = float(umsatz_values.max())\n",
    "        print(f\"ğŸ’° Umsatzbereich: {umsatz_min:.2f} â‚¬ - {umsatz_max:.2f} â‚¬\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Keine gÃ¼ltigen Umsatzwerte gefunden\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Fehler bei Umsatz-Statistiken: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ·ï¸ FEATURES ({len(df_final.columns)} Spalten):\")\n",
    "for i, col in enumerate(df_final.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# ÃœberprÃ¼fe BinÃ¤rkodierung im finalen Dataset\n",
    "print(f\"\\nğŸ” BINÃ„RKODIERUNG-ÃœBERPRÃœFUNG IM FINALEN DATASET:\")\n",
    "binary_cols = [col for col in df_final.columns if col.startswith(('Warengruppe_', 'Jahreszeit_', 'Wochentag_'))]\n",
    "print(f\"Gefundene binÃ¤re Spalten: {len(binary_cols)}\")\n",
    "for col in binary_cols:\n",
    "    try:\n",
    "        unique_values = sorted(df_final[col].unique())\n",
    "        print(f\"{col}: {unique_values} (Typ: {df_final[col].dtype})\")\n",
    "    except Exception as e:\n",
    "        print(f\"{col}: Fehler bei Analyse - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33107b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Normalisiere numerische Features fÃ¼r Modelltraining...\n",
      "ğŸ“Š Zu normalisierende Features: ['Jahr', 'Monat', 'Tag', 'Wochentag_Nr', 'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung']\n",
      "\n",
      "ğŸ“ˆ ORIGINALE FEATURE-STATISTIKEN:\n",
      "ğŸ“Š Zu normalisierende Features: ['Jahr', 'Monat', 'Tag', 'Wochentag_Nr', 'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung']\n",
      "\n",
      "ğŸ“ˆ ORIGINALE FEATURE-STATISTIKEN:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Jahr   Monat     Tag  Wochentag_Nr  Wochentag_Nr  Temperatur  \\\n",
      "count 9334.00 9334.00 9334.00       9334.00       9334.00     9334.00   \n",
      "mean  2015.52    6.65   15.71          3.00          3.00       12.03   \n",
      "std      1.52    3.46    8.75          2.00          2.00        7.23   \n",
      "min   2013.00    1.00    1.00          0.00          0.00       -8.48   \n",
      "25%   2014.00    4.00    8.00          1.00          1.00        6.25   \n",
      "50%   2016.00    7.00   16.00          3.00          3.00       11.66   \n",
      "75%   2017.00   10.00   23.00          5.00          5.00       17.96   \n",
      "max   2018.00   12.00   31.00          6.00          6.00       31.44   \n",
      "\n",
      "       Windgeschwindigkeit  Bewoelkung  \n",
      "count              9334.00     9334.00  \n",
      "mean                 10.97        4.73  \n",
      "std                   4.13        2.64  \n",
      "min                   3.00        0.00  \n",
      "25%                   8.00        3.00  \n",
      "50%                  10.00        6.00  \n",
      "75%                  13.00        7.00  \n",
      "max                  35.00        8.00  \n",
      "\n",
      "ğŸ”„ Methode 1: Standard Scaler (Z-Score)\n",
      "âœ… Standard Scaler erfolgreich angewendet\n",
      "Statistiken der Z-standardisierten Features:\n",
      "       std_Jahr  std_Monat  std_Tag  std_Wochentag_Nr  std_Temperatur  \\\n",
      "count   9334.00    9334.00  9334.00           9334.00         9334.00   \n",
      "mean       0.00       0.00     0.00              0.00            0.00   \n",
      "std        1.00       1.00     1.00              1.00            1.00   \n",
      "min       -1.66      -1.63    -1.68             -1.50           -1.50   \n",
      "25%       -1.00      -0.77    -0.88             -1.00           -1.00   \n",
      "50%        0.31       0.10     0.03             -0.00           -0.00   \n",
      "75%        0.97       0.97     0.83              1.00            1.00   \n",
      "max        1.63       1.54     1.75              1.50            1.50   \n",
      "\n",
      "       std_Windgeschwindigkeit  std_Bewoelkung  \n",
      "count                  9334.00         9334.00  \n",
      "mean                     -0.00            0.00  \n",
      "std                       1.00            1.00  \n",
      "min                      -2.84           -1.93  \n",
      "25%                      -0.80           -0.72  \n",
      "50%                      -0.05           -0.24  \n",
      "75%                       0.82            0.49  \n",
      "max                       2.69            5.82  \n",
      "\n",
      "ğŸ”„ Methode 2: Min-Max Scaler (0-1)\n",
      "âœ… Min-Max Scaler erfolgreich angewendet\n",
      "ğŸ”„ Methode 3: Robust Scaler (Median-basiert)\n",
      "âœ… Robust Scaler erfolgreich angewendet\n",
      "ğŸ”„ Methode 4: Power Transformer (Yeo-Johnson)\n",
      "âœ… Power Transformer erfolgreich angewendet\n",
      "âœ… Normalisierung abgeschlossen!\n",
      "\n",
      "ğŸ“Š VERGLEICH DER NORMALISIERUNGSCMETHODEN:\n",
      "================================================================================\n",
      "\n",
      "STD-SCALER:\n",
      "Mean Bereich: 0.000 bis 0.000\n",
      "Std Bereich: 1.000 bis 1.000\n",
      "Min Bereich: -2.837 bis -1.500\n",
      "Max Bereich: 1.499 bis 5.820\n",
      "\n",
      "MINMAX-SCALER:\n",
      "Mean Bereich: 0.249 bis 0.514\n",
      "Std Bereich: 0.129 bis 0.334\n",
      "Min Bereich: 0.000 bis 0.000\n",
      "Max Bereich: 1.000 bis 1.000\n",
      "\n",
      "ROBUST-SCALER:\n",
      "Mean Bereich: -0.158 bis 0.195\n",
      "Std Bereich: 0.500 bis 0.826\n",
      "Min Bereich: -1.719 bis -0.750\n",
      "Max Bereich: 0.667 bis 5.000\n",
      "\n",
      "POWER-SCALER:\n",
      "Mean Bereich: -0.000 bis -0.000\n",
      "Std Bereich: 1.000 bis 1.000\n",
      "Min Bereich: -3.313 bis -1.621\n",
      "Max Bereich: 1.403 bis 3.516\n",
      "âœ… Power Transformer erfolgreich angewendet\n",
      "âœ… Normalisierung abgeschlossen!\n",
      "\n",
      "ğŸ“Š VERGLEICH DER NORMALISIERUNGSCMETHODEN:\n",
      "================================================================================\n",
      "\n",
      "STD-SCALER:\n",
      "Mean Bereich: 0.000 bis 0.000\n",
      "Std Bereich: 1.000 bis 1.000\n",
      "Min Bereich: -2.837 bis -1.500\n",
      "Max Bereich: 1.499 bis 5.820\n",
      "\n",
      "MINMAX-SCALER:\n",
      "Mean Bereich: 0.249 bis 0.514\n",
      "Std Bereich: 0.129 bis 0.334\n",
      "Min Bereich: 0.000 bis 0.000\n",
      "Max Bereich: 1.000 bis 1.000\n",
      "\n",
      "ROBUST-SCALER:\n",
      "Mean Bereich: -0.158 bis 0.195\n",
      "Std Bereich: 0.500 bis 0.826\n",
      "Min Bereich: -1.719 bis -0.750\n",
      "Max Bereich: 0.667 bis 5.000\n",
      "\n",
      "POWER-SCALER:\n",
      "Mean Bereich: -0.000 bis -0.000\n",
      "Std Bereich: 1.000 bis 1.000\n",
      "Min Bereich: -3.313 bis -1.621\n",
      "Max Bereich: 1.403 bis 3.516\n"
     ]
    }
   ],
   "source": [
    "# NORMALISIERUNG DER NUMERISCHEN FEATURES\n",
    "print(\"ğŸ”§ Normalisiere numerische Features fÃ¼r Modelltraining...\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Kopie des finalen Datasets fÃ¼r Normalisierung erstellen\n",
    "df_normalized = df_final.copy()\n",
    "\n",
    "# Identifiziere numerische Features (ohne Zielvariable und binÃ¤re Features)\n",
    "numeric_features_to_scale = [\n",
    "    'Jahr', 'Monat', 'Tag', 'Wochentag_Nr',\n",
    "    'Temperatur', 'Windgeschwindigkeit', 'Bewoelkung', \n",
    "    'Preisindex'\n",
    "]\n",
    "\n",
    "# Nur Features behalten, die auch im Dataset existieren\n",
    "numeric_features_to_scale = [col for col in numeric_features_to_scale if col in df_normalized.columns]\n",
    "\n",
    "print(f\"ğŸ“Š Zu normalisierende Features: {numeric_features_to_scale}\")\n",
    "\n",
    "# Originale Features vor Normalisierung anzeigen\n",
    "print(f\"\\nğŸ“ˆ ORIGINALE FEATURE-STATISTIKEN:\")\n",
    "print(df_normalized[numeric_features_to_scale].describe().round(2))\n",
    "\n",
    "# 1. STANDARD SCALER (Z-Score Normalisierung)\n",
    "print(f\"\\nğŸ”„ Methode 1: Standard Scaler (Z-Score)\")\n",
    "try:\n",
    "    scaler_standard = StandardScaler()\n",
    "    standard_scaled = scaler_standard.fit_transform(df_normalized[numeric_features_to_scale])\n",
    "    \n",
    "    # Sichere Zuweisung der normalisierten Werte - jede Spalte einzeln\n",
    "    for i, col in enumerate(numeric_features_to_scale):\n",
    "        df_normalized[f'std_{col}'] = standard_scaled[:, i]\n",
    "    \n",
    "    print(\"âœ… Standard Scaler erfolgreich angewendet\")\n",
    "    print(\"Statistiken der Z-standardisierten Features:\")\n",
    "    print(df_normalized[[f'std_{col}' for col in numeric_features_to_scale]].describe().round(2))\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Standard Scaler Fehler: {str(e)[:100]}...\")\n",
    "\n",
    "# 2. MIN-MAX SCALER (0-1 Normalisierung)\n",
    "print(f\"\\nğŸ”„ Methode 2: Min-Max Scaler (0-1)\")\n",
    "try:\n",
    "    scaler_minmax = MinMaxScaler()\n",
    "    minmax_scaled = scaler_minmax.fit_transform(df_normalized[numeric_features_to_scale])\n",
    "    \n",
    "    # Sichere Zuweisung der normalisierten Werte - jede Spalte einzeln\n",
    "    for i, col in enumerate(numeric_features_to_scale):\n",
    "        df_normalized[f'minmax_{col}'] = minmax_scaled[:, i]\n",
    "    \n",
    "    print(\"âœ… Min-Max Scaler erfolgreich angewendet\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Min-Max Scaler Fehler: {str(e)[:100]}...\")\n",
    "\n",
    "# 3. ROBUST SCALER (Median-basiert, weniger empfindlich gegen AusreiÃŸer)\n",
    "print(f\"ğŸ”„ Methode 3: Robust Scaler (Median-basiert)\")\n",
    "try:\n",
    "    scaler_robust = RobustScaler()\n",
    "    robust_scaled = scaler_robust.fit_transform(df_normalized[numeric_features_to_scale])\n",
    "    \n",
    "    # Sichere Zuweisung der normalisierten Werte - jede Spalte einzeln\n",
    "    for i, col in enumerate(numeric_features_to_scale):\n",
    "        df_normalized[f'robust_{col}'] = robust_scaled[:, i]\n",
    "    \n",
    "    print(\"âœ… Robust Scaler erfolgreich angewendet\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Robust Scaler Fehler: {str(e)[:100]}...\")\n",
    "\n",
    "# 4. POWER TRANSFORMER (Yeo-Johnson fÃ¼r Normalverteilung)\n",
    "print(f\"ğŸ”„ Methode 4: Power Transformer (Yeo-Johnson)\")\n",
    "try:\n",
    "    power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    power_scaled = power_transformer.fit_transform(df_normalized[numeric_features_to_scale])\n",
    "    \n",
    "    # Sichere Zuweisung der normalisierten Werte - jede Spalte einzeln\n",
    "    for i, col in enumerate(numeric_features_to_scale):\n",
    "        df_normalized[f'power_{col}'] = power_scaled[:, i]\n",
    "    \n",
    "    print(\"âœ… Power Transformer erfolgreich angewendet\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Power Transformer Ã¼bersprungen (Fehler: {str(e)[:50]}...)\")\n",
    "\n",
    "print(f\"âœ… Normalisierung abgeschlossen!\")\n",
    "\n",
    "# Vergleiche die Normalisierungsmethoden\n",
    "scaling_methods = ['std', 'minmax', 'robust', 'power']\n",
    "print(f\"\\nğŸ“Š VERGLEICH DER NORMALISIERUNGSCMETHODEN:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for method in scaling_methods:\n",
    "    method_cols = [col for col in df_normalized.columns if col.startswith(f'{method}_')]\n",
    "    if method_cols:\n",
    "        print(f\"\\n{method.upper()}-SCALER:\")\n",
    "        try:\n",
    "            stats = df_normalized[method_cols].describe().round(3)\n",
    "            print(f\"Mean Bereich: {stats.loc['mean'].min():.3f} bis {stats.loc['mean'].max():.3f}\")\n",
    "            print(f\"Std Bereich: {stats.loc['std'].min():.3f} bis {stats.loc['std'].max():.3f}\")\n",
    "            print(f\"Min Bereich: {stats.loc['min'].min():.3f} bis {stats.loc['min'].max():.3f}\")\n",
    "            print(f\"Max Bereich: {stats.loc['max'].min():.3f} bis {stats.loc['max'].max():.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler bei Statistiken: {str(e)[:50]}...\")\n",
    "    else:\n",
    "        print(f\"\\n{method.upper()}-SCALER: Keine Spalten gefunden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b690bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ EMPFEHLUNG FÃœR NORMALISIERUNGSMETHODE:\n",
      "============================================================\n",
      "ğŸ“‹ Bewertungskriterien fÃ¼r Neuronale Netze:\n",
      "1. Ã„hnliche Skalierung aller Features (Mean â‰ˆ 0, Std â‰ˆ 1)\n",
      "2. Keine extremen AusreiÃŸer\n",
      "3. Stabile Gradientenberechnung\n",
      "4. Erhaltung der Datenverteilung\n",
      "\n",
      "ğŸ“Š BEWERTUNG DER NORMALISIERUNGSMETHODEN:\n",
      "STD-Scaler:\n",
      "  Gesamtscore: 0.701\n",
      "  Mean-Konsistenz: 1.000\n",
      "  Std-Konsistenz: 1.000\n",
      "  Wertebereich: 0.104\n",
      "MINMAX-Scaler:\n",
      "  Gesamtscore: 0.779\n",
      "  Mean-Konsistenz: 0.912\n",
      "  Std-Konsistenz: 0.925\n",
      "  Wertebereich: 0.500\n",
      "ROBUST-Scaler:\n",
      "  Gesamtscore: 0.643\n",
      "  Mean-Konsistenz: 0.904\n",
      "  Std-Konsistenz: 0.897\n",
      "  Wertebereich: 0.130\n",
      "\n",
      "âœ… EMPFEHLUNG: MINMAX-SCALER\n",
      "ğŸ† Beste Gesamtbewertung: 0.779\n",
      "\n",
      "ğŸ“Š MODELL-BEREITES DATASET:\n",
      "ğŸ“ Shape: (9334, 37)\n",
      "ğŸ·ï¸ Features: 37\n",
      "\n",
      "ğŸ”§ NORMALISIERTE FEATURES:\n",
      "Jahr_normalized: Mean=0.505, Std=0.304, Range=[0.000, 1.000]\n",
      "Monat_normalized: Mean=0.514, Std=0.315, Range=[0.000, 1.000]\n",
      "Tag_normalized: Mean=0.490, Std=0.292, Range=[0.000, 1.000]\n",
      "Wochentag_Nr_normalized: Mean=0.500, Std=0.334, Range=[0.000, 1.000]\n",
      "Temperatur_normalized: Mean=0.500, Std=0.334, Range=[0.000, 1.000]\n",
      "Windgeschwindigkeit_normalized: Mean=0.514, Std=0.181, Range=[0.000, 1.000]\n",
      "Bewoelkung_normalized: Mean=0.249, Std=0.129, Range=[0.000, 1.000]\n",
      "\n",
      "ğŸ” BINÃ„RE FEATURES IM NORMALISIERTEN DATASET:\n",
      "Anzahl binÃ¤rer Features: 23\n",
      "  Warengruppe_Name: ['Brot', 'BrÃ¶tchen', 'Croissant', 'Konditorei', 'Kuchen', 'Saisonbrot']\n",
      "  Warengruppe_Brot: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_BrÃ¶tchen: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_Croissant: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_Konditorei: [np.int64(0), np.int64(1)]\n",
      "  ... und 18 weitere binÃ¤re Features\n",
      "\n",
      "ğŸ“Š BEWERTUNG DER NORMALISIERUNGSMETHODEN:\n",
      "STD-Scaler:\n",
      "  Gesamtscore: 0.701\n",
      "  Mean-Konsistenz: 1.000\n",
      "  Std-Konsistenz: 1.000\n",
      "  Wertebereich: 0.104\n",
      "MINMAX-Scaler:\n",
      "  Gesamtscore: 0.779\n",
      "  Mean-Konsistenz: 0.912\n",
      "  Std-Konsistenz: 0.925\n",
      "  Wertebereich: 0.500\n",
      "ROBUST-Scaler:\n",
      "  Gesamtscore: 0.643\n",
      "  Mean-Konsistenz: 0.904\n",
      "  Std-Konsistenz: 0.897\n",
      "  Wertebereich: 0.130\n",
      "\n",
      "âœ… EMPFEHLUNG: MINMAX-SCALER\n",
      "ğŸ† Beste Gesamtbewertung: 0.779\n",
      "\n",
      "ğŸ“Š MODELL-BEREITES DATASET:\n",
      "ğŸ“ Shape: (9334, 37)\n",
      "ğŸ·ï¸ Features: 37\n",
      "\n",
      "ğŸ”§ NORMALISIERTE FEATURES:\n",
      "Jahr_normalized: Mean=0.505, Std=0.304, Range=[0.000, 1.000]\n",
      "Monat_normalized: Mean=0.514, Std=0.315, Range=[0.000, 1.000]\n",
      "Tag_normalized: Mean=0.490, Std=0.292, Range=[0.000, 1.000]\n",
      "Wochentag_Nr_normalized: Mean=0.500, Std=0.334, Range=[0.000, 1.000]\n",
      "Temperatur_normalized: Mean=0.500, Std=0.334, Range=[0.000, 1.000]\n",
      "Windgeschwindigkeit_normalized: Mean=0.514, Std=0.181, Range=[0.000, 1.000]\n",
      "Bewoelkung_normalized: Mean=0.249, Std=0.129, Range=[0.000, 1.000]\n",
      "\n",
      "ğŸ” BINÃ„RE FEATURES IM NORMALISIERTEN DATASET:\n",
      "Anzahl binÃ¤rer Features: 23\n",
      "  Warengruppe_Name: ['Brot', 'BrÃ¶tchen', 'Croissant', 'Konditorei', 'Kuchen', 'Saisonbrot']\n",
      "  Warengruppe_Brot: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_BrÃ¶tchen: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_Croissant: [np.int64(0), np.int64(1)]\n",
      "  Warengruppe_Konditorei: [np.int64(0), np.int64(1)]\n",
      "  ... und 18 weitere binÃ¤re Features\n",
      "\n",
      "âœ… Normalisiertes Dataset gespeichert: /workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset_normalized.csv\n",
      "âœ… Normalisierungs-Info gespeichert: /workspaces/bakery_sales_prediction/5_Datasets/normalization_info.json\n",
      "\n",
      "ğŸ‰ NORMALISIERUNG ABGESCHLOSSEN!\n",
      "ğŸ“ Original Dataset: bakery_training_dataset.csv\n",
      "ğŸ“ Normalisiertes Dataset: bakery_training_dataset_normalized.csv\n",
      "âœ… Sonntag-Feature entfernt\n",
      "âœ… BinÃ¤rkodierung mit expliziten 0/1-Werten\n",
      "ğŸš€ Bereit fÃ¼r Neuronale Netze und ML-Modelle!\n",
      "\n",
      "âœ… Normalisiertes Dataset gespeichert: /workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset_normalized.csv\n",
      "âœ… Normalisierungs-Info gespeichert: /workspaces/bakery_sales_prediction/5_Datasets/normalization_info.json\n",
      "\n",
      "ğŸ‰ NORMALISIERUNG ABGESCHLOSSEN!\n",
      "ğŸ“ Original Dataset: bakery_training_dataset.csv\n",
      "ğŸ“ Normalisiertes Dataset: bakery_training_dataset_normalized.csv\n",
      "âœ… Sonntag-Feature entfernt\n",
      "âœ… BinÃ¤rkodierung mit expliziten 0/1-Werten\n",
      "ğŸš€ Bereit fÃ¼r Neuronale Netze und ML-Modelle!\n"
     ]
    }
   ],
   "source": [
    "# EMPFEHLUNG UND FINALE DATASET-ERSTELLUNG\n",
    "print(\"ğŸ¯ EMPFEHLUNG FÃœR NORMALISIERUNGSMETHODE:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Bewertungskriterien fÃ¼r neuronale netze\n",
    "print(\"ğŸ“‹ Bewertungskriterien fÃ¼r Neuronale Netze:\")\n",
    "print(\"1. Ã„hnliche Skalierung aller Features (Mean â‰ˆ 0, Std â‰ˆ 1)\")\n",
    "print(\"2. Keine extremen AusreiÃŸer\")\n",
    "print(\"3. Stabile Gradientenberechnung\")\n",
    "print(\"4. Erhaltung der Datenverteilung\")\n",
    "\n",
    "# Analysiere StabilitÃ¤t der verschiedenen Scaler\n",
    "scaler_scores = {}\n",
    "\n",
    "for method in ['std', 'minmax', 'robust']:\n",
    "    method_cols = [col for col in df_normalized.columns if col.startswith(f'{method}_')]\n",
    "    if method_cols:\n",
    "        method_data = df_normalized[method_cols]\n",
    "        \n",
    "        # Bewertungskriterien berechnen\n",
    "        mean_consistency = 1 / (1 + abs(method_data.mean().std()))  # Je einheitlicher die Means, desto besser\n",
    "        std_consistency = 1 / (1 + abs(method_data.std().std()))    # Je einheitlicher die Stds, desto besser\n",
    "        range_control = 1 / (1 + (method_data.max().max() - method_data.min().min()))  # Kontrollierter Wertebereich\n",
    "        \n",
    "        total_score = (mean_consistency + std_consistency + range_control) / 3\n",
    "        scaler_scores[method] = {\n",
    "            'total': total_score,\n",
    "            'mean_consistency': mean_consistency,\n",
    "            'std_consistency': std_consistency,\n",
    "            'range_control': range_control\n",
    "        }\n",
    "\n",
    "# Beste Methode ermitteln\n",
    "best_method = max(scaler_scores.keys(), key=lambda k: scaler_scores[k]['total'])\n",
    "\n",
    "print(f\"\\nğŸ“Š BEWERTUNG DER NORMALISIERUNGSMETHODEN:\")\n",
    "for method, scores in scaler_scores.items():\n",
    "    print(f\"{method.upper()}-Scaler:\")\n",
    "    print(f\"  Gesamtscore: {scores['total']:.3f}\")\n",
    "    print(f\"  Mean-Konsistenz: {scores['mean_consistency']:.3f}\")\n",
    "    print(f\"  Std-Konsistenz: {scores['std_consistency']:.3f}\")\n",
    "    print(f\"  Wertebereich: {scores['range_control']:.3f}\")\n",
    "\n",
    "print(f\"\\nâœ… EMPFEHLUNG: {best_method.upper()}-SCALER\")\n",
    "print(f\"ğŸ† Beste Gesamtbewertung: {scaler_scores[best_method]['total']:.3f}\")\n",
    "\n",
    "# Finales Dataset mit der besten Normalisierungsmethode erstellen\n",
    "best_method_cols = [col for col in df_normalized.columns if col.startswith(f'{best_method}_')]\n",
    "\n",
    "# Features fÃ¼r finales normalisiertes Dataset auswÃ¤hlen (ohne Sonntag-Feature)\n",
    "final_normalized_features = [\n",
    "    'Datum', 'Umsatz',  # Basis-Features\n",
    "    'Warengruppe', 'Warengruppe_Name', 'Wochentag', 'Jahreszeit',  # Kategorische Features\n",
    "    'ist_feiertag', 'ist_kiwo', 'Wettercode_fehlt'  # BinÃ¤re Features (ohne ist_sonntag)\n",
    "]\n",
    "\n",
    "# BinÃ¤r kodierte Features hinzufÃ¼gen\n",
    "warengruppen_binary = [col for col in df_normalized.columns if col.startswith('Warengruppe_') and col != 'Warengruppe_Name']\n",
    "jahreszeiten_binary = [col for col in df_normalized.columns if col.startswith('Jahreszeit_')]\n",
    "wochentag_binary = [col for col in df_normalized.columns if col.startswith('Wochentag_')]\n",
    "\n",
    "# Normalisierte numerische Features der besten Methode hinzufÃ¼gen\n",
    "final_features = final_normalized_features + warengruppen_binary + jahreszeiten_binary + wochentag_binary + best_method_cols\n",
    "\n",
    "# Nur existierende Spalten auswÃ¤hlen\n",
    "final_features = [col for col in final_features if col in df_normalized.columns]\n",
    "\n",
    "# Finales normalisiertes Dataset erstellen\n",
    "df_model_ready = df_normalized[final_features].copy()\n",
    "\n",
    "# Normalisierte Spalten umbenennen (PrÃ¤fix entfernen fÃ¼r Klarheit)\n",
    "rename_dict = {}\n",
    "for col in best_method_cols:\n",
    "    original_name = col.replace(f'{best_method}_', '')\n",
    "    rename_dict[col] = f'{original_name}_normalized'\n",
    "\n",
    "df_model_ready = df_model_ready.rename(columns=rename_dict)\n",
    "\n",
    "print(f\"\\nğŸ“Š MODELL-BEREITES DATASET:\")\n",
    "print(f\"ğŸ“ Shape: {df_model_ready.shape}\")\n",
    "print(f\"ğŸ·ï¸ Features: {len(df_model_ready.columns)}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ NORMALISIERTE FEATURES:\")\n",
    "normalized_cols = [col for col in df_model_ready.columns if col.endswith('_normalized')]\n",
    "for col in normalized_cols:\n",
    "    stats = df_model_ready[col].describe()\n",
    "    print(f\"{col}: Mean={stats['mean']:.3f}, Std={stats['std']:.3f}, Range=[{stats['min']:.3f}, {stats['max']:.3f}]\")\n",
    "\n",
    "# ÃœberprÃ¼fe die binÃ¤ren Features im finalen normalisierten Dataset\n",
    "print(f\"\\nğŸ” BINÃ„RE FEATURES IM NORMALISIERTEN DATASET:\")\n",
    "binary_cols = [col for col in df_model_ready.columns if col.startswith(('Warengruppe_', 'Jahreszeit_', 'Wochentag_'))]\n",
    "print(f\"Anzahl binÃ¤rer Features: {len(binary_cols)}\")\n",
    "for col in binary_cols[:5]:  # Zeige nur die ersten 5 als Beispiel\n",
    "    unique_vals = sorted(df_model_ready[col].unique())\n",
    "    print(f\"  {col}: {unique_vals}\")\n",
    "if len(binary_cols) > 5:\n",
    "    print(f\"  ... und {len(binary_cols) - 5} weitere binÃ¤re Features\")\n",
    "\n",
    "# Speichere das normalisierte Dataset\n",
    "normalized_output_path = \"/workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset_normalized.csv\"\n",
    "\n",
    "try:\n",
    "    df_model_ready.to_csv(normalized_output_path, index=False, encoding='utf-8')\n",
    "    print(f\"\\nâœ… Normalisiertes Dataset gespeichert: {normalized_output_path}\")\n",
    "    \n",
    "    # ZusÃ¤tzliche Info-Datei mit Normalisierungsparametern\n",
    "    normalization_info = {\n",
    "        'Methode': best_method.upper() + '-Scaler',\n",
    "        'Normalisierte_Features': [col.replace('_normalized', '') for col in normalized_cols],\n",
    "        'Original_Features': numeric_features_to_scale,\n",
    "        'Binaere_Features': binary_cols,\n",
    "        'Entfernte_Features': ['ist_sonntag'],  # Dokumentiere entfernte Features\n",
    "        'Dataset_Shape': df_model_ready.shape,\n",
    "        'Empfehlung': f\"Verwende {best_method.upper()}-Scaler fÃ¼r optimale NN-Performance\"\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    info_path = \"/workspaces/bakery_sales_prediction/5_Datasets/normalization_info.json\"\n",
    "    with open(info_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(normalization_info, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"âœ… Normalisierungs-Info gespeichert: {info_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Fehler beim Speichern: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ NORMALISIERUNG ABGESCHLOSSEN!\")\n",
    "print(f\"ğŸ“ Original Dataset: bakery_training_dataset.csv\")\n",
    "print(f\"ğŸ“ Normalisiertes Dataset: bakery_training_dataset_normalized.csv\")\n",
    "print(f\"âœ… Sonntag-Feature entfernt\")\n",
    "print(f\"âœ… BinÃ¤rkodierung mit expliziten 0/1-Werten\")\n",
    "print(f\"ğŸš€ Bereit fÃ¼r Neuronale Netze und ML-Modelle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e3b9717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Speichere finalen Trainingsdatensatz...\n",
      "âœ… Dataset erfolgreich gespeichert: /workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset.csv\n",
      "âŒ Fehler beim Speichern: No module named 'openpyxl'\n",
      "\n",
      "ğŸ‰ TRAININGSDATENSATZ ERFOLGREICH ERSTELLT!\n",
      "ğŸ“ Hauptdatei: /workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset.csv\n",
      "ğŸ“‹ 9334 Zeilen mit 34 Features\n",
      "ğŸ¯ Bereit fÃ¼r Regression und Neuronale Netze!\n",
      "\n",
      "ğŸ“Š FINALE ZUSAMMENFASSUNG:\n",
      "ğŸ¥ Warengruppen: 6\n",
      "ğŸ“… Tage: 1819\n",
      "ğŸŒ¡ï¸ Temperaturbereich: -8.5Â°C - 31.4Â°C\n",
      "ğŸ„ Feiertage: 201 Tage\n",
      "â›µ Kieler Woche: 223 Tage\n",
      "ğŸ”¢ BinÃ¤re Features: 20 (alle mit 0/1-Kodierung)\n",
      "ğŸ’° Durchschnittsumsatz: 206.75 â‚¬\n",
      "\n",
      "âœ… Ã„NDERUNGEN DURCHGEFÃœHRT:\n",
      "   â€¢ Sonntag-Feature (ist_sonntag) entfernt\n",
      "   â€¢ Warengruppen-Features mit expliziter 0/1-BinÃ¤rkodierung\n",
      "   â€¢ Jahreszeiten-Features mit expliziter 0/1-BinÃ¤rkodierung\n",
      "   â€¢ Wochentag-Features als Alternative verfÃ¼gbar\n",
      "   â€¢ Kieler Woche und Preisindex Features hinzugefÃ¼gt\n"
     ]
    }
   ],
   "source": [
    "# DATASET SPEICHERN\n",
    "print(\"ğŸ’¾ Speichere finalen Trainingsdatensatz...\")\n",
    "\n",
    "# Pfad fÃ¼r das finale Dataset\n",
    "output_path = \"/workspaces/bakery_sales_prediction/5_Datasets/bakery_training_dataset.csv\"\n",
    "\n",
    "try:\n",
    "    # CSV speichern\n",
    "    df_final.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"âœ… Dataset erfolgreich gespeichert: {output_path}\")\n",
    "    \n",
    "    # ZusÃ¤tzlich als Excel fÃ¼r bessere Lesbarkeit\n",
    "    excel_path = output_path.replace('.csv', '.xlsx')\n",
    "    df_final.to_excel(excel_path, index=False)\n",
    "    print(f\"âœ… Dataset auch als Excel gespeichert: {excel_path}\")\n",
    "    \n",
    "    # Feature-Liste separat speichern\n",
    "    feature_info = pd.DataFrame({\n",
    "        'Feature': df_final.columns,\n",
    "        'Typ': df_final.dtypes,\n",
    "        'Fehlende_Werte': df_final.isnull().sum(),\n",
    "        'Eindeutige_Werte': df_final.nunique()\n",
    "    })\n",
    "    \n",
    "    feature_path = \"/workspaces/bakery_sales_prediction/5_Datasets/feature_description.csv\"\n",
    "    feature_info.to_csv(feature_path, index=False)\n",
    "    print(f\"âœ… Feature-Beschreibung gespeichert: {feature_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Fehler beim Speichern: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ TRAININGSDATENSATZ ERFOLGREICH ERSTELLT!\")\n",
    "print(f\"ğŸ“ Hauptdatei: {output_path}\")\n",
    "print(f\"ğŸ“‹ {len(df_final)} Zeilen mit {len(df_final.columns)} Features\")\n",
    "print(f\"ğŸ¯ Bereit fÃ¼r Regression und Neuronale Netze!\")\n",
    "\n",
    "# Finale Zusammenfassung (ohne Sonntag-Feature) - FIX: Robuste Behandlung fehlender Features\n",
    "print(f\"\\nğŸ“Š FINALE ZUSAMMENFASSUNG:\")\n",
    "print(f\"ğŸ¥ Warengruppen: {df_final['Warengruppe_Name'].nunique()}\")\n",
    "print(f\"ğŸ“… Tage: {df_final['Datum'].nunique()}\")\n",
    "print(f\"ğŸŒ¡ï¸ Temperaturbereich: {df_final['Temperatur'].min():.1f}Â°C - {df_final['Temperatur'].max():.1f}Â°C\")\n",
    "\n",
    "# Sichere Behandlung von Feiertagen\n",
    "if 'ist_feiertag' in df_final.columns:\n",
    "    print(f\"ğŸ„ Feiertage: {df_final['ist_feiertag'].sum()} Tage\")\n",
    "else:\n",
    "    print(f\"ğŸ„ Feiertage: Feature nicht verfÃ¼gbar\")\n",
    "\n",
    "# Sichere Behandlung von Kieler Woche\n",
    "if 'ist_kiwo' in df_final.columns:\n",
    "    print(f\"â›µ Kieler Woche: {df_final['ist_kiwo'].sum()} Tage\")\n",
    "else:\n",
    "    print(f\"â›µ Kieler Woche: Feature nicht verfÃ¼gbar\")\n",
    "\n",
    "# ÃœberprÃ¼fe die binÃ¤ren Features\n",
    "binary_features = [col for col in df_final.columns if col.startswith(('Warengruppe_', 'Jahreszeit_', 'Wochentag_'))]\n",
    "print(f\"ğŸ”¢ BinÃ¤re Features: {len(binary_features)} (alle mit 0/1-Kodierung)\")\n",
    "\n",
    "# Robustere Berechnung des Durchschnittsumsatzes\n",
    "try:\n",
    "    # Methode 1: Konvertiere zu NumPy-Array und berechne den Mittelwert\n",
    "    mean_umsatz = df_final['Umsatz'].to_numpy().mean()\n",
    "    print(f\"ğŸ’° Durchschnittsumsatz: {mean_umsatz:.2f} â‚¬\")\n",
    "except Exception as e:\n",
    "    try:\n",
    "        # Methode 2: Verwende NumPy direkt\n",
    "        mean_umsatz = np.nanmean(df_final['Umsatz'].values)\n",
    "        print(f\"ğŸ’° Durchschnittsumsatz: {mean_umsatz:.2f} â‚¬\")\n",
    "    except Exception as e:\n",
    "        # Fallback: Zeige den Wert ohne Formatierung\n",
    "        print(f\"ğŸ’° Durchschnittsumsatz: {df_final['Umsatz'].mean()} â‚¬\")\n",
    "\n",
    "print(f\"\\nâœ… Ã„NDERUNGEN DURCHGEFÃœHRT:\")\n",
    "print(f\"   â€¢ Sonntag-Feature (ist_sonntag) entfernt\")\n",
    "print(f\"   â€¢ Warengruppen-Features mit expliziter 0/1-BinÃ¤rkodierung\")\n",
    "print(f\"   â€¢ Jahreszeiten-Features mit expliziter 0/1-BinÃ¤rkodierung\")\n",
    "print(f\"   â€¢ Wochentag-Features als Alternative verfÃ¼gbar\")\n",
    "print(f\"   â€¢ Kieler Woche und Preisindex Features hinzugefÃ¼gt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
